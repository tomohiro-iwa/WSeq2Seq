Document how to make evcxr a rust jupyter kernel on Mac OS X __EoT__ I confirm evcxr_jupyter works on my mac box . But I spent more time than expected to make it work because I could n't find any relevant documentation . Below is what should be added to the docs . `` ` $ brew install zmq $ evcxr_jupyter -- install Writing /Users/CC/Library/Application Support/jupyter/kernels/rust/kernel.json Installation complete $ jupyter kernelspec install ~/Library/Application\ Support/jupyter/kernels/rust [ InstallKernelSpec ] Removing existing kernelspec in /usr/local/share/jupyter/kernels/rust [ InstallKernelSpec ] Installed kernelspec rust in /usr/local/share/jupyter/kernels/rust `` `
kasan_symbolize : finds wrong function __EoT__ Andrey Ryabinin noted a bug in kasan_symbolize.py : https : //groups.google.com/d/msg/syzkaller/Fu6BruqUHOU/dwSLNeHHAwAJ `` ` No , it 's a bug in your script . To find out source location , it uses 'function_name + offset ' instead of absolute address . We have 2 kthread ( ) functions in kernel and this confuses you script . E.g . my vmlinux : $ addr2line -i -e vmlinux ffffffff811b5290 /home/andrew/linux/kernel/kthread.c:178 $ addr2line -i -e kasan_conf/vmlinux ffffffff825c7240 /home/andrew/linux/drivers/block/aoe/aoecmd.c:1289 $ echo ' [ < ffffffff811b5290 > ] kthread+0x00/0x00 ' | python kasan_symbolize.py vmlinux [ < ffffffff811b5290 > ] kthread+0x00/0x00 drivers/block/aoe/aoecmd.c:462 $ echo ' [ < ffffffff825c7240 > ] kthread+0x00/0x00 ' | python kasan_symbolize.py vmlinux [ < ffffffff825c7240 > ] kthread+0x00/0x00 drivers/block/aoe/aoecmd.c:462 `` `
No keytemplate example __EoT__ The documentation at https : //github.com/google/tink/blob/master/doc/TINKEY.md points to https : //github.com/google/tink/tree/master/examples/keytemplates which does not exist at the moment . Did you forget to publish those ? Or are they still WIP ?
AttributeError : 'module ' object has no attribute 'Signals ' __EoT__ This happens with ` subprocess32==3.5.0 ` : `` ` pycon > > > import subprocess32 as s32 > > > exc = s32.CalledProcessError ( -1 , `` eggs '' ) > > > raise exc Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > subprocess32.CalledProcessError : < exception str ( ) failed > > > > str ( exc ) Traceback ( most recent call last ) : File `` < stdin > '' , line 1 , in < module > File `` /home/jwilk/.local/lib/python2.7/site-packages/subprocess32.py '' , line 72 , in __str__ self.cmd , signal.Signals ( -self.returncode ) ) AttributeError : 'module ' object has no attribute 'Signals' `` ` Aside from bugginess , I wonder why this code is there in the first place . subprocess uses ` signal.Signals ` only since Python 3.6 , whereas subprocess32 is supposed to be 3.2 with some 3.3 and 3.5 backports .
Restart hung process __EoT__ Hi , my computer hibernated overnight and the process has hung after loading 15000 mail messages . Is there any way to re-start the process from the point it has hung ?
prefer-const should be enabled ? __EoT__ It seems like it is n't enabled and eslint : recommended does n't enable it either . However , in the style guide [ here ] ( https : //google.github.io/styleguide/jsguide.html # features-use-const-and-let ) : > Declare all local variables with either const or let . Use const by default , unless a variable needs to be reassigned . The var keyword must not be used . So surely this means it should be an error to use ` let ` for a variable which is not reassigned ? edit : seems this is fixed by # 49 if it gets merged .
Could not resolve vkGetInstanceProcAddr on Linux __EoT__ Vulkan library is found successfully , but function is not resolved .
Could not resolve vkGetInstanceProcAddr on Linux __EoT__ Vulkan library is found successfully , but function is not resolved .
Support datetime serialisation in a standard string format __EoT__ ( I 've seen there 's a ` DateTimeRule ` , but it says `` A hard-coded rule for DateTime '' , so it 's not clear to me how ( or if ) I can override it ) . Currently when serialising something with ` DateTime ` s , I get this : `` ` { name : 'Danny ' , startDateUtc : [ 1414781347884 , false ] } `` ` This is messy to interop with other services ; many of which just use a standard string format for dates like ` yyyy-MM-ddTHH : mm : ss ` . It would be good if we could set a custom format on the serializer for this .
Wildcard on element step returns 0 results __EoT__ Using xpath expressions with a wildcard select for nodes doesnt return any results : I will be using this as my sample document `` ` xml < MyDocument xmlns= '' http : //testschema.com/ '' > < ParentNode > < ChildNode / > < /ParentNode > < /MyDocument > `` ` Then if you run this xpath query ` /a : MyDocument/a : Parent/* ` you will get 0 results . You can make the xpath query work by replacing the single ` / ` with a double ` // ` before the wildcard . ` /a : MyDocument/a : Parent//* ` . I did make a [ Plunker for testing this ] ( http : //plnkr.co/edit/4StePkprP9pvUFC92LB1 ) . The Plunker is using a wgxpath install which overwrites the ` document.evaluate ` method so you dont have to run it on IE to test it .
While evaluating xpath having element names with . ( period ) character fail with an error *Bad Token . * __EoT__ `` ` The library throws error when the element names in xpath has period ( . ) in it . To reproduce the problem 1 . Create an XML Document with period in element names . Using jQuery you can create the document using the code var xml = `` < abc > < a.d > 12 < /a.d > < /abc > '' ; var xmlDoc = $ .parseXML ( xml ) ; wgxpath.install ( ) ; var result = document.evaluate ( `` /abc/a.d '' , xmlDoc , null , XPathResult.ANY_TYPE , null ) ; 2 . The same script runs on Chrome which provides native support for document.evaluate but not on IE11 using this library . It fails with an error SCRIPT5022 : Bad token : . File : wgxpath.install.js , Line : 47 , Column : 609 3 . I have created a fiddle http : //jsfiddle.net/r0gruz30/ which reproduces the problem . Open the fiddle in chrome and the output will be *Result is 12* but in IE the output is *Result is* and there is
Error reporting code should handle null/undefined values __EoT__ Example incorrect code that throws lf.Exception `` ` function testFoo ( ) { var schemaBuilder = lf.schema.create ( 'hr ' , 1 ) ; schemaBuilder.createTable ( 'Job ' ) . addColumn ( 'id ' , lf.Type.STRING ) . addPrimaryKey ( [ 'id ' ] ) ; schemaBuilder.createTable ( 'JobHistory ' ) . addColumn ( 'jobId ' , lf.Type.STRING ) . // Incorrect constraint name addForeignKey ( undefined , { local : 'jobId ' , // Incorrect 'ref ' format , should be 'Job.id' ref : 'Job ' , } ) ; schemaBuilder.getSchema ( ) ; } `` ` Currently an error is thrown while constructing the ` lf.Exception ` at https : //github.com/google/lovefield/blob/master/lib/exception.js # L47 . `` ` Can not read property 'toString ' of undefined `` ` If we change lf.Exception to expect null/undefined values , then a much more helpful error is surfaced to the user , `` ` http : //google.github.io/lovefield/error_lookup/src/error_lookup.html ? c=540 & p0=undefined `` ` This is related to PR https : //github.com/google/lovefield/pull/145 .
Python 3.4 import error __EoT__ I have installed gflags on python3.4 env . And after i attempt import gflags it raise `` No module named 'cStringIO ' `` It should by changed from io import StringIO Possibly update look like below ? from `` ` gflags/argument_parser.py import cStringIO import csv import string `` ` to `` ` gflags/argument_parser.py try : from io import StringIO as cStringIO except : import cStringIO # original code here import csv import string `` `
ChartJS 2.0 __EoT__ Hiya , Thanks for maintaining this . I was wondering if there are any plans to move to the ChartJS 2.0 now that it 's released ? It fixes a ton of bugs and adds some nice new features . Kind regards , Mo .
ChartJS 2.0 __EoT__ Hiya , Thanks for maintaining this . I was wondering if there are any plans to move to the ChartJS 2.0 now that it 's released ? It fixes a ton of bugs and adds some nice new features . Kind regards , Mo .
Caches get mixed up if two repo URLs use the same final path component . __EoT__ Here 's the culprit : https : //github.com/google/googet/blob/b08ef8d56b9ea4b29407146ad7952f54919cfeb4/client/client.go # L151 Example : googet addrepo first http : //myserver.com/googet/myrepo googet addrepo second gs : //my-bucket/googet/myrepo This will result in googet using C : \ProgramData\googet\cache\myrepo.rs as the cache for both repos . ` googet available ` will list both repos but list the set of packages from whichever repo was read most recently for both of them . The client has a unique name for each repository ( 'first ' and 'second ' in the example above ) . It should use that instead of the final path component . Note that the fix is easy and I 'm glad to submit a PR . However , the rollout could be tricky due to the potential of mixed-up caches . Doing something like giving the cache file a new extension ( .rs2 or something ) seems like the best way to avoid that . Please let me know if I should : a ) Please submit a PR ! b ) No PR necessary , we 'll fix it . c ) Too risky -- Let
Print a nice error message when dealing with a docker repo that has never run containers __EoT__ I happened to run into this while targeting a local repo ( which had never run any container , just downloaded an image ) $ sudo de.py -r /var/lib/docker list all_containers Could n't find any container configuration file ( ' { 0 : s } ' ) . Make sure the docker repository ( { 1 : s } ) is correct . If it is correct , you might want to run this script with higher privileges . Traceback ( most recent call last ) : File `` /usr/local/bin/de.py '' , line 4 , in < module > __import__ ( 'pkg_resources ' ) .run_script ( 'docker-explorer==20180612 ' , 'de.py ' ) File `` /usr/lib/python2.7/dist-packages/pkg_resources/__init__.py '' , line 658 , in run_script self.require ( requires ) [ 0 ] .run_script ( script_name , ns ) File `` /usr/lib/python2.7/dist-packages/pkg_resources/__init__.py '' , line 1445 , in run_script exec ( script_code , namespace , namespace ) File `` /usr/local/lib/python2.7/dist-packages/docker_explorer-20180612-py2.7.egg/EGG-INFO/scripts/de.py '' , line 304 , in < module > File `` /usr/local/lib/python2.7/dist-packages/docker_explorer-20180612-py2.7.egg/EGG-INFO/scripts/de.py '' , line 293 , in Main File `` /usr/local/lib/python2.7/dist-packages/docker_explorer-20180612-py2.7.egg/EGG-INFO/scripts/de.py '' , line 232 ,
Raise something more explicit when wrong Docker folder is passed __EoT__ Something along the lines : `` ` diff -- git a/docker_explorer/lib/container.py b/docker_explorer/lib/container.py index 5f926e1..5a7df25 100644 -- - a/docker_explorer/lib/container.py +++ b/docker_explorer/lib/container.py @ @ -81,8 +81,14 @ @ class Container ( object ) : container_info_json_path = os.path.join ( self.docker_directory , 'containers ' , container_id , self.container_config_filename ) - with open ( container_info_json_path ) as container_info_json_file : - container_info_dict = json.load ( container_info_json_file ) + try : + with open ( container_info_json_path ) as container_info_json_file : + container_info_dict = json.load ( container_info_json_file ) + except IOError as exception : + raise errors.BadContainerException ( + 'Unable to open configuration file { 0 } make sure you use the proper docker dir'.format ( + container_info_json_path ) + ) if container_info_dict is None : raise errors.BadContainerException ( `` `
Raise something more explicit when wrong Docker folder is passed __EoT__ Something along the lines : `` ` diff -- git a/docker_explorer/lib/container.py b/docker_explorer/lib/container.py index 5f926e1..5a7df25 100644 -- - a/docker_explorer/lib/container.py +++ b/docker_explorer/lib/container.py @ @ -81,8 +81,14 @ @ class Container ( object ) : container_info_json_path = os.path.join ( self.docker_directory , 'containers ' , container_id , self.container_config_filename ) - with open ( container_info_json_path ) as container_info_json_file : - container_info_dict = json.load ( container_info_json_file ) + try : + with open ( container_info_json_path ) as container_info_json_file : + container_info_dict = json.load ( container_info_json_file ) + except IOError as exception : + raise errors.BadContainerException ( + 'Unable to open configuration file { 0 } make sure you use the proper docker dir'.format ( + container_info_json_path ) + ) if container_info_dict is None : raise errors.BadContainerException ( `` `
Use file to configure processing pipeline __EoT__ We have a nice pipeline abstraction for defining at runtime how to process all of the NEL reports that are uploaded to the collector . Right now , you have to manually instantiate the particular processors you want to use via Go code . We want this to be configurable via a config file , so that we can have a single binary that works regardless of how you want to process and publish the reports .
Add confict supports on Database __EoT__ Right now , Agera does n't use any conflict actions on the database . Is this is a good idea for this ?
Request : Using concourse metadata in the Gerrit output message __EoT__ Concourse provides < a href= '' https : //concourse.ci/implementing-resources.html # resource-metadata '' > Metadata < /a > access to Resources in the form of environment variables . It would be neat if we could use these in the output messages for the Gerrit resource . Having the configuration look something like this : `` ` put : example-gerrit params : repository : example-gerrit message : CI passed ! http : //URL/teams/ $ BUILD_TEAM_NAME/pipelines/ $ BUILD_PIPELINE_NAME/jobs/ $ BUILD_JOB_NAME/builds/ $ BUILD_ID labels : { Verified : 1 } `` ` output the following message : `` ` CI passed ! http : //URL/teams/example_team/pipelines/example_pipleine/jobs/example_job/builds/1 `` ` Similar functionality has been added to the email resource < a href= '' https : //github.com/pivotal-cf/email-resource/pull/12 '' > here < /a >
Request : Using concourse metadata in the Gerrit output message __EoT__ Concourse provides < a href= '' https : //concourse.ci/implementing-resources.html # resource-metadata '' > Metadata < /a > access to Resources in the form of environment variables . It would be neat if we could use these in the output messages for the Gerrit resource . Having the configuration look something like this : `` ` put : example-gerrit params : repository : example-gerrit message : CI passed ! http : //URL/teams/ $ BUILD_TEAM_NAME/pipelines/ $ BUILD_PIPELINE_NAME/jobs/ $ BUILD_JOB_NAME/builds/ $ BUILD_ID labels : { Verified : 1 } `` ` output the following message : `` ` CI passed ! http : //URL/teams/example_team/pipelines/example_pipleine/jobs/example_job/builds/1 `` ` Similar functionality has been added to the email resource < a href= '' https : //github.com/pivotal-cf/email-resource/pull/12 '' > here < /a >
Missing comma in css-whitelist.json __EoT__ @ felix9 See https : //github.com/google/caja/commit/af1b007acca76df24e078aecc7d79ff8da3018b0 https : //github.com/google/caja/blob/master/src/com/google/caja/lang/css/css3-whitelist.json There 's a missing comma after ` z-index ` , before ` zoom `
Private templates should not be suggested __EoT__ Completion suggestions currently include private templates like `` ` { template foo_ visibility= '' private '' } ... { /template } `` ` This functionality should ideally be based on the ` visibility= '' private '' ` marked , but I think it might just be fine use the trailing underscore if easier to implement .
Make code Python3.6 compatible __EoT__
Update Podspec __EoT__ The current pod release , 2.1.1 , does not include changes made which allow GTM to be compatible with macOS 10.12 . Specifically , in ` GTMSystemVersion.m ` the change from ` require_noerr ` to ` __Require_noErr ` . Please create a new pod version which includes the ( more ) recent changes so GTM can be built against the latest macOS SDK .
Update Podspec __EoT__ The current pod release , 2.1.1 , does not include changes made which allow GTM to be compatible with macOS 10.12 . Specifically , in ` GTMSystemVersion.m ` the change from ` require_noerr ` to ` __Require_noErr ` . Please create a new pod version which includes the ( more ) recent changes so GTM can be built against the latest macOS SDK .
Uncaught exception for garbage timestamp for windows images __EoT__ Garbage timestamp values in arrow ( 0.10 ) throws OSError [ Errno 22 ] when garbage value timestamp is provided . Screenshot below : ! [ image ] ( https : //user-images.githubusercontent.com/16841220/49343080-82c98f00-f689-11e8-92f7-af02c6e81800.png ) This can be reproduced by running netscan plugin on windows images . For the connections which are in closed state , memory has garbage value of time . In this case the value input to function `` ` arrow.Arrow.utcfromtimestamp `` ` method was `` ` 731972518059 `` ` for a closed state connection causing it to crash . Will submit a PR for fixing the same . Environment : 1 . Version of rekall 1.7.2.rc1 2 . Operating system used to run rekall : Windows 10 3 . Version of python : 3.7.1 4 . OS of memory image : live mode 5 . Complete Command : `` ` rekall -- live Memory netscan `` `
Maven support for Java module __EoT__ Is adding support for Maven for Java module is in scope for this project ? That should greatly help adopt this library easily by users . I can help contribute the initial version if you going to PR .
Nuget __EoT__ Good afternoon , I 'd love to be able to pull in the C # version of this library into a project I am working on , but see no way to do that via the package ecosystem of .NET/Visual Studio ( e.g . Nuget . ) Would the recommended way currently be to simply clone this repo and pull down the C # file we need ?
Error message formatting __EoT__
Setup npm package __EoT__ - confirm package.json is formed correctly - add to npm registry - fix how to use section with docs
wire : generate failed could not import `` package '' permission denied __EoT__ When ever I run wire the genration fails Here is the output : `` ` Tareks-MacBook-Pro : backend tarekkma $ wire ./di /Users/tarekkma/Developer/Projects/go/pkg/mod/github.com/spf13/afero @ v1.1.2/util.go:28:2 : could not import golang.org/x/text/transform ( go/build : importGo golang.org/x/text/transform : exit status 1 go : finding golang.org/x/text/transform latest go : open /Users/tarekkma/Developer/Projects/go/pkg/mod/github.com/spf13/afero @ v1.1.2/go.mod : permission denied ) /Users/tarekkma/Developer/Projects/go/pkg/mod/github.com/spf13/afero @ v1.1.2/util.go:29:2 : could not import golang.org/x/text/unicode/norm ( go/build : importGo golang.org/x/text/unicode/norm : exit status 1 go : finding golang.org/x/text/unicode/norm latest go : finding golang.org/x/text/unicode latest go : open /Users/tarekkma/Developer/Projects/go/pkg/mod/github.com/spf13/afero @ v1.1.2/go.mod : permission denied ) wire : generate failed `` ` System Information `` ` go version go1.11.1 darwin/amd64 GOARCH= '' amd64 '' GOBIN= '' /Users/tarekkma/Developer/Projects/go/bin '' GOCACHE= '' /Users/tarekkma/Library/Caches/go-build '' GOEXE= '' '' GOFLAGS= '' '' GOHOSTARCH= '' amd64 '' GOHOSTOS= '' darwin '' GOOS= '' darwin '' GOPATH= '' /Users/tarekkma/Developer/Projects/go '' GOPROXY= '' '' GORACE= '' '' GOROOT= '' /usr/local/go '' GOTMPDIR= '' '' GOTOOLDIR= '' /usr/local/go/pkg/tool/darwin_amd64 '' GCCGO= '' gccgo '' CC= '' clang '' CXX= '' clang++ '' CGO_ENABLED= '' 1 '' GOMOD= '' /Users/tarekkma/Developer/biz/****/backend/go.mod '' GOROOT/bin/go version : go version go1.11.1 darwin/amd64 GOROOT/bin/go tool compile -V
gowire reports `` can not find package '' for various pkgs after running vgo mod -vendor __EoT__ Here 's my session . vgo installs for example fsnotify , but then gowire does n't find it . `` ` issactrotts-macbookpro : ~ issactrotts $ git clone git @ github.com : google/go-cloud.git issactrotts-macbookpro : ~ issactrotts $ cd go-cloud issactrotts-macbookpro : go-cloud issactrotts $ vgo install ./wire/cmd/gowire issactrotts-macbookpro : go-cloud issactrotts $ cd samples/guestbook/ issactrotts-macbookpro : guestbook issactrotts $ vgo mod -vendor go : downloading github.com/dnaeon/go-vcr v0.0.0-20180504081357-f8a7e8b9c630 go : downloading github.com/stretchr/testify v1.2.1 go : downloading github.com/go-sql-driver/mysql v0.0.0-20180308100310-1a676ac6e4dc go : downloading github.com/census-ecosystem/opencensus-go-exporter-aws v0.0.0-20180411051634-41633bc1ff6b go : downloading github.com/GoogleCloudPlatform/cloudsql-proxy v0.0.0-20180321230639-1e456b1c68cb go : downloading github.com/pmezard/go-difflib v1.0.0 go : downloading github.com/fsnotify/fsnotify v1.4.7 go : downloading github.com/gorilla/mux v1.6.1 go : downloading github.com/davecgh/go-spew v1.1.0 go : downloading github.com/gorilla/context v1.1.1 go : downloading google.golang.org/appengine v1.1.0 go : downloading gopkg.in/ini.v1 v1.37.0 go : downloading github.com/aws/aws-xray-sdk-go v1.0.0-rc.5 go : downloading golang.org/x/sys v0.0.0-20180329131831-378d26f46672 go : downloading gopkg.in/yaml.v2 v2.2.1 go : downloading github.com/google/go-cmp v0.2.0 go : downloading github.com/smartystreets/goconvey v0.0.0-20180222194500-ef6db91d284a go : downloading contrib.go.opencensus.io/exporter/stackdriver v0.0.0-20180421005815-665cf5131b71 go : downloading gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405 go : downloading github.com/jtolds/gls v0.0.0-20170503224851-77f18212c9c7 go : downloading github.com/golang/glog v0.0.0-20160126235308-23def4e6c14b go : downloading golang.org/x/sync v0.0.0-20180314180146-1d60e4601c6f go : downloading github.com/gopherjs/gopherjs v0.0.0-20180424202546-8dffc02ea1cb go : downloading github.com/smartystreets/assertions v0.0.0-20180301161246-7678a5452ebe
Cause the Homebrew build to use newer source __EoT__ I 'm using the Guetzli build with Homebrew . When compressing smaller images I 'm getting a segmentation fault . The file in question is 419 bytes , and can be found at [ https : //www.coastercms.org/themes/coaster/img/line.jpg ] ( url ) .
Build errors/warnings . __EoT__ `` ` In file included from /usr/local/include/gflags/gflags.h:84 : /usr/local/include/gflags/gflags_declare.h:43:9 : warning : 'GFLAGS_NAMESPACE ' macro redefined [ -Wmacro-redefined ] # define GFLAGS_NAMESPACE gflags ^ < command line > :1:9 : note : previous definition is here # define GFLAGS_NAMESPACE google ^ output_image.cc guetzli/output_image.cc:206:5 : error : use of undeclared identifier 'exit' exit ( 1 ) ; ^ `` ` On FreeBSD 11 .
tools/fast_importer should take a compression_level argument __EoT__ Right now , it defaults to compression level 9 ( in ` v2_2_image.FromTarball ` ) which is terribly slow . A more reasonable default or a configurable level will make it more usable .
container_pull authentication failure __EoT__ I 'm having an issue with ` container_pull ` from private Docker Registry based on Sonatype OSS Nexus Repository Manager : `` ` containerregistry.client.v2_2.docker_http_.BadStateException : Unexpected `` www-authenticate '' challenge type : BASIC ( /usr/bin/python /private/var/tmp/_bazel_eric/35e4f788de46e2946a456376a62aa566/external/puller/file/puller.par -- directory /private/var/tmp/_bazel_eric/35e4f788de46e2946a456376a62aa566/external/base/image -- name suxen.vipdmp.com/com.veon/jvm-builder @ sha256 : e3aefc80adf5c8d403a1887d55f53c54ded0cdaf3b1c3e95bb861526b9cb515f ) . `` ` Looking at Python source code , I see , that it only accepts ` Basic ` challenge type and not ` BASIC ` I suppose the check should be case-insensitive .
` from __future__ ` in the main file causes SyntaxError __EoT__ If the main python script contains ` from __future__ import ... ` statements , running par file causes the following error : `` ` $ ./train.par Traceback ( most recent call last ) : File `` /usr/lib/python2.7/runpy.py '' , line 165 , in _run_module_as_main mod_name , loader , code , fname = _get_main_module_details ( _Error ) File `` /usr/lib/python2.7/runpy.py '' , line 133 , in _get_main_module_details return _get_module_details ( main_name ) File `` /usr/lib/python2.7/runpy.py '' , line 119 , in _get_module_details code = loader.get_code ( mod_name ) File `` ./train.par/__main__.py '' , line 74 SyntaxError : from __future__ imports must occur at the beginning of the file `` ` This seems to be due to the boilerplate added by subpar compiler before the ` __future ` statements . : `` ` # Boilerplate added by subpar/compiler/python_archive.py from subpar.runtime import support as _ _.setup ( import_roots= [ u'__main__ ' , u'six_archive ' ] ) del _ # End boilerplate `` `
gradle sync failing __EoT__ `` Error : Conflict with dependency 'com.google.errorprone : error_prone_annotations ' in project 'mobly-bundled-snippets ' . Resolved versions for app ( 2.0.18 ) and test app ( 2.0.19 ) differ . See http : //g.co/androidstudio/app-test-app-conflict for details . '' Adding this seems to work around it : ` compile 'com.google.errorprone : error_prone_annotations:2.0.19 ' ` Not sure what 's going on
gradle sync failing __EoT__ `` Error : Conflict with dependency 'com.google.errorprone : error_prone_annotations ' in project 'mobly-bundled-snippets ' . Resolved versions for app ( 2.0.18 ) and test app ( 2.0.19 ) differ . See http : //g.co/androidstudio/app-test-app-conflict for details . '' Adding this seems to work around it : ` compile 'com.google.errorprone : error_prone_annotations:2.0.19 ' ` Not sure what 's going on
rate limit the API call __EoT__ It would be nice to have rate limiter before making a call to API if it is in the local cache .
Create manpage ( s ) __EoT__ Forked off of https : //github.com/google/google-authenticator-libpam/issues/58
Please make it possible to run tests on local jars __EoT__ Current wycherproof tries to download BouncyCastle from the Internet . I need wycherproof to run tests on the local BouncyCastle jars . How can I do it ?
JUnit3 runner is being used instead of JUnit4 __EoT__ The JUnit3 runner is being used instead of JUnit4 . This causes the annotations ( Slow/Fast ) to be ignored . I have a fix and will submit a pull request shortly . There was a [ timeout issue ] ( https : //groups.google.com/forum/ # ! topic/wycheproof-users/usPJedwMPkw ) reported on the mailing list in Feb . It was identified then that SlowTest annotations were being ignored . I believe this was likely the cause of that issue . **Repro Steps : ** - In junit-4.12.jar ! /org/junit/internal/builders/AllDefaultPossibilitiesBuilder.class , set a breakpoint in the ` return runner ` line of the runnerForClass method - Set a breakpoint inside of testWrappedAroundCounter ( a SlowTest ) - Run a test with debugger attached i.e . bazel test -- java_debug -- genrule_strategy=standalone -- spawn_strategy=standalone BouncyCastleTest_1_57 **Actual Result : ** - The breakpoint will show JUnit38ClassRunner being initialized - The breakpoint in testWrappedAroundCounter will be hit **Expected Result : ** - A JUnit4 runner is initialized - testWrappedAroundCounter does **not** run during a 'Fast ' test **Fix : ** - Remove ` extends TestCase ` which triggers usage of the JUnit3 runner - Replace ` import
Repeated input values for primality test __EoT__ I noticed some values are checked more than once by the primality test , and do n't see any obvious reason for the test to repeat for just those inputs . The values 164280218643672633986221 , 318665857834031151167461 , and 360681321802296925566181 all appear multiple times in the test data in https : //github.com/google/wycheproof/blob/master/java/com/google/security/wycheproof/testcases/BigIntegerTest.java Maybe they can be removed , or maybe there should be a comment explaining the duplication . Thanks for this great set of tests .
Copy-paste mistake in EcdhTest.java __EoT__ In https : //github.com/google/wycheproof/blob/master/java/com/google/security/wycheproof/testcases/EcdhTest.java # L867 in method testDecode ( ) there appears to be a copy-paste mistake : ` ECParameterSpec params2 = key1.getParams ( ) ; // should be key2 ` This makes the following asserts to always be true .
Camera preview not working as fullscreen in Nexus 6P with Nougat OS __EoT__ I have raised this issue here in this thread : # 85 But as it is suggested to start new thread , I have created this one . @ ataulm @ rajeshct I want you to participate in this and help me to resolved this issue . The issue is that , I am not able to see the preview in Nexus 6P with Nougat OS ( Here I want to see the preview in full screen only ) . Earlier I have made my camera layout as follow to make it full screen : `` ` < include layout= '' @ layout/include_camera '' android : layout_width= '' match_parent '' android : layout_height= '' match_parent '' / > `` ` With this , it was working fine in all the device except Nexus 6p with Nougat OS . After that , I have updated the library and check for the issue . Here the Issue is get resolved but I need to make camera layout as per below code and also need to add AspectRatio to the Camera preview to display in that ratio . `` `
nil slice/map should compare equal to empty slice/map __EoT__ The documentation states : Slices and arrays are equal if they have the same length and the elements at each index are equal . Maps are equal if their keys are exactly equal ( according to the == operator ) and the corresponding elements for each key are equal . I read this as implying that this program would print ` true true ` , but instead it prints ` false false ` . package main import ( '' github.com/google/go-cmp/cmp '' '' log '' ) func main ( ) { log.Println ( cmp.Equal ( map [ int ] string { } , map [ int ] string ( nil ) ) , cmp.Equal ( [ ] int { } , [ ] int ( nil ) ) , ) }
Add AcyclicTransformer that does not recursively apply to itself in an infinite cycle __EoT__ Despite the efforts from https : //github.com/google/go-cmp/pull/29 and related , the below code will still cause an infinite recursion , if no Filter is provided . Is this expected ? If yes , I think the removed comments referring to this issue ( e.g . https : //github.com/google/go-cmp/pull/29/commits/c2186a466cf8bf4fd66b9fbc17f3be426bb08e0f # diff-80edd9c1cccda07a46cde07a99b4e236L261 ) should be re-added . `` ` go package main import ( '' fmt '' '' github.com/google/go-cmp/cmp '' '' strings '' ) func main ( ) { a : = `` foo bar '' b : = `` baz qux '' fmt.Printf ( `` % v '' , cmp.Equal ( a , b , cmp.Transformer ( `` '' , func ( in string ) [ ] string { return strings.Split ( in , `` `` ) } ) ) ) } `` ` `` ` $ go run cmptest.go runtime : goroutine stack exceeds 1000000000-byte limit fatal error : stack overflow runtime stack : runtime.throw ( 0x4fc7d2 , 0xe ) /usr/lib/google-golang/src/runtime/panic.go:622 +0x8a runtime.newstack ( ) /usr/lib/google-golang/src/runtime/stack.go:1054 +0x71f runtime.morestack ( ) /usr/lib/google-golang/src/runtime/asm_amd64.s:480 +0x89 goroutine 1 [ running ] : sync . ( *Map ) .Load ( 0x5973c0
Incorrect gradient calculation __EoT__ Hi , I 've been experimenting with this library for a few months now and really like the capabilities present . I 'm working to develop an AD capability via code generation for a set of aerospace engineering codes in Python . However , I think I 've run into either a bug or a usage misunderstanding . Consider the following stand-alone function , which takes several parameters and returns a scalar : `` ` import tangent BTU_s2HP , HP_per_RPM_to_FT_LBF = 1.4148532 , 5252.11 def enthalpyandpower ( W_in , W_out , ht_in , ht_out_ideal , eff , Nmech , b1_W , b1_ht , b1_ht_ideal ) : ht_out = W_in/W_out * ( ht_in * ( 1.0 - eff ) + ht_out_ideal * eff ) power = W_in * eff * ( ht_in - ht_out_ideal ) * BTU_s2HP ht_out += b1_W / W_out * \ ( b1_ht * ( 1.0 - eff ) + b1_ht_ideal * eff ) power += b1_W * eff * \ ( b1_ht - b1_ht_ideal ) * BTU_s2HP # calculate torque based on revised power and shaft speed trq = power / \ Nmech * HP_per_RPM_to_FT_LBF return power `` ` If I generate the
Builds break with astor 0.6.1 __EoT__ Filed https : //github.com/berkerpeksag/astor/issues/87
gradients are inconsistently vectorized __EoT__ Sometimes gradients are vectorized , and sometimes they are not . Consider this example where the gradient is vectorized ( ie . array in , array of derivatives out ) . `` ` def f ( x ) : return x**2 df = tangent.grad ( f ) print ( df ( np.array ( [ 0 , 1 , 2 ] ) ) ) `` ` This comes out like I would expect . : RESULTS : [ 0 . 2 . 4 . ] : END : Compare it to this : `` ` def f1 ( x ) : return x + 2.0 * np.cos ( x ) # df/dx = 1 - 2*sin ( x ) df1 = tangent.grad ( f1 ) x = np.array ( [ 0.0 , 1.0 , 2.0 ] ) print ( df1 ( x ) ) # It is not clear this is even correct . print ( 1 - 2 * np.sin ( x ) ) # A vectorized version df1v = np.vectorize ( tangent.grad ( f1 ) ) print ( df1v ( np.array ( [ 0 , 1 , 2 ] ) ) ) `` `
UnicodeDecodeError : 'gbk ' codec ca n't decode byte 0x9d in position 6304 : illegal multibyte sequence __EoT__ Environment : Windows 7 Python : 3.6.2 pip installation failed . `` ` Collecting tangent Using cached tangent-0.1.0.tar.gz Complete output from command python setup.py egg_info : Traceback ( most recent call last ) : File `` < string > '' , line 1 , in < module > File `` C : \Users\ADMINI~1\AppData\Local\Temp\pip-build-k2pei7vz\tangent\setu p.py '' , line 5 , in < module > readme = f.read ( ) UnicodeDecodeError : 'gbk ' codec ca n't decode byte 0x9d in position 6304 : ill egal multibyte sequence -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Command `` python setup.py egg_info '' failed with error code 1 in C : \Users\ADMINI~1 \AppData\Local\Temp\pip-build-k2pei7vz\tangent\ `` `
New ( pypi ) release __EoT__ Hi , it would be nice to have a new release , as 1.1.1 available from pypi does not include # 5 which makes it hard to install for https : //github.com/happyleavesaoc/python-firetv ( forcing pip install against git master in principle ) . Thanks in advance !
TypeError : 'in < string > ' requires string as left operand , not bytes __EoT__ **Steps to reproduce : ** 1 . ` pip install adb pycryptodome rsa ` 2 . Run the following Python commands : `` ` python from adb import adb_commands from adb.sign_pythonrsa import PythonRSASigner from adb.adb_protocol import InvalidChecksumError serial = '192.168.0.74:5555' adbkey = '/home/jeff/.android/adbkey' signer = PythonRSASigner.FromRSAKeyPath ( adbkey ) conn = adb_commands.AdbCommands ( ) .ConnectDevice ( serial=serial , rsa_keys= [ signer ] ) `` ` **Error message : ** `` ` -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - TypeError Traceback ( most recent call last ) < ipython-input-1-85eb2f5c0fc0 > in < module > ( ) 8 signer = PythonRSASigner.FromRSAKeyPath ( adbkey ) 9 -- - > 10 conn = adb_commands.AdbCommands ( ) .ConnectDevice ( serial=serial , rsa_keys= [ signer ] ) ~/.virtualenvs/adb/lib/python3.5/site-packages/adb/adb_commands.py in ConnectDevice ( self , port_path , serial , default_timeout_ms , **kwargs ) 128 if 'handle ' in kwargs : 129 self._handle = kwargs.pop ( 'handle ' ) -- > 130
TypeError : 'in < string > ' requires string as left operand , not bytes __EoT__ **Steps to reproduce : ** 1 . ` pip install adb pycryptodome rsa ` 2 . Run the following Python commands : `` ` python from adb import adb_commands from adb.sign_pythonrsa import PythonRSASigner from adb.adb_protocol import InvalidChecksumError serial = '192.168.0.74:5555' adbkey = '/home/jeff/.android/adbkey' signer = PythonRSASigner.FromRSAKeyPath ( adbkey ) conn = adb_commands.AdbCommands ( ) .ConnectDevice ( serial=serial , rsa_keys= [ signer ] ) `` ` **Error message : ** `` ` -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - TypeError Traceback ( most recent call last ) < ipython-input-1-85eb2f5c0fc0 > in < module > ( ) 8 signer = PythonRSASigner.FromRSAKeyPath ( adbkey ) 9 -- - > 10 conn = adb_commands.AdbCommands ( ) .ConnectDevice ( serial=serial , rsa_keys= [ signer ] ) ~/.virtualenvs/adb/lib/python3.5/site-packages/adb/adb_commands.py in ConnectDevice ( self , port_path , serial , default_timeout_ms , **kwargs ) 128 if 'handle ' in kwargs : 129 self._handle = kwargs.pop ( 'handle ' ) -- > 130
Support on Mac OS __EoT__
Support on Mac OS __EoT__
Fairplay support patch __EoT__ **Shaka Packager Version** : b451d3a7ca6f454b4905043d56dd9620247d8c71 **Packager Command** : `` ` ./packager \ 'input=udp : //127.0.0.1:11000 , stream=3 , segment_template=fairplay_video- $ Number $ .ts , playlist_name=video.m3u8 ' \ 'input=udp : //127.0.0.1:11000 , stream=0 , segment_template=fairplay_audio1- $ Number $ .ts , playlist_name=audio1.m3u8 , hls_group_id=audio1 , hls_name=EN ' \ -- enable_fixed_key_encryption \ -- clear_lead 0 \ -- key ... \ -- key_id ... \ -- iv ... \ -- hls_playlist_type LIVE \ -- hls_master_playlist_output= '' playlist.m3u8 '' `` ` I would like to properly patch shaka-packager to produce SAMPLE-AES crypted content . The end result is having this kind of header in HLS chunklist : `` ` # EXT-X-KEY : METHOD=SAMPLE-AES , URI= '' skd : //entry '' , KEYFORMAT= '' com.apple.streamingkeydelivery '' , KEYFORMATVERSIONS= '' 1 '' `` ` instead of this one which requires transfering key in clear format : `` ` # EXT-X-KEY : METHOD=SAMPLE-AES , URI= '' data : text/plain ; base64 , ... '' , IV=0x ... , KEYFORMAT= '' identity '' `` ` I already have it working by hardcoding the header into media_playlist . I would like to submit a proper patch for this . Any pointers on how should I approach
Maven plugin __EoT__ Support using Google Java Format as part a build in Maven thru a plugin . Google coding styles helps a great deal to apply a common scheme to a code repository . Although , there are ways to share/automate workspace settings , it essentially relies on developer to follow and apply the coding styles . As a build plugin , the formatting of the source can be applied before any commit , during builds on CI/CD servers and other applications .
EPL license formatting __EoT__ In Eclipse Che I 'm trying to adapt Google Java Style Guide https : //github.com/eclipse/che/issues/5772 As a part of this issue , I want to reformat all our codebase with google-java-format tool . One of the issues I have is how EPL license looks like after formatting . Before https : //eclipse.org/legal/copyrightandlicensenotice.php `` ` /******************************************************************************* * Copyright ( c ) 2012-2017 Codenvy , S.A. * All rights reserved . This program and the accompanying materials * are made available under the terms of the Eclipse Public License v1.0 * which accompanies this distribution , and is available at * http : //www.eclipse.org/legal/epl-v10.html * * Contributors : * Codenvy , S.A. - initial API and implementation *******************************************************************************/ `` ` After `` ` /** * ***************************************************************************** Copyright ( c ) * 2012-2017 Codenvy , S.A. All rights reserved . This program and the accompanying materials are made * available under the terms of the Eclipse Public License v1.0 which accompanies this distribution , * and is available at http : //www.eclipse.org/legal/epl-v10.html * * < p > Contributors : Codenvy , S.A. - initial API and implementation * ***************************************************************************** `` ` Can you suggest me a better way to
Versioning in GoogleJavaFormatVersion.java does not reflect real version . __EoT__ In commit [ 833201c ] ( https : //github.com/google/google-java-format/commit/833201c903f90d272989198f197b106c7a14fea3 ) the version was changed to 1.0 and has been static since then . This seems a bit odd with version 1.4 being released afterwards . If it needs to be this way for the intellij plugin to function , that is fine . But I thought it should be brought to your attention . Sincerely , Michael Stergianis
cleanup : Expose ForwardingFileSystem publicly __EoT__ ` ForwardingFileSystem ` already exists and is the basis of ` ChrootFileSystem ` and ` LocalFileSystem ` . We probably should expose it publicly .
RecordingFileSystem & ReplayFileSystem __EoT__ ` RecordingFileSystem ` will delegate to an underlying ` FileSystem ` , recording all activity that passes through it . ` ReplayFileSystem ` will take a serialized recording and respond to matching calls with recorded info ( and throw for calls not in the recording ) . This will enable record/replay tests of code that accesses the file system .
RecordingFileSystem & ReplayFileSystem __EoT__ ` RecordingFileSystem ` will delegate to an underlying ` FileSystem ` , recording all activity that passes through it . ` ReplayFileSystem ` will take a serialized recording and respond to matching calls with recorded info ( and throw for calls not in the recording ) . This will enable record/replay tests of code that accesses the file system .
RecordingFileSystem & ReplayFileSystem __EoT__ ` RecordingFileSystem ` will delegate to an underlying ` FileSystem ` , recording all activity that passes through it . ` ReplayFileSystem ` will take a serialized recording and respond to matching calls with recorded info ( and throw for calls not in the recording ) . This will enable record/replay tests of code that accesses the file system .
Enable Windows testing __EoT__
libusb.free ( ) race __EoT__ libusb.free ( ) frees the libusbTransfer pointer and only then removes it from xferDoneMap ( under a lock ) . If libusb.alloc ( ) is called from another goroutine in between these two operations , it might allocate the same pointer that was just freed . The new element might then be removed from xferDoneMap prematurely , resulting in a deadlock when xferCallback tries to notify a nil channel . I believe changing the order in free ( ) should fix this .
libusb.free ( ) race __EoT__ libusb.free ( ) frees the libusbTransfer pointer and only then removes it from xferDoneMap ( under a lock ) . If libusb.alloc ( ) is called from another goroutine in between these two operations , it might allocate the same pointer that was just freed . The new element might then be removed from xferDoneMap prematurely , resulting in a deadlock when xferCallback tries to notify a nil channel . I believe changing the order in free ( ) should fix this .
Feature Request : Cancellable API __EoT__ Has there been thought put into an Async API ? I 'd like to be able to queue an RX transaction and either receive notification or the packet itself on a channel .
syntax : support multiprecision integer literals __EoT__ Currently the scanner refuses to parse integer literals that ca n't be represented as ` int64 ` : `` ` $ skylark Welcome to Skylark ( github.com/google/skylark ) > > > 1234567890123456789 1234567890123456789 > > > 12345678901234567890 ... < stdin > :1:1 : invalid int literal `` ` The solution is to change the syntax.Literal.Value to allow ` string ` , ` int64 ` , or ` *big.Int ` , and change the interpreter accordingly . Also , the comment next to the Value field is wrong : it says int , not int64 .
hashtable : sprinkle per-table salt into hash ( x ) before choosing a bucket __EoT__ Currently the hash table used by dict and set uses ` slot = hash ( x ) % len ( table ) ` to choose a bucket . This is vulnerable to hash flooding , a denial of service attack in in which the adversary chooses values of x that result in the same slot , causing the table to degenerate to a single bucket and server performance to suffer . To mitigate this , we 'll need to change the signature of ` Value.Hash ` to accept a seed value , and change implementations to incorporate this seed value into their result in an order-dependent manner , such as multiplication but not simply XOR . Each table will need to provide a random seed value , created during table initialization and constant for the life of each table , to ` hash ( x , seed ) ` . It is not sufficient to merely incorporate the seed value after computing ` hash ( x ) ` . This will not change the semantics of the hash table observed by the Skylark program ;
Unpack { Positional } Args with custom Value types __EoT__ UnpackPositionalArg seems to not work if I have a custom type . What should I do ? Make my own UnpackArgs with my custom type ( s ) ? Use of reflect ? Or some kind of Unpacker struct that lets me register the extra types ?
Remove ` IPython ` dependency from ` fire.Fire ` or improve load times some other way __EoT__ ` import IPython ` takes about 0.5 seconds on my machine to import and is the longest part of running simple ` fire ` scripts at the moment . Launching a python script and printing something takes 0.15 seconds so the total with ` import fire ` becomes about 0.65 seconds . It 's only used in ` _EmbedIPython ` and ` inspectutils.py ` so maybe it can be optional for command line apps and the inspection part ported over ( or placed in a different smaller package ) ? Would such a pull request be useful ?
travis ci failing with advent of ipython 6.0.0rc1 __EoT__ ipython 6.0.0rc1 was released 7 hours ago : * https : //github.com/ipython/ipython/releases Now when building fire , travis CI finds that version , which does n't support Python 2.7 , and complains , e.g . at https : //travis-ci.org/nealmcb/python-fire/jobs/220816890 `` ` ... Processing dependencies for fire==0.1.0 Searching for ipython Reading https : //pypi.python.org/simple/ipython/ Downloading https : //pypi.python.org/packages/76/46/c505c48d264715136edde1a1960f9158133e34c03bbfe4c955dbc5948ce4/ipython-6.0.0rc1.tar.gz # md5=e57acb5fabd9f18b739de35a1f00eac5 Best match : ipython 6.0.0rc1 Processing ipython-6.0.0rc1.tar.gz Writing /tmp/easy_install-Aw9UOD/ipython-6.0.0rc1/setup.cfg Running ipython-6.0.0rc1/setup.py -q bdist_egg -- dist-dir /tmp/easy_install-Aw9UOD/ipython-6.0.0rc1/egg-dist-tmp-JVscrS IPython 6.0+ does not support Python 2.6 , 2.7 , 3.0 , 3.1 , or 3.2 . ... error : Setup script exited with 1 The command `` python setup.py develop '' failed and exited with 1 during . `` ` Time for a dependency change . For a lot of projects ... .
RenderPassLoadOpTests.ColorClearThenLoadAndDraw/OpenGL fails __EoT__ RenderPassLoadOpTests.ColorClearThenLoadAndDraw/OpenGL ( to be introduced in # 107 ) currently fails on the OpenGL backend . Need to investigate .
RenderPassLoadOpTests.ColorClearThenLoadAndDraw/OpenGL fails __EoT__ RenderPassLoadOpTests.ColorClearThenLoadAndDraw/OpenGL ( to be introduced in # 107 ) currently fails on the OpenGL backend . Need to investigate .
Depth Stencil State roadmap __EoT__ - [ X ] Implement depth stencil state - [ X ] Make changes to next.json and state-tracking and null backend - [ X ] Implement on the OpenGL backend - [ X ] Implement on the Metal backend - [ X ] Add tests for depth stencil state - [ X ] Add validation tests - [ X ] For DepthStencilState creation # 26 - [ X ] ~~That it can not be used on a compute pipeline ( we are probably going to split the graphics and compute pipelines types down the line ) ~~ graphics and compute split # 80 - [ x ] Add end2end tests # 92 - [ ] Write documentation - [ ] Add a description of the API in the NXT doc - [ ] Write an investigation and explanation of why we designed the API this way .
Depth Stencil State roadmap __EoT__ - [ X ] Implement depth stencil state - [ X ] Make changes to next.json and state-tracking and null backend - [ X ] Implement on the OpenGL backend - [ X ] Implement on the Metal backend - [ X ] Add tests for depth stencil state - [ X ] Add validation tests - [ X ] For DepthStencilState creation # 26 - [ X ] ~~That it can not be used on a compute pipeline ( we are probably going to split the graphics and compute pipelines types down the line ) ~~ graphics and compute split # 80 - [ x ] Add end2end tests # 92 - [ ] Write documentation - [ ] Add a description of the API in the NXT doc - [ ] Write an investigation and explanation of why we designed the API this way .
Depth Stencil State roadmap __EoT__ - [ X ] Implement depth stencil state - [ X ] Make changes to next.json and state-tracking and null backend - [ X ] Implement on the OpenGL backend - [ X ] Implement on the Metal backend - [ X ] Add tests for depth stencil state - [ X ] Add validation tests - [ X ] For DepthStencilState creation # 26 - [ X ] ~~That it can not be used on a compute pipeline ( we are probably going to split the graphics and compute pipelines types down the line ) ~~ graphics and compute split # 80 - [ x ] Add end2end tests # 92 - [ ] Write documentation - [ ] Add a description of the API in the NXT doc - [ ] Write an investigation and explanation of why we designed the API this way .
Unmarshalling an array of objects ? __EoT__ If I 've got an array back as my Json body , and I want to unmarshal that into an array of structs , it appears , according to your docs , that I ca n't do this . Is this feature coming soon ?
New vim-job based CallAsync does n't handle complex commands correctly __EoT__ It looks like the new job version of CallAsync is breaking up commands incorrectly . Example of broken behavior : let g : command = maktaba # syscall # Create ( [ 'echo ' , 'a ' ] ) .And ( [ 'echo ' , 'b ' ] ) g : command.Call ( and CallAsync with vim jobs disabled ) gives the correct stdout of `` a\nb '' with vim jobs , g : command.CallAsync gives the incorrect stdout of `` a & & echo b ''
Add helper to get current visual selection __EoT__ Plugins sometimes need to grab highlighted text , e.g . to implement visual-mode mappings that do something with highlighted text , but vim does n't seem to have any convenient support built in . Maktaba could have a helper like ` maktaba # buffer # GetVisualSelection ( ) ` that returns the text between the ` < ` and ` > ` marks .
Logger.Warn and above should display incoming messages to user __EoT__ If a plugin logs a warning or error to maktaba 's logging mechanism , it should also be displayed to the user using echomsg . The alternative would be for plugin authors to both log *and* display errors , but that 's less convenient and not configurable . Also users could end up seeing duplicate warnings if there 's confusion about who 's responsible for displaying the errors .
pkg/csource : broken on openbsd __EoT__ pkg/csource tests fail on openbsd ( even for openbsd target ) , I 've disabled them for now : https : //github.com/google/syzkaller/commit/fd85ed48854729938fad986fc81e1c57a667fb36 pkg/csource is required to generate reproducers , need to fix it and re-enable tests ( at least for openbsd target ) . First it complains that cpp does not have -fdirectives-only flag . But even if I remove it still produces lots of errors : `` ` < stdin > :5:2 : error : implicit declaration of function 'kill ' is invalid in C99 [ -Werror , -Wimplicit-function-declaration ] kill ( pid , SIGKILL ) ; ^ < stdin > :5:12 : error : use of undeclared identifier 'SIGKILL' kill ( pid , SIGKILL ) ; ^ < stdin > :6:9 : error : implicit declaration of function 'waitpid ' is invalid in C99 [ -Werror , -Wimplicit-function-declaration ] while ( waitpid ( -1 , status , 0 ) ! = pid ) { ^ < stdin > :10:22 : error : unknown type name 'uint64_t' static void sleep_ms ( uint64_t ms ) ^ < stdin > :12:2 : error : implicit declaration of function 'usleep ' is invalid in C99 [
pkg/csource : do n't generate syscall defines for openbsd __EoT__ Our first openbsd reproducer ( woohoo ! ) contains : `` ` # ifndef SYS_connect # define SYS_connect 98 # endif # ifndef SYS_getsockopt # define SYS_getsockopt 118 # endif # ifndef SYS_mmap # define SYS_mmap 197 # endif # ifndef SYS_socket # define SYS_socket 97 # endif `` ` https : //syzkaller.appspot.com/x/repro.c ? x=117e5733400000 I suspect this may be unnecessary for OpenBSD . Emission of these defines is controlled in sys/targets/targets.go by NeedSyscallDefine predicate . This stems from linux where user-space may not contain defines for the latest syscalls : `` ` NeedSyscallDefine : func ( nr uint64 ) bool { // Only generate defines for new syscalls // ( added after commit 8a1ab3155c2ac on 2012-10-04 ) . return nr > = 313 } , `` ` We can also limit this for OpenBSD , or just disable with : `` ` NeedSyscallDefine : dontNeedSyscallDefine , `` ` @ blackgnezdo @ mptre
Subject.isEqualTo ( ) does n't work with arrays __EoT__ Subject # isEqualTo javadoc says it does n't fail if both values are arrays and considered equal by the appropriate Arrays # equals overload , but it is not true . It seems to be an error between static and dynamic types . Truth # assertThat ( byte [ ] ) , for example , returns a PrimitiveByteArraySubject , which works as intended . But Truth # assertThat ( object ) returns a Subject . Subject # isEqualTo delegates into Subject # doEqualCheck , which has an special case when values are integral boxes but treats arrays as normal objects , so at the end it uses Object # equals to test for equality . I have forked the repository to add a test that verifies it ( [ link ] ( https : //github.com/gortiz/truth/commit/5c3f04562cd5e6acd33953877d9d8a40ac3a43e3 ) ) . If you are interested in , I can create a PR
*ArraySubject # hasLength __EoT__ similar to https : //github.com/google/truth/pull/101
*ArraySubject # hasLength __EoT__ similar to https : //github.com/google/truth/pull/101
PrimitiveByteArraySubject ? __EoT__ The other ones are all there for TestVerb but this one is conspicuously missing .
Reuse Trillian Merkle Tree verifiers __EoT__ Proof verification code in [ merkletree/ ] ( https : //github.com/google/certificate-transparency-go/tree/master/merkletree ) looks the same as in Trillian 's [ merkle/ ] ( https : //github.com/google/trillian/tree/master/merkle ) folder . Could we just reuse Trillian code here ? Some features in Trillian are also needed in this repo , e.g . [ range inclusion verification ] ( https : //github.com/google/trillian/pull/1108 ) will be used in Migrillian to verify the fetched batches .
Invalid SCTs for PreCerts __EoT__ We 've seen some evidence that Trillian CTFE is generating SCTs for pre-certs that are not valid . This is being investigated .
Invalid SCTs for PreCerts __EoT__ We 've seen some evidence that Trillian CTFE is generating SCTs for pre-certs that are not valid . This is being investigated .
trillian/ctfe : Debug and metrics endpoints are exposed on API endpoint __EoT__ Which requires URL filtering if the operator wants to hide potentially sensitive information . Probably a separate listener should be used for the various debug/prom endpoints instead .
Make API for Monitor work better with Java 8 __EoT__ The ` SafeBox ` example for ` Monitor ` is much more verbose than expected . We can do better : `` ` java public class SafeBox < V > { private V value ; private final Monitor monitor = new Monitor ( ) ; private final Monitor.Guard valuePresent = guard ( monitor , ( ) - > value ! = null ) ; private final Monitor.Guard valueAbsent = guard ( monitor , ( ) - > value == null ) ; public V get ( ) throws InterruptedException { try ( LockedMonitor ignored = monitor.autoEnterWhen ( valuePresent ) ) { V result = value ; value = null ; return result ; } } public void set ( V newValue ) throws InterruptedException { try ( LockedMonitor ignored = monitor.autoEnterWhen ( valueAbsent ) ) { value = newValue ; } } } `` `
EventBus : Constructor for Exception handler AND identifier __EoT__ It would be great to add an constructor which allows so set as well an Exception handler and an identifier .
CI error on ` gclient sync ` __EoT__ `` ` 2017-07-20_20:34:51.18723 Running : 2017-07-20_20:34:51.18725 cmd : PATH= '' /usr/local/bin : /usr/local/sbin : /bin : /sbin : /usr/bin : /usr/sbin : /usr/X11R6/bin : /home/clusterfuzz/depot_tools '' gclient sync -- reset 2017-07-20_20:34:51.18726 cwd : /home/clusterfuzz/chromium/src 2017-07-20_20:34:58.79479 2017-07-20_20:34:58.79481 src/third_party/skia ( ERROR ) 2017-07-20_20:34:58.79483 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 2017-07-20_20:34:58.79484 [ 0:00:00 ] Started . 2017-07-20_20:34:58.79484 [ 0:00:05 ] Up-to-date ; skipping checkout . 2017-07-20_20:34:58.79484 _____ src/third_party/skia at ae9718f1d40556ed5a49e616dbe54087f4d0d546 2017-07-20_20:34:58.79484 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 2017-07-20_20:34:58.79493 Traceback ( most recent call last ) : 2017-07-20_20:34:58.79494 File `` /home/clusterfuzz/depot_tools/gclient.py '' , line 2523 , in < module > 2017-07-20_20:34:58.79529 sys.exit ( main ( sys.argv [ 1 : ] ) ) 2017-07-20_20:34:58.79530 File `` /home/clusterfuzz/depot_tools/gclient.py '' , line 2509 , in main 2017-07-20_20:34:58.79562 return dispatcher.execute ( OptionParser ( ) , argv ) 2017-07-20_20:34:58.79563 File `` /home/clusterfuzz/depot_tools/subcommand.py '' , line 252 , in execute 2017-07-20_20:34:58.79567 return command ( parser , args [ 1 : ] ) 2017-07-20_20:34:58.79568 File `` /home/clusterfuzz/depot_tools/gclient.py '' , line 2270 , in CMDsync 2017-07-20_20:34:58.79595
do n't use -j CPU count * 10 for non-goma builds . __EoT__ -j CPU * 10 without goma can make the machine it 's building on quite slow and unusable for other tasks .
feature : physic.Units flag.Value interface __EoT__ Probably should have a discussion on what should be parsed . My thoughts are the format should be compatible with the physic.units stringers . eg : value|SI Prefix|unit ( in ascii or unicode ) 2.78µA or as 2.78uA as a concession for type-ability . [ see here ] ( https : //github.com/NeuralSpaz/periph/commit/babdbea0dbdca828fe05f0acd8104833a66eac1f )
Neopixel LED driver stability __EoT__ This is more of a question than a feature request . The [ Neopixel LED driver ] ( https : //github.com/google/periph/pull/291/files # diff-2bd32e5fdee2559ed8d3a2d6b29ea462 ) has been merged into [ experimental ] ( https : //github.com/google/periph/blob/master/experimental/devices/nrzled/ ) with a few [ examples ] ( https : //github.com/google/periph/blob/master/experimental/cmd/nrzled/ ) . My question is the stability of the driver , and if it will be merged into master , it seem well implemented , but would like to what are the future decisions with the driver .
bitbang/spi : pin initialization should occur in ` Connect ( ) ` rather than ` NewSPI ( ) ` __EoT__ We should move the pin initialization to ` Connect ( ) ` so that we know the options to determine the initial states , which are indeterminate at ` NewSPI ( ) ` .
hostname case sensitivity __EoT__ We found that we can create hostnames of mixed case , both in and out of zone , through EPP . We would not expect that both ns1.UPPER.foo and ns1.upper.foo could be created for example based on our interpretation of RFC 952 that states : A `` name '' ( Net , Host , Gateway , or Domain name ) is a text string up to 24 characters ... No distinction is made between upper and lower case . ... . a mixed case domain < create > returns : Domain names can only contain a-z , 0-9 , ' . ' and '-' **On Wed , Oct 19 , 2016 at 3:46 PM , Nick Felt nickfelt @ google.com wrote : ** You 're right , this is a bug . Thanks for pointing it out . The real issue ( IMO ) is that we rely too much on Guava 's InternetDomainName.from ( ) method to do validation for us . It 's too permissive for my tastes , and for example , when constructing a domain name , it normalizes uppercase to lowercase . If you try to use this as a validity
hostname case sensitivity __EoT__ We found that we can create hostnames of mixed case , both in and out of zone , through EPP . We would not expect that both ns1.UPPER.foo and ns1.upper.foo could be created for example based on our interpretation of RFC 952 that states : A `` name '' ( Net , Host , Gateway , or Domain name ) is a text string up to 24 characters ... No distinction is made between upper and lower case . ... . a mixed case domain < create > returns : Domain names can only contain a-z , 0-9 , ' . ' and '-' **On Wed , Oct 19 , 2016 at 3:46 PM , Nick Felt nickfelt @ google.com wrote : ** You 're right , this is a bug . Thanks for pointing it out . The real issue ( IMO ) is that we rely too much on Guava 's InternetDomainName.from ( ) method to do validation for us . It 's too permissive for my tastes , and for example , when constructing a domain name , it normalizes uppercase to lowercase . If you try to use this as a validity
hostname case sensitivity __EoT__ We found that we can create hostnames of mixed case , both in and out of zone , through EPP . We would not expect that both ns1.UPPER.foo and ns1.upper.foo could be created for example based on our interpretation of RFC 952 that states : A `` name '' ( Net , Host , Gateway , or Domain name ) is a text string up to 24 characters ... No distinction is made between upper and lower case . ... . a mixed case domain < create > returns : Domain names can only contain a-z , 0-9 , ' . ' and '-' **On Wed , Oct 19 , 2016 at 3:46 PM , Nick Felt nickfelt @ google.com wrote : ** You 're right , this is a bug . Thanks for pointing it out . The real issue ( IMO ) is that we rely too much on Guava 's InternetDomainName.from ( ) method to do validation for us . It 's too permissive for my tastes , and for example , when constructing a domain name , it normalizes uppercase to lowercase . If you try to use this as a validity
hostname case sensitivity __EoT__ We found that we can create hostnames of mixed case , both in and out of zone , through EPP . We would not expect that both ns1.UPPER.foo and ns1.upper.foo could be created for example based on our interpretation of RFC 952 that states : A `` name '' ( Net , Host , Gateway , or Domain name ) is a text string up to 24 characters ... No distinction is made between upper and lower case . ... . a mixed case domain < create > returns : Domain names can only contain a-z , 0-9 , ' . ' and '-' **On Wed , Oct 19 , 2016 at 3:46 PM , Nick Felt nickfelt @ google.com wrote : ** You 're right , this is a bug . Thanks for pointing it out . The real issue ( IMO ) is that we rely too much on Guava 's InternetDomainName.from ( ) method to do validation for us . It 's too permissive for my tastes , and for example , when constructing a domain name , it normalizes uppercase to lowercase . If you try to use this as a validity
Can not build on macOS 10.12.2 __EoT__ $ git show -- summary commit e91485cb544304d67e4c240f8ac77aa72072248a $ make clean $ make build/src/grumpy/builtin_types.go:780 : z.Text undefined ( type big.Int has no field or method Text ) build/src/grumpy/long.go:144 : x.Text undefined ( type *big.Int has no field or method Text ) build/src/grumpy/long.go:298 : toLongUnsafe ( o ) .value.Text undefined ( type big.Int has no field or method Text ) build/src/grumpy/long.go:306 : toLongUnsafe ( o ) .value.Text undefined ( type big.Int has no field or method Text ) build/src/grumpy/str.go:855 : toLongUnsafe ( o ) .Value ( ) .Text undefined ( type *big.Int has no field or method Text ) make : *** [ build/pkg/darwin_amd64/grumpy.a ] Error 1 $ /usr/bin/env python -V Python 2.7.3
open ( ) does not support w+ and a+ modes __EoT__ Per the [ docs ] ( https : //docs.python.org/2/library/functions.html # open ) this mode is legit . For some reason I did n't support it in [ file.go ] ( https : //github.com/google/grumpy/blob/master/runtime/file.go # L123 ) . `` ` $ echo 'open ( `` foo.txt '' , `` w+ '' ) ' | make run ValueError : invalid mode string : `` w+ '' `` `
Ca n't access Go struct fields from Python __EoT__ The following code fails with the error message : ` AttributeError : '*Response ' object has no attribute 'Body ' ` `` ` py from __go__.os import Stdout as stdout from __go__.net.http import DefaultClient as client rsp , err = client.Get ( `` http : //www.google.com '' ) if err is not None : raise Exception ( err.Error ( ) ) _ , err = Copy ( stdout , rsp.Body ) if err is not None : raise Exception ( err.Error ( ) ) `` `
Build gets into strange state when first invocation uses wrong version of Python __EoT__ [ @ localhost grumpy ] $ make build/src/grumpy/lib/itertools/module.go:5 : ca n't find import : `` grumpy/lib/sys '' make : *** [ build/pkg/linux_amd64/grumpy/lib/itertools.a ] Error 1
Add `` common corruption '' transformations as a warmup attack __EoT__ I think that the transformations from [ Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations by Dan Hendrycks and Tom Dietterich ] ( https : //arxiv.org/abs/1807.01697 ) would be a good addition to the warmup attacks . ! [ image ] ( https : //user-images.githubusercontent.com/306655/45835540-3d6b0780-bcbf-11e8-82d9-6b3bd8939260.png ) Three reasons why I think that this would be a good addition : 1 . Their corruptions clearly illustrate a set of non-lp-restricted attacks 2 . These corruptions are strictly easier than an unrestricted adversary 3 . They have a [ simple and well-designed codebase ] ( https : //github.com/hendrycks/robustness ) # # # # Tasks - [ x ] Check their license ( 👍 Apache is good ) - [ x ] Convert [ their library ] ( https : //github.com/hendrycks/robustness ) into a pip module ( either in place or copy it into this repository ) - [ x ] Add all transformations at severity 1 as a warmup attack on the ` bird-or-bicycle ` dataset ( Perhaps 10 transformations per image ) - [ x ] Evaluate baseline defenses - [ x ] Inspect confident misclassifications to ensure
Add TravisCI __EoT__
question about the dataset __EoT__ I believe there are a few erroneous images in the training set for bird ( which is significant when there are only 500 images per class ) . Q 1 in Appendix `` .1 Instructions given to taskers says : > Does this photo contain a bird , or a depiction of a bird ( e.g. , a toy bird , a painting of a bird , a stuffed animal bird , a cartoon bird ) anywhere in the image ? However , point 4 and the italicized text below suggest that paintings/depictions are not allowed : > ( It is okay if the object is a **photorealistic** rendering of a bird/bicycle . ) .. , is not truncated , is not occluded , and is **not a depiction** of any sort . In any case , I do not believe this black and white sketch from the bird training set should be included : ! [ 2e8cd9f60546ac55 ] ( https : //user-images.githubusercontent.com/5000421/48081465-25dbd580-e1be-11e8-9053-47d6b5517bad.jpg ) There 's also : ! [ 954c63e11aa1232e ] ( https : //user-images.githubusercontent.com/5000421/48082599-085c3b00-e1c1-11e8-9815-1b15129abfa2.jpg ) And this image ( also from the bird training set ) is of a moth : ! [
SpatialAttack in eval_kit is sometimes extremely slow __EoT__ This attack should take no more than 1 hour . I ran it for 24 hours on the ` undefended_keras_resnet ` and it only completed 50 % . This replaces the attack with a faster implementation that is pending in cleverhans . https : //github.com/tensorflow/cleverhans/pull/623 I 've copied the attack into our repo for now , and will swap to the cleverhans version after it is released .
` gts check ` does n't work when installed globally __EoT__ I am trying to use gts by doing an ` npm install -g gts ` . I got this library that 's going to be a train wreck to upgrade , so I want to do it one piece at a time with a global tool . So I ` cd ` into my app dir , and run the check command , and this is the call stack : `` ` beckwith-macbookpro : google-api-nodejs-client beckwith $ gts check FatalError : Could not find config file at : /Users/beckwith/Code/google-api-nodejs-client/node_modules/gts/tslint.json at new FatalError ( /Users/beckwith/.nvm/versions/node/v8.9.1/lib/node_modules/gts/node_modules/tslint/lib/error.js:27:28 ) at findConfigurationPath ( /Users/beckwith/.nvm/versions/node/v8.9.1/lib/node_modules/gts/node_modules/tslint/lib/configuration.js:55:19 ) at Object.findConfiguration ( /Users/beckwith/.nvm/versions/node/v8.9.1/lib/node_modules/gts/node_modules/tslint/lib/configuration.js:41:22 ) at lint ( /Users/beckwith/.nvm/versions/node/v8.9.1/lib/node_modules/gts/build/src/lint.js:31:48 ) at /Users/beckwith/.nvm/versions/node/v8.9.1/lib/node_modules/gts/build/src/cli.js:77:46 at step ( /Users/beckwith/.nvm/versions/node/v8.9.1/lib/node_modules/gts/build/src/cli.js:33:23 ) at Object.next ( /Users/beckwith/.nvm/versions/node/v8.9.1/lib/node_modules/gts/build/src/cli.js:14:53 ) at /Users/beckwith/.nvm/versions/node/v8.9.1/lib/node_modules/gts/build/src/cli.js:8:71 at new Promise ( < anonymous > ) at __awaiter ( /Users/beckwith/.nvm/versions/node/v8.9.1/lib/node_modules/gts/build/src/cli.js:4:12 ) `` `
Add ` gts clean ` command __EoT__ We should add a command to do ` clean ` instead of inserting an ` rm -rf ` command into package.json .
Add npm badge to README __EoT__
test : Set up first unit tests __EoT__ We got integration tests mostly figured out . Need something we can use as a template for more unit tests moving forward .
` gts check/fix ` uses a list of files created by tslint __EoT__ See below comment ~On my mac , ` gts check ` exits with code 0 before checking all files [ here ] ( https : //github.com/GoogleCloudPlatform/cloud-trace-nodejs/pull/621 ) . On linux this does n't happen , and I get my clang-format error reports.~ ~I combed ` gts ` dependencies for ` process.exit ` calls but could n't find any . I believe unresolved Promises are somehow being ignored , and the event loops drains fully . Unsure about this ... ~
Thread 2 `` FEngine : :loop '' received signal SIGSEGV , Segmentation fault . __EoT__ OK ... vk_strobecolor works , but when I try vk_hellotriangle , I get a SIGSEGV : ( gdb ) run Starting program : /home/kjh/filament/out/cmake-debug/samples/vk_hellotriangle Missing separate debuginfos , use : dnf debuginfo-install glibc-2.27-30.fc28.x86_64 [ Thread debugging using libthread_db enabled ] Using host libthread_db library `` /lib64/libthread_db.so.1 '' . FEngine ( 64 bits ) created at 0x7ffff5be7010 [ New Thread 0x7ffff4fe4700 ( LWP 31293 ) ] FEngine resolved backend : Vulkan INTEL-MESA : warning : Bay Trail Vulkan support is incomplete Selected physical device : Intel ( R ) Bay Trail warning : Loadable section `` .note.gnu.property '' outside of ELF segments warning : Loadable section `` .note.gnu.property '' outside of ELF segments warning : Loadable section `` .note.gnu.property '' outside of ELF segments warning : Loadable section `` .note.gnu.property '' outside of ELF segments Missing separate debuginfo for /lib64/libGLX_mesa.so.0 Try : dnf -- enablerepo='*debug* ' install /usr/lib/debug/.build-id/aa/7b9e010d068bc3af7d14f1491eb0e37809bff0.debug Missing separate debuginfo for /lib64/libglapi.so.0 Try : dnf -- enablerepo='*debug* ' install /usr/lib/debug/.build-id/c9/9c6d22a33c958d0a790732dabd8c38d1c22d1e.debug warning : Loadable section `` .note.gnu.property '' outside of ELF segments Missing separate debuginfo for /usr/lib64/dri/i965_dri.so Try : dnf -- enablerepo='*debug* ' install /usr/lib/debug/.build-id/21/a8eb9367b7c6065170ed4ce6690914c0be6f8f.debug [
Thread 2 `` FEngine : :loop '' received signal SIGSEGV , Segmentation fault . __EoT__ OK ... vk_strobecolor works , but when I try vk_hellotriangle , I get a SIGSEGV : ( gdb ) run Starting program : /home/kjh/filament/out/cmake-debug/samples/vk_hellotriangle Missing separate debuginfos , use : dnf debuginfo-install glibc-2.27-30.fc28.x86_64 [ Thread debugging using libthread_db enabled ] Using host libthread_db library `` /lib64/libthread_db.so.1 '' . FEngine ( 64 bits ) created at 0x7ffff5be7010 [ New Thread 0x7ffff4fe4700 ( LWP 31293 ) ] FEngine resolved backend : Vulkan INTEL-MESA : warning : Bay Trail Vulkan support is incomplete Selected physical device : Intel ( R ) Bay Trail warning : Loadable section `` .note.gnu.property '' outside of ELF segments warning : Loadable section `` .note.gnu.property '' outside of ELF segments warning : Loadable section `` .note.gnu.property '' outside of ELF segments warning : Loadable section `` .note.gnu.property '' outside of ELF segments Missing separate debuginfo for /lib64/libGLX_mesa.so.0 Try : dnf -- enablerepo='*debug* ' install /usr/lib/debug/.build-id/aa/7b9e010d068bc3af7d14f1491eb0e37809bff0.debug Missing separate debuginfo for /lib64/libglapi.so.0 Try : dnf -- enablerepo='*debug* ' install /usr/lib/debug/.build-id/c9/9c6d22a33c958d0a790732dabd8c38d1c22d1e.debug warning : Loadable section `` .note.gnu.property '' outside of ELF segments Missing separate debuginfo for /usr/lib64/dri/i965_dri.so Try : dnf -- enablerepo='*debug* ' install /usr/lib/debug/.build-id/21/a8eb9367b7c6065170ed4ce6690914c0be6f8f.debug [
Add support for CommonJS packaging __EoT__ I 'm on web and I 'm trying to use Filament via the npm filament @ 1.0.0 package . I 'm bundling filament.js with the rest of my code using webpack . I 'm running into headwinds trying to get Filament init'ed ... For starters , I do n't have any assets to load , I am just trying to Filament.init with an empty assets array . Filament.init ( [ ] , ( ) = > { this.filament = Filament.Engine.create ( this.canvas ) ; this.scene = this.filament.createScene ( ) ; this.camera = this.filament.createCamera ( ) ; const eye = [ 0 , 0 , 4 ] , center = [ 0 , 0 , 0 ] , up = [ 0 , 1 , 0 ] ; this.camera.lookAt ( eye , center , up ) ; console.log ( `` GOT HERE ! `` ) ; // never gets here } ) ; Filament does call my callback , but Filament.Engine is undefined and my code does not continue . I see some interesting warnings in my logs as well . 14:27:21.347 wasm streaming compile failed : TypeError : Filament.Engine is undefined filament.js:6:20487 14:27:21.347 falling
Fedora build problems __EoT__ On Fedora 28 : CC=/usr/bin/clang CXX=/usr/bin/clang++ ./build.sh debug ... works ( though bluegl does n't work on 32 bit systems ) . If I do : ./build.sh debug ... I get `` Could NOT find Threads ( missing : Threads_FOUND ) '' . Dunno why . Also , if I do ./build.sh -m debug ... I get : c++ : error : unrecognized command line option ‘ -stdlib=libc++ ’ This is GNU 8.1.1
Thread 2 `` FEngine : :loop '' received signal SIGSEGV , Segmentation fault ( not Bay Trail ) __EoT__ Deja vu : [ kjh @ usd32 cmake-debug ] $ gdb samples/vk_hellotriangle GNU gdb ( GDB ) Fedora 8.1.1-3.fc28 Copyright ( C ) 2018 Free Software Foundation , Inc. License GPLv3+ : GNU GPL version 3 or later < http : //gnu.org/licenses/gpl.html > This is free software : you are free to change and redistribute it . There is NO WARRANTY , to the extent permitted by law . Type `` show copying '' and `` show warranty '' for details . This GDB was configured as `` x86_64-redhat-linux-gnu '' . Type `` show configuration '' for configuration details . For bug reporting instructions , please see : < http : //www.gnu.org/software/gdb/bugs/ > . Find the GDB manual and other documentation resources online at : < http : //www.gnu.org/software/gdb/documentation/ > . For help , type `` help '' . Type `` apropos word '' to search for commands related to `` word '' ... Reading symbols from samples/vk_hellotriangle ... done . ( gdb ) run Starting program : /home/kjh/filament/out/cmake-debug/samples/vk_hellotriangle Missing separate debuginfos , use : dnf debuginfo-install glibc-2.27-30.fc28.x86_64 [ Thread debugging
Crash on AMD GPU __EoT__ All examples on OpenGL crashes on same function ` OpenGLDriver : :updateUniformBuffer ` , on friend pc ( GTX 970 , win 10 ) works fine . OS : Win 10 GPU : RX 480 8GB Drivers : 18.5.1 ( on older drivers same situation ) > FEngine ( 64 bits ) created at 000002C860664080 > FEngine resolved backend : OpenGL > HwFence : 16 > GLIndexBuffer : 24 > GLSamplerBuffer : 24 > GLRenderPrimitive : 48 > GLTexture : 64 > OpenGLProgram : 56 > GLRenderTarget : 72 > GLVertexBuffer : 112 > GLUniformBuffer : 128 > GLStream : 128 > ATI Technologies Inc. > Radeon ( TM ) RX 480 Graphics > 4.1.13521 Core Profile Forward-Compatible Context 24.20.11016.4 > 4.50 > OS version : 0 > Hierarchy depth = 1 > OpenGL error 1282 ( GL_INVALID_OPERATION ) in `` void filament : :OpenGLDriver : :updateUniformBuffer ( Driver : :UniformBufferHandle , filament : :UniformBuffer & & ) '' at line 1851 Callstack : > ucrtbased.dll ! 00007fff40f67135 ( ) Unknown > ucrtbased.dll ! 00007fff40f672d3 ( ) Unknown > ucrtbased.dll ! 00007fff40f7bf3d ( ) Unknown > ucrtbased.dll ! 00007fff40f7af86 ( ) Unknown > material_sandbox.exe !
Set presentation time on Android right before eglSwapBuffers ( ) __EoT__ To facilitate media recording , etc . we should set the presentation time just before swapping buffers .
Ca n't read `` contentType '' from cache __EoT__ I have met a issue , when I try to get the `` contentType '' from cache , it always return null . Then I checked the source code of `` DiskBasedCache '' and `` BasicNetwork '' . It turns out that in `` DiskBasedCache '' , the volley use `` HaspMap '' to save the http headers and in '' BasicNetwork '' it uses `` new TreeMap < String , String > ( String.CASE_INSENSITIVE_ORDER ) '' which is not case sensitive . So if the server 's response header , uses `` content-type '' rather than `` Content-Type '' , it can work properly with a response from network but ca n't get contentType from the cache . And because of this issue , you ca n't parse the charset from cache , and it will use the default charset which is `` ISO_8859_1 '' , and if it does n't match the actually charset which you should get from `` content-type '' , then you ca n't get the right response content with the right charset from cache .
Add main fileld to package.json __EoT__ Is it possible to add `` main '' field to package.json configuration file ? We are currently setting it to `` bin/prettify.min.js '' after npm update . This is necessary in order to use this library with angular-cli . If approved , I can provide a pull request .
/api/teams is O ( n ) again ! __EoT__ /api/teams has fallen back to O ( n ) somehow .
/api/teams is O ( n ) again ! __EoT__ /api/teams has fallen back to O ( n ) somehow .
/api/teams is O ( n ) again ! __EoT__ /api/teams has fallen back to O ( n ) somehow .
/api/teams is O ( n ) again ! __EoT__ /api/teams has fallen back to O ( n ) somehow .
Support for many to many attachments __EoT__ To quote from : https : //github.com/Matir/pwnableweb-scoreboard/issues/89 '' Complexity : Deleting one attachment from one challenge - do we cascade delete ? leave them as is ? '' It might be worthwhile having a new admin section called `` Attachments '' where admins can upload new files , and then having a drop down style box . For orphaned files - this could be identified by having the number of links to the attachment displayed - ala foo.zip ( 1 ) vs bar.zip ( 0 )
Support for many to many attachments __EoT__ To quote from : https : //github.com/Matir/pwnableweb-scoreboard/issues/89 '' Complexity : Deleting one attachment from one challenge - do we cascade delete ? leave them as is ? '' It might be worthwhile having a new admin section called `` Attachments '' where admins can upload new files , and then having a drop down style box . For orphaned files - this could be identified by having the number of links to the attachment displayed - ala foo.zip ( 1 ) vs bar.zip ( 0 )
Support for many to many attachments __EoT__ To quote from : https : //github.com/Matir/pwnableweb-scoreboard/issues/89 '' Complexity : Deleting one attachment from one challenge - do we cascade delete ? leave them as is ? '' It might be worthwhile having a new admin section called `` Attachments '' where admins can upload new files , and then having a drop down style box . For orphaned files - this could be identified by having the number of links to the attachment displayed - ala foo.zip ( 1 ) vs bar.zip ( 0 )
Support for many to many attachments __EoT__ To quote from : https : //github.com/Matir/pwnableweb-scoreboard/issues/89 '' Complexity : Deleting one attachment from one challenge - do we cascade delete ? leave them as is ? '' It might be worthwhile having a new admin section called `` Attachments '' where admins can upload new files , and then having a drop down style box . For orphaned files - this could be identified by having the number of links to the attachment displayed - ala foo.zip ( 1 ) vs bar.zip ( 0 )
Clarify whether the error string passed to glsl-reduce is a regex or just a string , and related issues __EoT__ - Is it a regex or just a string ? ( WebUI says regex ; command-line option says string . ) - Do you need to provide quotes when passing in from the WebUI ? ( Whatever the answer , update the documentation . ) - Can characters such as `` ( `` appear verbatim , or do they need to be escaped ?
WebUI : 'delete these results ' button does not delete the reductions __EoT__ I guess it only deletes the _exp folder , not the _inv
Doc : write glsl-reduce walkthrough __EoT__
Doc : write glsl-reduce walkthrough __EoT__
Doc : write glsl-reduce walkthrough __EoT__
Doc : write glsl-reduce walkthrough __EoT__
Doc : write glsl-reduce walkthrough __EoT__
Doc : write glsl-reduce walkthrough __EoT__
Doc : write glsl-reduce walkthrough __EoT__
Doc : write glsl-reduce walkthrough __EoT__
Doc : write glsl-reduce intro __EoT__
Doc : write glsl-reduce intro __EoT__
Doc : write glsl-reduce intro __EoT__
Doc : write glsl-reduce intro __EoT__
Doc : write glsl-reduce intro __EoT__
Doc : write glsl-reduce intro __EoT__
Doc : write glsl-reduce intro __EoT__
Move the libraries under pkg __EoT__
remote.Delete __EoT__ See [ here ] ( https : //github.com/google/containerregistry/blob/master/client/v2_2/docker_session_.py # L316 )
ko does not load images into minikube daemon __EoT__ I was testing ` ko ` to build/upload container images into Minikube 's daemon but so far I found the following problems : 1 . It defaults to the docker env you have set up , in my case `` Docker for Mac '' not to Minikube ( I think that should be said at least in the Readme ) 2 . I tried to run ` eval $ ( minikube docker-env ) ` , so that the minikube daemon would be used as describe in [ Reusing the docker daemon from Minikube ] ( https : //github.com/kubernetes/minikube/blob/master/docs/reusing_the_docker_daemon.md ) . This did n't help 3 . I investigate this library and found that you use [ NewEnvClient ] ( https : //godoc.org/github.com/docker/docker/client # NewEnvClient ) for the daemon , which should use the variables provided by ` minikube docker-env ` . `` ` $ minikube docker-env export DOCKER_TLS_VERIFY= '' 1 '' export DOCKER_HOST= '' tcp : //192.168.64.12:2376 '' export DOCKER_CERT_PATH= '' /Users/jszroberto/.minikube/certs '' export DOCKER_API_VERSION= '' 1.23 '' # Run this command to configure your shell : # eval $ ( minikube docker-env ) `` ` 4 . I tracked it
Unified CLI surface __EoT__ The ` cmd ` directory currently has four separate binaries a user can use to interact with a remote registry : ` puller ` , ` pusher ` , ` poke ` ( which gets only metadata ) , and ` deleter ` . It looks like we 'll be adding ` appender ` soon in # 63 . I think we should head off the proliferation of separate binaries and move them into a single command with subcommands for each operation . This will allow users to install a single binary into their path , and could allow some code reuse and help standardize conventions for the commands . This would also make it easier to package as a [ builder image ] ( https : //github.com/GoogleCloudPlatform/cloud-builders ) for GCB . # # Subcommands * ` get ` ( instead of ` poke ` ) * ` pull ` * ` push ` * ` delete ` * ` append ` * ` tag ( list|add|remove ) ` * ` label ( list|add|remove ) ` * [ ` flatten ` ] ( https : //github.com/google/containerregistry # flattenpar ) * [ ` rebase ` ] (
Experiment : //cmd/ko __EoT__ **tl ; dr** I want a more purpose built variation of what you get with ` bazel ` + ` rules_go ` + ` rules_docker ` + ` rules_k8s ` that simply wraps the Go toolchain directly . # # # Background : State of Bazel Today in Bazel , my build is described in ` BUILD ` files ( largely generated by Gazelle ) and extended to handle containerization and kubernetesification via : `` ` python # BEGIN generated by Gazelle go_library ( name = `` go_default_library '' , ... ) go_test ( name = `` go_default_test '' , ... ) go_binary ( name = `` name-of-directory '' , ... ) # END generated by Gazelle # Wrap the Go binary into a minimal container image . go_image ( name = `` image '' , binary = `` : name-of-directory '' , ) # Helper for interacting with my K8s `` Deployment '' k8s_object ( name = `` deployment '' , template = `` deployment.yaml '' , images = { # Associate the image reference in deployment.yaml with the associated binary target . `` gcr.io/foo/bar : baz '' : `` : image '' , }
[ windows ] : update_flutter_engine.bat is failing on latest flutter master channel __EoT__ I 'm on flutter master branch and have upgraded flutter to the latest version . When update_flutter_engine.bat is run either directly from cmd or as part of the windows desktop embedder library build , there are a bunch of errors eg : file : ///C : /Users/james/Source/github.com/clarkezone/flutter-desktop-embedding/third_party/dart_packages/crypto/lib/src/hash_sink.dart ( 20,9 ) : error G7D2AEF3C : Type 'Endianness ' not found . 1 > final Endianness _endian ;
[ linux ] Text input plugin broken in master branch __EoT__ Flutter version : bf531ba87119cb2292ec3bb4c88231c15e115a6b Flutter engine version : 15ddcd448d972381cfa9c93c041dfa31a2061e0c When building a binary that should run the ` examples/flutter_gallery ` app in the embedder , text input fails to function correctly . This should be investigated after # 61 has landed to prevent spurious bugs/conflicts from merging .
Add text input support to Linux __EoT__ We need a text input implementation for the Linux embedder once the core implementation is fleshed out more .
Add text input support to Linux __EoT__ We need a text input implementation for the Linux embedder once the core implementation is fleshed out more .
[ windows ] flutter_location needs a .bat version __EoT__ Currently tools/flutter_location is only a bash script , so Windows still requires that the Flutter tree be at a known location for the tooling to work . We need a .bat port , at which point update_flutter_engine.bat can use it the way update_flutter_engine uses flutter_location .
[ windows ] flutter_location needs a .bat version __EoT__ Currently tools/flutter_location is only a bash script , so Windows still requires that the Flutter tree be at a known location for the tooling to work . We need a .bat port , at which point update_flutter_engine.bat can use it the way update_flutter_engine uses flutter_location .
Windows Support __EoT__ It seems like there should be an issue tracking all major operating systems . Windows support would also be awesome ! : )
Windows Support __EoT__ It seems like there should be an issue tracking all major operating systems . Windows support would also be awesome ! : )
Windows Support __EoT__ It seems like there should be an issue tracking all major operating systems . Windows support would also be awesome ! : )
Windows Support __EoT__ It seems like there should be an issue tracking all major operating systems . Windows support would also be awesome ! : )
Windows Support __EoT__ It seems like there should be an issue tracking all major operating systems . Windows support would also be awesome ! : )
Redefined buffer types __EoT__ AmberScript 's [ buffer type ] ( https : //github.com/google/amber/blob/master/docs/amber_script.md # buffer-types ) is confusing . I feel that we should have the following buffer types : - color for color attachment of framebuffer - depth stencil for depth/stencil attachment of framebuffer - vertex for vertex buffer - index for index buffer - storage image - sampled image - combined image sampler - uniform texel buffer - storage texel buffer - storage buffer - uniform buffer - input attachment Note that here we also uses the term 'buffer ' for image because in the view of Amber script , the image is also a sequence of bytes . Since storage image , sampled image , combined image sampler , uniform texel buffer , storage texel buffer , storage buffer , uniform buffer , and input attachment are [ Vulkan specific descriptor types ] ( https : //www.khronos.org/registry/vulkan/specs/1.1/html/vkspec.html # descriptorsets-types ) and I am not sure WebGPU will define the same types or not , we would better consider how to support buffers of WebGPU efficiently .
Vulkan : Implement PROBE for SSBO and general images . __EoT__
Add Vulkan feature and extension setup __EoT__
Cleanup Requirements code __EoT__ The current code which passes the [ require ] information into the engines is ... not good . Need to refactor the code to make it easier to extend .
Support for gradle experimental android plugin __EoT__ Hi ! Is it possible to use this plugin with the [ gradle experimental android plugin ] ( http : //tools.android.com/tech-docs/new-build-system/gradle-experimental ) ? Since the experimental plugin requires gradle 2.10 , I 've tried it with release 0.7.5 of this plugin , but it does n't seem to work , it says it requires the android plugin . Is there a way to work around this ?
Support for gradle experimental android plugin __EoT__ Hi ! Is it possible to use this plugin with the [ gradle experimental android plugin ] ( http : //tools.android.com/tech-docs/new-build-system/gradle-experimental ) ? Since the experimental plugin requires gradle 2.10 , I 've tried it with release 0.7.5 of this plugin , but it does n't seem to work , it says it requires the android plugin . Is there a way to work around this ?
Support for gradle experimental android plugin __EoT__ Hi ! Is it possible to use this plugin with the [ gradle experimental android plugin ] ( http : //tools.android.com/tech-docs/new-build-system/gradle-experimental ) ? Since the experimental plugin requires gradle 2.10 , I 've tried it with release 0.7.5 of this plugin , but it does n't seem to work , it says it requires the android plugin . Is there a way to work around this ?
Support for gradle experimental android plugin __EoT__ Hi ! Is it possible to use this plugin with the [ gradle experimental android plugin ] ( http : //tools.android.com/tech-docs/new-build-system/gradle-experimental ) ? Since the experimental plugin requires gradle 2.10 , I 've tried it with release 0.7.5 of this plugin , but it does n't seem to work , it says it requires the android plugin . Is there a way to work around this ?
Large number of proto files leads to generate failure on Windows __EoT__ Windows has a relatively small command line length . We have a lot of proto files . When the plugin tries to make the call to generate , we get an error because the command line that the plugin tries to call is too long . When I printed out debug logging , I saw it was because it places each proto file on the command line . Is there a way that a directory can be used instead of listing every proto file on the command line ? That way a directory can be passed to the command instead of a huge amount of file paths . Edit , adding in the protoc used : ` protobuf { protoc { artifact = 'com.google.protobuf : protoc:3.3.0' } } `
Error on build in Android Studio 3 Canary & Gradle 4 __EoT__ Just testing the new canary release of Android Studio 3 , and I am getting this error on build . `` ` Error : Could not determine the dependencies of task ' : app : extractIncludeDebugProto ' . > Resolving configuration 'debugCompile ' directly is not allowed `` ` It might be possible to work around in ` app/build.gradle ` with the protobuf compilation configuration . However , my gradle-fu is insufficient to figure it out right now .
Error on build in Android Studio 3 Canary & Gradle 4 __EoT__ Just testing the new canary release of Android Studio 3 , and I am getting this error on build . `` ` Error : Could not determine the dependencies of task ' : app : extractIncludeDebugProto ' . > Resolving configuration 'debugCompile ' directly is not allowed `` ` It might be possible to work around in ` app/build.gradle ` with the protobuf compilation configuration . However , my gradle-fu is insufficient to figure it out right now .
Add TravisCI support __EoT__ Add TravisCI support Right after cloning the project , simply running ` gradlew tasks ` fails : `` ` FAILURE : Build failed with an exception . * What went wrong : A problem occurred configuring project ' : testProject ' . > Could not resolve all dependencies for configuration ' : testProject : classpath ' . > Could not find com.google.protobuf : protobuf-gradle-plugin:0.7.8-SNAPSHOT . Searched in the following locations : https : //jcenter.bintray.com/com/google/protobuf/protobuf-gradle-plugin/0.7.8-SNAPSHOT/maven-metadata.xml https : //jcenter.bintray.com/com/google/protobuf/protobuf-gradle-plugin/0.7.8-SNAPSHOT/protobuf-gradle-plugin-0.7.8-SNAPSHOT.pom https : //jcenter.bintray.com/com/google/protobuf/protobuf-gradle-plugin/0.7.8-SNAPSHOT/protobuf-gradle-plugin-0.7.8-SNAPSHOT.jar file : /Users/noname/.m2/repository/com/google/protobuf/protobuf-gradle-plugin/0.7.8-SNAPSHOT/maven-metadata.xml file : /Users/noname/.m2/repository/com/google/protobuf/protobuf-gradle-plugin/0.7.8-SNAPSHOT/protobuf-gradle-plugin-0.7.8-SNAPSHOT.pom file : /Users/noname/.m2/repository/com/google/protobuf/protobuf-gradle-plugin/0.7.8-SNAPSHOT/protobuf-gradle-plugin-0.7.8-SNAPSHOT.jar Required by : protobuf-gradle-plugin : testProject : unspecified * Try : Run with -- stacktrace option to get the stack trace . Run with -- info or -- debug option to get more log output . BUILD FAILED Total time : 9.575 secs `` ` Adding CI will fix these issues .
Add TravisCI support __EoT__ Add TravisCI support Right after cloning the project , simply running ` gradlew tasks ` fails : `` ` FAILURE : Build failed with an exception . * What went wrong : A problem occurred configuring project ' : testProject ' . > Could not resolve all dependencies for configuration ' : testProject : classpath ' . > Could not find com.google.protobuf : protobuf-gradle-plugin:0.7.8-SNAPSHOT . Searched in the following locations : https : //jcenter.bintray.com/com/google/protobuf/protobuf-gradle-plugin/0.7.8-SNAPSHOT/maven-metadata.xml https : //jcenter.bintray.com/com/google/protobuf/protobuf-gradle-plugin/0.7.8-SNAPSHOT/protobuf-gradle-plugin-0.7.8-SNAPSHOT.pom https : //jcenter.bintray.com/com/google/protobuf/protobuf-gradle-plugin/0.7.8-SNAPSHOT/protobuf-gradle-plugin-0.7.8-SNAPSHOT.jar file : /Users/noname/.m2/repository/com/google/protobuf/protobuf-gradle-plugin/0.7.8-SNAPSHOT/maven-metadata.xml file : /Users/noname/.m2/repository/com/google/protobuf/protobuf-gradle-plugin/0.7.8-SNAPSHOT/protobuf-gradle-plugin-0.7.8-SNAPSHOT.pom file : /Users/noname/.m2/repository/com/google/protobuf/protobuf-gradle-plugin/0.7.8-SNAPSHOT/protobuf-gradle-plugin-0.7.8-SNAPSHOT.jar Required by : protobuf-gradle-plugin : testProject : unspecified * Try : Run with -- stacktrace option to get the stack trace . Run with -- info or -- debug option to get more log output . BUILD FAILED Total time : 9.575 secs `` ` Adding CI will fix these issues .
Add TravisCI support __EoT__ Add TravisCI support Right after cloning the project , simply running ` gradlew tasks ` fails : `` ` FAILURE : Build failed with an exception . * What went wrong : A problem occurred configuring project ' : testProject ' . > Could not resolve all dependencies for configuration ' : testProject : classpath ' . > Could not find com.google.protobuf : protobuf-gradle-plugin:0.7.8-SNAPSHOT . Searched in the following locations : https : //jcenter.bintray.com/com/google/protobuf/protobuf-gradle-plugin/0.7.8-SNAPSHOT/maven-metadata.xml https : //jcenter.bintray.com/com/google/protobuf/protobuf-gradle-plugin/0.7.8-SNAPSHOT/protobuf-gradle-plugin-0.7.8-SNAPSHOT.pom https : //jcenter.bintray.com/com/google/protobuf/protobuf-gradle-plugin/0.7.8-SNAPSHOT/protobuf-gradle-plugin-0.7.8-SNAPSHOT.jar file : /Users/noname/.m2/repository/com/google/protobuf/protobuf-gradle-plugin/0.7.8-SNAPSHOT/maven-metadata.xml file : /Users/noname/.m2/repository/com/google/protobuf/protobuf-gradle-plugin/0.7.8-SNAPSHOT/protobuf-gradle-plugin-0.7.8-SNAPSHOT.pom file : /Users/noname/.m2/repository/com/google/protobuf/protobuf-gradle-plugin/0.7.8-SNAPSHOT/protobuf-gradle-plugin-0.7.8-SNAPSHOT.jar Required by : protobuf-gradle-plugin : testProject : unspecified * Try : Run with -- stacktrace option to get the stack trace . Run with -- info or -- debug option to get more log output . BUILD FAILED Total time : 9.575 secs `` ` Adding CI will fix these issues .
Need to resolve versioning of mobly api documentation generation through readthedocs.org __EoT__ Currently we have a pip requirements file ( docs/rtd_requirements.txt ) that is used by the readthedocs.org to generate mobly api documentation . This causes the code thats used to generate the documentation to come from the latest version of mobly thats available through pip , and not from the latest code in master branch . We need to fix the documentation setup so that readthedocs build grabs the right version of code from the branch .
Snippet crashes on phones that print extra instrumentation output . __EoT__ Some phones print extra information prior to instrumentation output . On such phones , the snippet crashes .
snippet launching is broken in py3 __EoT__ When trying to launch snippet , it fails with : `` ` File `` /Users/angli/Developer/mobly/tools/snippet_shell.py '' , line 43 , in _start_services self._ad.load_snippet ( name='snippet ' , package=self._package ) File `` /Users/angli/Developer/mobly/mobly/controllers/android_device.py '' , line 727 , in load_snippet client.start_app_and_connect ( ) File `` /Users/angli/Developer/mobly/mobly/controllers/android_device_lib/snippet_client.py '' , line 92 , in start_app_and_connect persists_shell_cmd = self._get_persist_command ( ) File `` /Users/angli/Developer/mobly/mobly/controllers/android_device_lib/snippet_client.py '' , line 312 , in _get_persist_command if command in self._adb.shell ( 'which % s ' % command ) : TypeError : a bytes-like object is required , not 'str' `` ` Looks like this is related to recent change # 251
snippet launching is broken in py3 __EoT__ When trying to launch snippet , it fails with : `` ` File `` /Users/angli/Developer/mobly/tools/snippet_shell.py '' , line 43 , in _start_services self._ad.load_snippet ( name='snippet ' , package=self._package ) File `` /Users/angli/Developer/mobly/mobly/controllers/android_device.py '' , line 727 , in load_snippet client.start_app_and_connect ( ) File `` /Users/angli/Developer/mobly/mobly/controllers/android_device_lib/snippet_client.py '' , line 92 , in start_app_and_connect persists_shell_cmd = self._get_persist_command ( ) File `` /Users/angli/Developer/mobly/mobly/controllers/android_device_lib/snippet_client.py '' , line 312 , in _get_persist_command if command in self._adb.shell ( 'which % s ' % command ) : TypeError : a bytes-like object is required , not 'str' `` ` Looks like this is related to recent change # 251
Deprecate old output file format __EoT__ Once new format is rolled out . Targeting release 1.7
Deprecate old output file format __EoT__ Once new format is rolled out . Targeting release 1.7
Flake in unit tests __EoT__ Reported in # 87 https : //travis-ci.org/google/mobly/jobs/197104959 =================================== FAILURES =================================== ____________________ UtilsTest.test_start_standing_subproc _____________________ self = < tests.mobly.utils_test.UtilsTest testMethod=test_start_standing_subproc > def test_start_standing_subproc ( self ) : with self.assertRaisesRegexp ( utils.Error , `` Process . * has terminated '' ) : \ > utils.start_standing_subprocess ( `` sleep 0 '' , check_health_delay=0.1 ) E AssertionError : Error not raised tests/mobly/utils_test.py:32 : AssertionErro
Support recording error without failing the test immediately __EoT__ In system level tests , multiple errors could occur in a test . In some cases , we might need the test to proceed with the remaining steps when an error occurs , and eventually fail at the end . For example , in a two device test , if one device 's teardown failed , we want the other device to tear down properly ; or if one teardown step failed , we want the remaining teardown steps to still happen . Another example , in a two device test , if a step is required for both devices , and this step is parallelized in different threads , error may happen in either or both threads . For all these cases , we want to report all the errors that happened during the test with stacktrace and mark the test as `` Error '' in the end . In order to do this , Mobly could provide APIs for a test to record errors , fail the test if any error was recorded , and record all the errors as `` Extra Errors ''
Instrumentation parser does not decode adb output __EoT__ In python3 , adb returns byte arrays , which do not interoperate with unicode strings correctly . This usually necessitates a decode ( 'utf-8 ' ) , which the instrumentation parser is not doing . There was a fix for this in https : //github.com/google/mobly/pull/392 But that PR has been stalled , so splitting this out .
Provide more information in conditional functions like ` on_fail ` __EoT__ The current conditional functions have signatures like : `` ` def on_fail ( self , test_name , begin_time ) `` ` This has very limited information . For example , if a test fails with multiple errors , which is very common in large e2e tests , the ` on_fail ` would have no way to know exactly what failed . We should provide more information to the test in these functions . We could do this by passing in a copy of the test record for a particular test .
Add presubmit unit test run for windows __EoT__ Travis could n't support windows runs . So we need to figure out how to run presubmit unit tests on windows
Docstring errors found by sphinx api doc generation tool __EoT__ /mobly/asserts.py : docstring of mobly.asserts.abort_all:7 : ERROR : Unexpected indentation . /mobly/asserts.py : docstring of mobly.asserts.abort_all_if:8 : ERROR : Unexpected indentation . /mobly/asserts.py : docstring of mobly.asserts.abort_class:10 : ERROR : Unexpected indentation . /mobly/asserts.py : docstring of mobly.asserts.abort_class_if:11 : ERROR : Unexpected indentation . /mobly/asserts.py : docstring of mobly.asserts.assert_equal:10 : ERROR : Unexpected indentation . /mobly/asserts.py : docstring of mobly.asserts.assert_false:7 : ERROR : Unexpected indentation . /mobly/asserts.py : docstring of mobly.asserts.assert_true:7 : ERROR : Unexpected indentation . /mobly/asserts.py : docstring of mobly.asserts.explicit_pass:10 : ERROR : Unexpected indentation . /mobly/asserts.py : docstring of mobly.asserts.fail:6 : ERROR : Unexpected indentation . /mobly/asserts.py : docstring of mobly.asserts.skip:6 : ERROR : Unexpected indentation . /mobly/asserts.py : docstring of mobly.asserts.skip_if:7 : ERROR : Unexpected indentation . /mobly/base_test.py : docstring of mobly.base_test.BaseTestClass:12 : ERROR : Unexpected indentation . /mobly/base_test.py : docstring of mobly.base_test.BaseTestClass:13 : WARNING : Block quote ends without a blank line ; unexpected unindent . /mobly/base_test.py : docstring of mobly.base_test.BaseTestClass:15 : ERROR : Unexpected indentation . /mobly/base_test.py : docstring of mobly.base_test.BaseTestClass:16 : WARNING : Block quote ends without a blank line ; unexpected unindent . /mobly/base_test.py : docstring of mobly.base_test.BaseTestClass.run:7 : ERROR : Unexpected
Make Mobly run on Windows __EoT__ Mobly is currently unix-only . It would be nice to have Windows support .
The timeout value for ` AndroidDevice._wait_for_device ` is incorrectly implemented __EoT__ ` AndroidDevice._wait_for_device ` calls ` self.adb.wait_for_device ` , which is a blocking call to adb binary . This call might block for longer than ` timeout ` specified ( or forever ) , in which case the timeout value would not be accurate . E.g . if ` AndroidDevice._wait_for_device ` is called with 10s , the ` adb wait_for_device ` can still block for 15s before returning .
TextFormat prints fields in alphabetical order __EoT__ Given this protobuf `` ` protobuf message Date { int32 year = 1 ; int32 month = 2 ; int32 day = 3 ; } `` ` you get `` ` plain day : 1 month : 9 year : 2016 `` ` I think it would be better to order by tag number .
Generate ` Ord ` instances for messages . __EoT__ It would be nice if protolens automatically generated ` Ord ` instances of messages . This could be hidden behind a cabal flag if it turned out to be egregiously slow .
Define our own `` def '' rather than the one from data-default-class __EoT__ The ` data-default-class ` package is ... somewhat controversial . The main criticism is that ` def ` does n't satisfy any laws , so tends to be abused . See for example the following haskell-cafe thread : https : //mail.haskell.org/pipermail/haskell-cafe/2018-May/129053.html Indeed , from our experience using ` proto-lens ` on a larger team , we 've found that ` data-default ` creeps into places it 's not really intended for ; e.g. , using it for an integer value instead of an explicit ` 0 ` . However , for protobufs we *do* have a useful law , namely , ` encodeMessage def == `` '' ` . Note that the ` proto-lens ` library already reexports ` def ` directly : ( ` Data.ProtoLens.Message ` and ` Data.ProtoLens ` ) I propose the following changes : - Remove all our dependencies on ` data-default-class ` ( and ` data-default ` ) - Change the ` Message ` class to define its own method ` def ` : class Message m where ... def : : m Note that this is a breaking change , so
Better support for `` oneof '' fields __EoT__ We currently treat `` oneof '' fields similar to optional fields . This is an intended backwards-compatible behavior of the wire encoding , but makes some use cases more awkward . https : //developers.google.com/protocol-buffers/docs/proto # oneof One possible approach is to store the value as a sum type internally , and provide lenses that return a default value when their case is n't set ( as well as `` maybe'foo '' variants ) . Another option ( less memory efficient ) is to store the fields normally , but make each field 's lens clear out all the other fields when it 's being set .
readMessage wrongly uses Haskell string escaping conventions __EoT__ readMessage is using Haskell string escaping conventions . This will lead it to reading text protocol , e.g . `` \101 '' as decimal , i.e . 'e ' instead of as octal , i.e . 'a ' . I think proto-lens should match the behavior of https : //github.com/google/protobuf/blob/master/src/google/protobuf/io/tokenizer.cc # L1039 .
Do not rename .gs files to .js before pushing __EoT__ # # Expected Behavior When using : clasp push a file named : Code.gs is still named Code.gs ( locally , i 'm not talking about remote ) # # Actual Behavior When using : clasp push a file named : Code.gs is renamed to Code.js The project files should not be changed by the pushing operation . Any preparation to the push operation should be done by the tool , without affecting project files . # # Steps to Reproduce the Problem 1. use clasp push with *.gs files # # Specifications - Version ( ` npm list | grep clasp ` ) : 1.1.5 - OS ( Mac/Linux/Windows ) : all ?
Remove fs write warning when fetching __EoT__ There 's a warning showing when fetching a project : **Warning** : ` ( node:46497 ) [ DEP0013 ] DeprecationWarning : Calling an asynchronous function without callback is deprecated. ` **Node version** : v9.0.0 **Fix** : In ` fetchProject ` , use a proper callback : `` ` js fs.writeFile ( ` ./ $ { filePath } ` , file.source , ( err ) = > { if ( err ) console.error ( err ) ; } ) ; `` `
Make clasp compile without errors with strict flag __EoT__ # # Expected Behavior Clasp compiles without errors using -- strict flag . The strict settings would help catch future bugs and make for a cleaner codebase . # # Actual Behavior Compiling under -- strict shows these errors : `` ` [ anthony @ ambp ] ~/dev/clasp master 2018-05-22 22:21:33 > npm run build > @ google/clasp @ 1.3.0 build /Users/anthony/dev/clasp > tsc -- project tsconfig.json ; npm i -g ; index.ts ( 216,17 ) : error TS7006 : Parameter 'answers ' implicitly has an 'any ' type . index.ts ( 218,17 ) : error TS7006 : Parameter 'err ' implicitly has an 'any ' type . index.ts ( 313,17 ) : error TS7034 : Variable 'fileIds ' implicitly has type 'any [ ] ' in some locations where its type can not be determined . index.ts ( 316,15 ) : error TS7005 : Variable 'fileIds ' implicitly has an 'any [ ] ' type . index.ts ( 322,25 ) : error TS7005 : Variable 'fileIds ' implicitly has an 'any [ ] ' type . index.ts ( 323,23 ) : error TS7006 : Parameter 'answers ' implicitly has an
EarlGrey 2.0 Helper Bundle ( Swift ) ca n't find XCTest.h __EoT__ I set up EarlGrey 2.0 according to [ setup.md ] ( https : //github.com/google/EarlGrey/blob/earlgrey2/docs/setup.md ) . [ In Helper Bundle Section ] ( https : //github.com/google/EarlGrey/blob/earlgrey2/docs/swift-white-boxing.md ) , I added Bridging-Header for Helper Bundle , then I got compile error . Error message is ` 'XCTest/XCTest.h ' file not found ` in XCTestCase+GREYSystemAlertHandler.h . How can i resolve this issue . I tried to find what is different to [ FunctionalTests HostDOCategoriesSwift ] ( https : //github.com/google/EarlGrey/tree/earlgrey2/Tests/FunctionalTests ) with my bundle . * Build Settings * Always Embed Swift Standard Libraries * FunctionalTests is ` Yes ` * Enable Bitcode * FunctionalTests is ` No ` I fix these , but i ca n't resolve the issue ...
Add API for shaking device __EoT__ Hello , I 've noticed that Earl Grey does not have `` shake device '' API . I plan to add this as a PR soon . The proposed API will be supported on both device and simulator .
Add API for shaking device __EoT__ Hello , I 've noticed that Earl Grey does not have `` shake device '' API . I plan to add this as a PR soon . The proposed API will be supported on both device and simulator .
Improve earlgrey gem to set up the project easily __EoT__ Proposing the following improvements in the earlgrey gem : 1. remove post_install , earlgrey.pod would automatically set up the target : `` ` ruby target `` ExampleTests '' do pod 'EarlGrey' end # remove post_install `` ` This will add the copy script and modify the schemes for the target `` ExampleTests '' only when it is a unit test bundle . 2. config xcscheme files per target It would apply required changes to all the schemes that include the given target ; currently , it targets only the given test target ; sometimes , the app target can contain the test target as well . 3. copy EarlGrey.swift when the test target has a swift source code or ` swift_version ` is defined in podfile : `` ` ruby target `` ExampleTests '' do pod 'EarlGrey' swift_version = '3.0' end `` ` Note if the project has ` SWIFT_VERSION ` or ` Use Legacy Swift Language Version ` defined , it will also include the swift file
Add grey_atIndex ( ... ) element matcher __EoT__ Useful for correctly resolving duplicate element issues by index ( picking first , last , etc. ) . Background : https : //github.com/google/EarlGrey/pull/135 ` grey_atIndex ( ... ) ` Use cases - Find nth element in a table - Resolve duplicate match
Document Xcode 8 build-for-testing & test-without-building __EoT__ https : //github.com/fastlane/fastlane/issues/6543 # issuecomment-253849019
Custom action Swift example __EoT__ It 'd be nice to have a simple example of how to do a custom action using Swift .
repeatedly calling gopacket.NewDecodingLayerParser.DecodeLayers to decode DHCP packets causes DHCPOptions to be appended instead of reset __EoT__ something like the following code will cause dhcp options to get appended each time a DHCP packet is processed var ( dhcp layers.DHCPv4 ) parser : = gopacket.NewDecodingLayerParser ( layers.LayerTypeEthernet , & dhcp ) decoded : = [ ] gopacket.LayerType { } for packet : = range packetSource.Packets ( ) { _ = parser.DecodeLayers ( packet.Data ( ) , & decoded ) for _ , layerType : = range decoded { switch layerType { case layers.LayerTypeDHCPv4 : // second time here dhcp.Options will contain data from both packets fmt.Println ( dhcp.Options ) } } }
afpacket unix.Poll does n't report revents which lead in infinite loop __EoT__ Since this commit : https : //github.com/google/gopacket/commit/5527bfcadcaabe78fa8db887e997d3a71171d425 afpacket makes use of unix.Poll instead of C.poll . It seems that this change introduced a bug . The Revents field of pollSet struct is never set , meaning errors are not anymore reported . This can be tested like this : 1. creating a veth pair `` ` ip link add veth0 type veth peer name veth1 ip l set veth1 up ip l set veth0 up `` ` 2. starting capture `` ` package main import ( `` fmt '' `` github.com/google/gopacket/afpacket '' ) func main ( ) { tpacket , _ : = afpacket.NewTPacket ( afpacket.OptInterface ( `` veth0 '' ) , ) fmt.Println ( `` Start capture '' ) for { if _ , _ , err : = tpacket.ReadPacketData ( ) ; err ! = nil { panic ( err ) } } fmt.Println ( `` Capture stopped '' ) } `` ` 3. deleting the veth pair while capturing I did n't find exactly why this happens but we could reuse C.poll instead of unix.Poll . Applying the following patch makes the errors reported again
dhcpv6 __EoT__ Add support for handling dhcpv6 packets https : //en.wikipedia.org/wiki/DHCPv6
dhcpv6 __EoT__ Add support for handling dhcpv6 packets https : //en.wikipedia.org/wiki/DHCPv6
Error in serialization of the IPv6HopByHop layer __EoT__ I 'm experiencing an error when serializing IPv6HopByHop packets . The problem , to be exact , is that the HopByHop header is written twice to the buffer . It 's very easy to replicate this problem : `` ` go // in icmp6hopbyhop_test.go func TestPacketICMPv6WithHopByHop ( t *testing.T ) { // ... // add this after the original code p : = gopacket.NewPacket ( icmp6HopByHopData , LinkTypeEthernet , gopacket.Default ) if p.ErrorLayer ( ) ! = nil { t.Error ( `` Failed to decode packet : '' , p.ErrorLayer ( ) .Error ( ) ) } checkLayers ( p , [ ] gopacket.LayerType { LayerTypeEthernet , LayerTypeIPv6 , LayerTypeIPv6HopByHop , LayerTypeICMPv6 } , t ) checkSerialization ( p , t ) } // I 've added the checkSerialization ( ) function to ` base_test.go ` in my code // Checks that when a serialized version of p is decoded , p and the serialized version of p are the same . // Does not work for packets where the order of options can change , like icmpv6 router advertisements , dhcpv6 , etc . func checkSerialization ( p gopacket.Packet , t
psqworker fails and ca n't be killed if GCS_MOUNT_DIR points to a non existing directory in a non writable parent __EoT__ `` ` $ turbiniactl psqworker [ INFO ] Starting PSQ listener on queue turbinia-psq [ INFO ] Running Turbinia PSQ Worker . [ INFO ] Starting Task StatTask c45b770ca70b4c518a6970a67df6d2b8 [ INFO ] Creating new directory /mnt/turbinia/output/1534207046-c45b770ca70b4c518a6970a67df6d2b8-StatTask [ ERROR ] StatTask Task failed with exception : [ Permission error ( [ Errno 13 ] Permission denied : '/mnt/turbinia ' ) ] [ ERROR ] Traceback ( most recent call last ) : File `` /home/romaing/src/turbinia/turbinia/workers/__init__.py '' , line 399 , in run_wrapper self.result = self.setup ( evidence ) File `` /home/romaing/src/turbinia/turbinia/workers/__init__.py '' , line 318 , in setup self.output_manager.setup ( self ) File `` /home/romaing/src/turbinia/turbinia/output_manager.py '' , line 156 , in setup self._output_writers = self.get_output_writers ( task ) File `` /home/romaing/src/turbinia/turbinia/output_manager.py '' , line 64 , in get_output_writers unique_dir=unique_dir ) ] File `` /home/romaing/src/turbinia/turbinia/output_manager.py '' , line 221 , in __init__ super ( LocalOutputWriter , self ) .__init__ ( *args , **kwargs ) File `` /home/romaing/src/turbinia/turbinia/output_manager.py '' , line 177 , in __init__ self.create_output_dir ( ) File `` /home/romaing/src/turbinia/turbinia/output_manager.py '' , line 235 , in create_output_dir raise TurbiniaException ( msg
turbinia_job_graph.py does n't support new job manager __EoT__ turbinia_job_graph.py needs to be updated to support the new job manager ( from # 257 ) .
Selective imports of Celery/Redis __EoT__ In # 182 we added support for alternate local backends ( yay ! ) , and in setup.py we have the dependencies properly segregated , but some of the modules common to both installation types still load the celery/redis modules . We should find a way to guard these against import in the non-local case . @ ericzinnikas Any chance you want to take a look at this ? : ) . I have n't looked to closely , so I 'm not sure what the best solution is , but we could try to check the config and selectively load them . Or we could load them further down in the file closer to the code that uses it . At worst we could add a try/except around them . LMK if you might have time for this , otherwise I 'll try to take a look at it soon . Thanks !
Selective imports of Celery/Redis __EoT__ In # 182 we added support for alternate local backends ( yay ! ) , and in setup.py we have the dependencies properly segregated , but some of the modules common to both installation types still load the celery/redis modules . We should find a way to guard these against import in the non-local case . @ ericzinnikas Any chance you want to take a look at this ? : ) . I have n't looked to closely , so I 'm not sure what the best solution is , but we could try to check the config and selectively load them . Or we could load them further down in the file closer to the code that uses it . At worst we could add a try/except around them . LMK if you might have time for this , otherwise I 'll try to take a look at it soon . Thanks !
Attach cloud disks __EoT__ ... This potentially means implementing pre-processors as well .
Attach cloud disks __EoT__ ... This potentially means implementing pre-processors as well .
Tests do n't run without GCP credentials __EoT__ We need to mock the calls to create_state_manager .
Turbiniactl status __EoT__ Currently , ` turbiniactl status ` invokes cloud functions that live on the zone specified in the config file : https : //github.com/google/turbinia/blob/master/turbiniactl # L440 Based on my observation in my GCP , deploy_gcf.py pushed the function definition files to GCF on US-CENTRAL1 from my GCS instance on US-WEST1 . However , when running turbiniactl status , it made an attempt of invoking the functions on US-WEST1 . Obviously , this failed because they do n't exist on that region . `` ` $ ./turbiniactl status [ DEBUG ] Setting up PSQ Task Manager requirements on project testtest-turbinia [ DEBUG ] Connecting to PubSub Subscription on turbinia-dev [ DEBUG ] Calling Cloud Function [ gettasks ] with args [ { u'instance ' : u'turbinia-dev ' , u'kind ' : u'TurbiniaTask ' } ] Traceback ( most recent call last ) : File `` ./turbiniactl '' , line 455 , in < module > request_id=args.request_id , all_fields=args.all_fields ) File `` ./turbiniactl '' , line 149 , in PrintTaskStatus request_id ) File `` ./turbiniactl '' , line 112 , in GetTaskData response = function.ExecuteFunction ( 'gettasks ' , func_args ) File `` /home/turbinia/turbinia/turbinia/lib/google_cloud.py '' , line 231 , in
Programatically setting config file __EoT__ I am trying to write tests for the dftimewolf turbinia module . Loading the configuration on a system that does n't have one will fail , so I am trying to manually feed in a test config data to see if the module behaves as expected . I tried setting the ` TURBINIA_CONFIG_PATH ` environment variable , but this just *adds* the path the list of possible config paths . This would work in a pristine test environment , but it will break in my dev setup where I already have a production turbinia config file set up . What do you think of giving ` TURBINIA_CONFIG_PATH ` environment variable precedence over the other potential config locations ?
Connect PubSub code for incoming evidence processing __EoT__
Handle separate dependencies as `` extras '' __EoT__ Now that we 're adding new local backend types , we should try to separate our dependencies into different package sets . One way to do that is with package `` extras '' as per here : http : //setuptools.readthedocs.io/en/latest/setuptools.html # declaring-extras-optional-features-with-their-own-dependencies Full set of dependencies by role . | gcp-client | gcp-worker | gcp-server | local-client | local-worker | local-server -- | -- | -- | -- | -- | -- | -- PSQ | x | x | x | | | Celery / redis | | | | | x | x kombu | | | | x | | x Plaso | | x | | | x | GoogleCloud | x | x | x | | | Proposed simplified package groups : | gcp | gcp-worker | local | local-worker -- | -- | -- | -- | -- PSQ | x | x | | Celery / redis / kombu | | | x | x Plaso | | x | | x GoogleCloud | x | x | | Notes : * The simplified roles combines the client and the server ( for cloud
Add username to request/tasks __EoT__ This might be somewhere in the pubsub message context so we do n't actually have to add it to the client itself , but can read it implicitly for each request . Should also print/filter this out in 'turbiniactl status ' .
Print version on start __EoT__ We should output the version when Turbinia starts to make it easier to determine what is running ( especially if it 's been running for a while ) .
Schedule : Filter - remove breadcrumbs when open __EoT__ Breadcrumbs shouldn ’ t be visible w/ filter open ( only when filter is closed ) since you can see your selections when open . @ jeffposnick
Change countdown to 10am __EoT__
Subnav on Attending and Schedule pages : action highlight remains visibile __EoT__ On both the Attending and Schedule pages , if you click on another item in the subnav the action highlight stays instead of going away . You have to click elsewhere for it to go away . @ paulfarning
QA Ph3_Accessibility : My Schedule - Settings link not accessible with keyboard only __EoT__ Using the keyboard only to navigate , the settings link on the My Schedule page ( that appears when the message regarding notifications has not yet been closed ) can not be accessed . Steps : 1 . Go to https : //io-webapp-staging.appspot.com/io2016/ 2 . Sign in 3 . Select Schedule 4 . Select the My Schedule tab 5 . Use the keyboard to navigate to the Settings link on the message regarding notifications 6 . Once landed on Settings tap enter 7 . Note there is no response Chrome 49 Mac OS X 10.11 ! [ gio_629 ] ( https : //cloud.githubusercontent.com/assets/15128124/14443314/454094b0-fff3-11e5-891b-f67a2543d6c1.png )
Wire up TransferServiceProviderRegistry __EoT__ Similar to the AuthServiceProviderRegistry . Implementation was started in # 122 but this needs to be wired up to be ready for use .
Wire up auth in Google contacts transfer extensions __EoT__
Migrate Instagram __EoT__
Travis fails on master when we merge __EoT__ Our builds on master have been failing https : //travis-ci.org/google/data-transfer-project/builds The builds on the PRs have succeeded , before merging to master We seem to have lost a bunch of build history on Travis ( or do n't retain it ) past 7 days ago , but it does show up for 6+ months ago . So hard to say when this started happening but it seems like it could be related to the new dockerhub push in https : //github.com/google/data-transfer-project/pull/474
GoogleCalendarExporter throws NPE __EoT__ When running on the demo server , GoogleCalendarExporter throws an NPE in the return statement of export ( UUID jobId , TokensAndUrlAuthData authData , ExportInformation exportInformation ) .
GoogleCalendarExporter throws NPE __EoT__ When running on the demo server , GoogleCalendarExporter throws an NPE in the return statement of export ( UUID jobId , TokensAndUrlAuthData authData , ExportInformation exportInformation ) .
GoogleCalendarExporter throws NPE __EoT__ When running on the demo server , GoogleCalendarExporter throws an NPE in the return statement of export ( UUID jobId , TokensAndUrlAuthData authData , ExportInformation exportInformation ) .
Implement GoogleTransferExtension __EoT__ blocks # 168
Add support for InputStreams to Google Cloud Storage __EoT__
data-transfer-project/Documentation/Keys.md - error 404 on clicking the link __EoT__ when the link is clicked , it gives a 404 page of Github link - https : //github.com/google/data-transfer-project/blob/master/portability-spi-cloud/src/main/java/org/dataportabilityproject/spi/cloud/storage/AppCredentialStore.java
Rework local secrets to support a publishable docker image __EoT__ We would like to have a docker image we can publish that folks can just add their own API keys to to try out DTP . Right now for local deployments we bake the secrets into the JAR , so we could n't publish the JAR . A better way would be to publish a docker image with no secrets , and then have an easy way to use our image a a base and add in your own secrets/api keys and launch the image .
Rework local secrets to support a publishable docker image __EoT__ We would like to have a docker image we can publish that folks can just add their own API keys to to try out DTP . Right now for local deployments we bake the secrets into the JAR , so we could n't publish the JAR . A better way would be to publish a docker image with no secrets , and then have an easy way to use our image a a base and add in your own secrets/api keys and launch the image .
Update Website and documentation __EoT__ Update developer and integration guide .
firestore_genrule_py_impl should be re-done in Java __EoT__ Python was chosen for speed of implementation , Java should be used for consistency
New design for file-changes page __EoT__ PR : # 418
.circleci/config.yml error __EoT__ `` ` Config Processing Error $ # ! /bin/sh -eo pipefail # Unable to parse YAML # while constructing a mapping # in 'string ' , line 64 , column 11 : # key : \ '' android_sdk\ '' # ^ # found duplicate key key # in 'string ' , line 67 , column 11 : # key : \ '' v3-bazel_cache\ '' # ^ false Exited with code 1 `` ` `` ` $ circleci config validate Error : Unable to parse YAML : while constructing a mapping : in 'string ' , line 64 , column 11 : : key : `` android_sdk '' : ^ : found duplicate key key : in 'string ' , line 67 , column 11 : : key : `` v3-bazel_cache '' : `` `
Fix reviewer webapp README __EoT__ - new line bug - use ` npm ` instead of ` ng ` - more steps # 296
Rename ` tools/buildtools_wrappers ` to ` tools/bazel_tools ` __EoT__
Display diff threads __EoT__ # 296
Move completion.sh to tools __EoT__ This one is really trivial - Move https : //github.com/google/startup-os/blob/master/completion.sh To ` tools `
Ca n't change label colors __EoT__ Hi everyone . I 'm trying to change the default text color from black to white , but everytime i use the ArcLabelDecorator on my pie chart , it says this : > |flutter : ══╡ EXCEPTION CAUGHT BY RENDERING LIBRARY ╞═════════════════════════════════════════════════════════ flutter : The following NoSuchMethodError was thrown during paint ( ) : flutter : The method '/ ' was called on null . flutter : Receiver : null flutter : Tried calling : / ( 2 ) flutter : flutter : When the exception was thrown , this was the stack : flutter : # 0 Object.noSuchMethod ( dart : core/runtime/libobject_patch.dart:48:5 ) flutter : # 1 ArcLabelDecorator._drawOutsideLabel ( package : charts_common/src/chart/pie/arc_label_decorator.dart:282:67 ) flutter : # 2 ArcLabelDecorator.decorate ( package : charts_common/src/chart/pie/arc_label_decorator.dart:197:21 ) flutter : # 3 ArcRenderer.paint. < anonymous closure > . < anonymous closure > ( package : charts_common/src/chart/pie/arc_renderer.dart:374:19 ) flutter : # 4 Iterable.forEach ( dart : core/iterable.dart:277:30 ) flutter : # 5 ArcRenderer.paint. < anonymous closure > ( package : charts_common/src/chart/pie/arc_renderer.dart:373:12 ) flutter : # 6 __InternalLinkedHashMap & _HashVMBase & MapMixin & _LinkedHashMapMixin.forEach ( dart : collection/runtime/libcompact_hash.dart:365:8 ) flutter : # 7 ArcRenderer.paint ( package : charts_common/src/chart/pie/arc_renderer.dart:332:19 ) flutter
Ca n't change label colors __EoT__ Hi everyone . I 'm trying to change the default text color from black to white , but everytime i use the ArcLabelDecorator on my pie chart , it says this : > |flutter : ══╡ EXCEPTION CAUGHT BY RENDERING LIBRARY ╞═════════════════════════════════════════════════════════ flutter : The following NoSuchMethodError was thrown during paint ( ) : flutter : The method '/ ' was called on null . flutter : Receiver : null flutter : Tried calling : / ( 2 ) flutter : flutter : When the exception was thrown , this was the stack : flutter : # 0 Object.noSuchMethod ( dart : core/runtime/libobject_patch.dart:48:5 ) flutter : # 1 ArcLabelDecorator._drawOutsideLabel ( package : charts_common/src/chart/pie/arc_label_decorator.dart:282:67 ) flutter : # 2 ArcLabelDecorator.decorate ( package : charts_common/src/chart/pie/arc_label_decorator.dart:197:21 ) flutter : # 3 ArcRenderer.paint. < anonymous closure > . < anonymous closure > ( package : charts_common/src/chart/pie/arc_renderer.dart:374:19 ) flutter : # 4 Iterable.forEach ( dart : core/iterable.dart:277:30 ) flutter : # 5 ArcRenderer.paint. < anonymous closure > ( package : charts_common/src/chart/pie/arc_renderer.dart:373:12 ) flutter : # 6 __InternalLinkedHashMap & _HashVMBase & MapMixin & _LinkedHashMapMixin.forEach ( dart : collection/runtime/libcompact_hash.dart:365:8 ) flutter : # 7 ArcRenderer.paint ( package : charts_common/src/chart/pie/arc_renderer.dart:332:19 ) flutter
Ca n't change label colors __EoT__ Hi everyone . I 'm trying to change the default text color from black to white , but everytime i use the ArcLabelDecorator on my pie chart , it says this : > |flutter : ══╡ EXCEPTION CAUGHT BY RENDERING LIBRARY ╞═════════════════════════════════════════════════════════ flutter : The following NoSuchMethodError was thrown during paint ( ) : flutter : The method '/ ' was called on null . flutter : Receiver : null flutter : Tried calling : / ( 2 ) flutter : flutter : When the exception was thrown , this was the stack : flutter : # 0 Object.noSuchMethod ( dart : core/runtime/libobject_patch.dart:48:5 ) flutter : # 1 ArcLabelDecorator._drawOutsideLabel ( package : charts_common/src/chart/pie/arc_label_decorator.dart:282:67 ) flutter : # 2 ArcLabelDecorator.decorate ( package : charts_common/src/chart/pie/arc_label_decorator.dart:197:21 ) flutter : # 3 ArcRenderer.paint. < anonymous closure > . < anonymous closure > ( package : charts_common/src/chart/pie/arc_renderer.dart:374:19 ) flutter : # 4 Iterable.forEach ( dart : core/iterable.dart:277:30 ) flutter : # 5 ArcRenderer.paint. < anonymous closure > ( package : charts_common/src/chart/pie/arc_renderer.dart:373:12 ) flutter : # 6 __InternalLinkedHashMap & _HashVMBase & MapMixin & _LinkedHashMapMixin.forEach ( dart : collection/runtime/libcompact_hash.dart:365:8 ) flutter : # 7 ArcRenderer.paint ( package : charts_common/src/chart/pie/arc_renderer.dart:332:19 ) flutter
Ca n't change label colors __EoT__ Hi everyone . I 'm trying to change the default text color from black to white , but everytime i use the ArcLabelDecorator on my pie chart , it says this : > |flutter : ══╡ EXCEPTION CAUGHT BY RENDERING LIBRARY ╞═════════════════════════════════════════════════════════ flutter : The following NoSuchMethodError was thrown during paint ( ) : flutter : The method '/ ' was called on null . flutter : Receiver : null flutter : Tried calling : / ( 2 ) flutter : flutter : When the exception was thrown , this was the stack : flutter : # 0 Object.noSuchMethod ( dart : core/runtime/libobject_patch.dart:48:5 ) flutter : # 1 ArcLabelDecorator._drawOutsideLabel ( package : charts_common/src/chart/pie/arc_label_decorator.dart:282:67 ) flutter : # 2 ArcLabelDecorator.decorate ( package : charts_common/src/chart/pie/arc_label_decorator.dart:197:21 ) flutter : # 3 ArcRenderer.paint. < anonymous closure > . < anonymous closure > ( package : charts_common/src/chart/pie/arc_renderer.dart:374:19 ) flutter : # 4 Iterable.forEach ( dart : core/iterable.dart:277:30 ) flutter : # 5 ArcRenderer.paint. < anonymous closure > ( package : charts_common/src/chart/pie/arc_renderer.dart:373:12 ) flutter : # 6 __InternalLinkedHashMap & _HashVMBase & MapMixin & _LinkedHashMapMixin.forEach ( dart : collection/runtime/libcompact_hash.dart:365:8 ) flutter : # 7 ArcRenderer.paint ( package : charts_common/src/chart/pie/arc_renderer.dart:332:19 ) flutter
Ca n't change label colors __EoT__ Hi everyone . I 'm trying to change the default text color from black to white , but everytime i use the ArcLabelDecorator on my pie chart , it says this : > |flutter : ══╡ EXCEPTION CAUGHT BY RENDERING LIBRARY ╞═════════════════════════════════════════════════════════ flutter : The following NoSuchMethodError was thrown during paint ( ) : flutter : The method '/ ' was called on null . flutter : Receiver : null flutter : Tried calling : / ( 2 ) flutter : flutter : When the exception was thrown , this was the stack : flutter : # 0 Object.noSuchMethod ( dart : core/runtime/libobject_patch.dart:48:5 ) flutter : # 1 ArcLabelDecorator._drawOutsideLabel ( package : charts_common/src/chart/pie/arc_label_decorator.dart:282:67 ) flutter : # 2 ArcLabelDecorator.decorate ( package : charts_common/src/chart/pie/arc_label_decorator.dart:197:21 ) flutter : # 3 ArcRenderer.paint. < anonymous closure > . < anonymous closure > ( package : charts_common/src/chart/pie/arc_renderer.dart:374:19 ) flutter : # 4 Iterable.forEach ( dart : core/iterable.dart:277:30 ) flutter : # 5 ArcRenderer.paint. < anonymous closure > ( package : charts_common/src/chart/pie/arc_renderer.dart:373:12 ) flutter : # 6 __InternalLinkedHashMap & _HashVMBase & MapMixin & _LinkedHashMapMixin.forEach ( dart : collection/runtime/libcompact_hash.dart:365:8 ) flutter : # 7 ArcRenderer.paint ( package : charts_common/src/chart/pie/arc_renderer.dart:332:19 ) flutter
Ca n't change label colors __EoT__ Hi everyone . I 'm trying to change the default text color from black to white , but everytime i use the ArcLabelDecorator on my pie chart , it says this : > |flutter : ══╡ EXCEPTION CAUGHT BY RENDERING LIBRARY ╞═════════════════════════════════════════════════════════ flutter : The following NoSuchMethodError was thrown during paint ( ) : flutter : The method '/ ' was called on null . flutter : Receiver : null flutter : Tried calling : / ( 2 ) flutter : flutter : When the exception was thrown , this was the stack : flutter : # 0 Object.noSuchMethod ( dart : core/runtime/libobject_patch.dart:48:5 ) flutter : # 1 ArcLabelDecorator._drawOutsideLabel ( package : charts_common/src/chart/pie/arc_label_decorator.dart:282:67 ) flutter : # 2 ArcLabelDecorator.decorate ( package : charts_common/src/chart/pie/arc_label_decorator.dart:197:21 ) flutter : # 3 ArcRenderer.paint. < anonymous closure > . < anonymous closure > ( package : charts_common/src/chart/pie/arc_renderer.dart:374:19 ) flutter : # 4 Iterable.forEach ( dart : core/iterable.dart:277:30 ) flutter : # 5 ArcRenderer.paint. < anonymous closure > ( package : charts_common/src/chart/pie/arc_renderer.dart:373:12 ) flutter : # 6 __InternalLinkedHashMap & _HashVMBase & MapMixin & _LinkedHashMapMixin.forEach ( dart : collection/runtime/libcompact_hash.dart:365:8 ) flutter : # 7 ArcRenderer.paint ( package : charts_common/src/chart/pie/arc_renderer.dart:332:19 ) flutter
Ca n't change label colors __EoT__ Hi everyone . I 'm trying to change the default text color from black to white , but everytime i use the ArcLabelDecorator on my pie chart , it says this : > |flutter : ══╡ EXCEPTION CAUGHT BY RENDERING LIBRARY ╞═════════════════════════════════════════════════════════ flutter : The following NoSuchMethodError was thrown during paint ( ) : flutter : The method '/ ' was called on null . flutter : Receiver : null flutter : Tried calling : / ( 2 ) flutter : flutter : When the exception was thrown , this was the stack : flutter : # 0 Object.noSuchMethod ( dart : core/runtime/libobject_patch.dart:48:5 ) flutter : # 1 ArcLabelDecorator._drawOutsideLabel ( package : charts_common/src/chart/pie/arc_label_decorator.dart:282:67 ) flutter : # 2 ArcLabelDecorator.decorate ( package : charts_common/src/chart/pie/arc_label_decorator.dart:197:21 ) flutter : # 3 ArcRenderer.paint. < anonymous closure > . < anonymous closure > ( package : charts_common/src/chart/pie/arc_renderer.dart:374:19 ) flutter : # 4 Iterable.forEach ( dart : core/iterable.dart:277:30 ) flutter : # 5 ArcRenderer.paint. < anonymous closure > ( package : charts_common/src/chart/pie/arc_renderer.dart:373:12 ) flutter : # 6 __InternalLinkedHashMap & _HashVMBase & MapMixin & _LinkedHashMapMixin.forEach ( dart : collection/runtime/libcompact_hash.dart:365:8 ) flutter : # 7 ArcRenderer.paint ( package : charts_common/src/chart/pie/arc_renderer.dart:332:19 ) flutter
Ca n't change label colors __EoT__ Hi everyone . I 'm trying to change the default text color from black to white , but everytime i use the ArcLabelDecorator on my pie chart , it says this : > |flutter : ══╡ EXCEPTION CAUGHT BY RENDERING LIBRARY ╞═════════════════════════════════════════════════════════ flutter : The following NoSuchMethodError was thrown during paint ( ) : flutter : The method '/ ' was called on null . flutter : Receiver : null flutter : Tried calling : / ( 2 ) flutter : flutter : When the exception was thrown , this was the stack : flutter : # 0 Object.noSuchMethod ( dart : core/runtime/libobject_patch.dart:48:5 ) flutter : # 1 ArcLabelDecorator._drawOutsideLabel ( package : charts_common/src/chart/pie/arc_label_decorator.dart:282:67 ) flutter : # 2 ArcLabelDecorator.decorate ( package : charts_common/src/chart/pie/arc_label_decorator.dart:197:21 ) flutter : # 3 ArcRenderer.paint. < anonymous closure > . < anonymous closure > ( package : charts_common/src/chart/pie/arc_renderer.dart:374:19 ) flutter : # 4 Iterable.forEach ( dart : core/iterable.dart:277:30 ) flutter : # 5 ArcRenderer.paint. < anonymous closure > ( package : charts_common/src/chart/pie/arc_renderer.dart:373:12 ) flutter : # 6 __InternalLinkedHashMap & _HashVMBase & MapMixin & _LinkedHashMapMixin.forEach ( dart : collection/runtime/libcompact_hash.dart:365:8 ) flutter : # 7 ArcRenderer.paint ( package : charts_common/src/chart/pie/arc_renderer.dart:332:19 ) flutter
Ca n't change label colors __EoT__ Hi everyone . I 'm trying to change the default text color from black to white , but everytime i use the ArcLabelDecorator on my pie chart , it says this : > |flutter : ══╡ EXCEPTION CAUGHT BY RENDERING LIBRARY ╞═════════════════════════════════════════════════════════ flutter : The following NoSuchMethodError was thrown during paint ( ) : flutter : The method '/ ' was called on null . flutter : Receiver : null flutter : Tried calling : / ( 2 ) flutter : flutter : When the exception was thrown , this was the stack : flutter : # 0 Object.noSuchMethod ( dart : core/runtime/libobject_patch.dart:48:5 ) flutter : # 1 ArcLabelDecorator._drawOutsideLabel ( package : charts_common/src/chart/pie/arc_label_decorator.dart:282:67 ) flutter : # 2 ArcLabelDecorator.decorate ( package : charts_common/src/chart/pie/arc_label_decorator.dart:197:21 ) flutter : # 3 ArcRenderer.paint. < anonymous closure > . < anonymous closure > ( package : charts_common/src/chart/pie/arc_renderer.dart:374:19 ) flutter : # 4 Iterable.forEach ( dart : core/iterable.dart:277:30 ) flutter : # 5 ArcRenderer.paint. < anonymous closure > ( package : charts_common/src/chart/pie/arc_renderer.dart:373:12 ) flutter : # 6 __InternalLinkedHashMap & _HashVMBase & MapMixin & _LinkedHashMapMixin.forEach ( dart : collection/runtime/libcompact_hash.dart:365:8 ) flutter : # 7 ArcRenderer.paint ( package : charts_common/src/chart/pie/arc_renderer.dart:332:19 ) flutter
Ca n't change label colors __EoT__ Hi everyone . I 'm trying to change the default text color from black to white , but everytime i use the ArcLabelDecorator on my pie chart , it says this : > |flutter : ══╡ EXCEPTION CAUGHT BY RENDERING LIBRARY ╞═════════════════════════════════════════════════════════ flutter : The following NoSuchMethodError was thrown during paint ( ) : flutter : The method '/ ' was called on null . flutter : Receiver : null flutter : Tried calling : / ( 2 ) flutter : flutter : When the exception was thrown , this was the stack : flutter : # 0 Object.noSuchMethod ( dart : core/runtime/libobject_patch.dart:48:5 ) flutter : # 1 ArcLabelDecorator._drawOutsideLabel ( package : charts_common/src/chart/pie/arc_label_decorator.dart:282:67 ) flutter : # 2 ArcLabelDecorator.decorate ( package : charts_common/src/chart/pie/arc_label_decorator.dart:197:21 ) flutter : # 3 ArcRenderer.paint. < anonymous closure > . < anonymous closure > ( package : charts_common/src/chart/pie/arc_renderer.dart:374:19 ) flutter : # 4 Iterable.forEach ( dart : core/iterable.dart:277:30 ) flutter : # 5 ArcRenderer.paint. < anonymous closure > ( package : charts_common/src/chart/pie/arc_renderer.dart:373:12 ) flutter : # 6 __InternalLinkedHashMap & _HashVMBase & MapMixin & _LinkedHashMapMixin.forEach ( dart : collection/runtime/libcompact_hash.dart:365:8 ) flutter : # 7 ArcRenderer.paint ( package : charts_common/src/chart/pie/arc_renderer.dart:332:19 ) flutter
Area And Line Line Chart Example __EoT__ Area And Line Line Chart Example is not working
Area And Line Line Chart Example __EoT__ Area And Line Line Chart Example is not working
Area And Line Line Chart Example __EoT__ Area And Line Line Chart Example is not working
Area And Line Line Chart Example __EoT__ Area And Line Line Chart Example is not working
Area And Line Line Chart Example __EoT__ Area And Line Line Chart Example is not working
Area And Line Line Chart Example __EoT__ Area And Line Line Chart Example is not working
Area And Line Line Chart Example __EoT__ Area And Line Line Chart Example is not working
Area And Line Line Chart Example __EoT__ Area And Line Line Chart Example is not working
Area And Line Line Chart Example __EoT__ Area And Line Line Chart Example is not working
Area And Line Line Chart Example __EoT__ Area And Line Line Chart Example is not working
Area And Line Line Chart Example __EoT__ Area And Line Line Chart Example is not working
Deeclare compatibility with Dart 2 stable __EoT__ The Dart SDK constraints need to be updated to [ declare compatibility with Dart 2 stable ] ( https : //www.dartlang.org/dart-2 # upper-constraints-on-the-sdk-version ) : 1. https : //github.com/google/charts/blob/master/charts_common/pubspec.yaml # L8 1. https : //github.com/google/charts/blob/master/charts_flutter/pubspec.yaml # L8
Add build support for Windows __EoT__
Add build support for Windows __EoT__
Tons of UnsafeFinalization warnings in the gradle build __EoT__ Nothing in the history immediately sticks out as the culprit . We should investigate to see if these are legitimate issues or if we should suppress the warning .
Investigate handshake performance __EoT__ We seem to be trailing Netty tcnative in handshake performance . Profiling shows that the we 're spending a lot of time in ` sun.security.ssl.X509TrustManagerImpl.checkServerTrusted ( X509Certificate [ ] , String , SSLEngine ) ` . I suspect that Netty may be handling server verification differently . `` ` Benchmark ( a_cipher ) ( b_buffer ) ( c_engine ) Mode Cnt Score Error Units JmhEngineHandshakeBenchmark.hs TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 HEAP JDK thrpt 10 162.348 ± 10.382 ops/s JmhEngineHandshakeBenchmark.hs TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 HEAP CONSCRYPT_UNPOOLED thrpt 10 716.748 ± 102.178 ops/s JmhEngineHandshakeBenchmark.hs TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 HEAP CONSCRYPT_POOLED thrpt 10 742.854 ± 150.313 ops/s JmhEngineHandshakeBenchmark.hs TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 HEAP NETTY thrpt 10 1236.226 ± 9.631 ops/s JmhEngineHandshakeBenchmark.hs TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 HEAP NETTY_REF_CNT thrpt 10 1199.334 ± 97.748 ops/s JmhEngineHandshakeBenchmark.hs TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 DIRECT JDK thrpt 10 163.654 ± 9.940 ops/s JmhEngineHandshakeBenchmark.hs TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 DIRECT CONSCRYPT_UNPOOLED thrpt 10 755.786 ± 142.212 ops/s JmhEngineHandshakeBenchmark.hs TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 DIRECT CONSCRYPT_POOLED thrpt 10 755.845 ± 135.823 ops/s JmhEngineHandshakeBenchmark.hs TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 DIRECT NETTY thrpt 10 1247.186 ± 9.735 ops/s JmhEngineHandshakeBenchmark.hs TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 DIRECT NETTY_REF_CNT thrpt 10 1217.178 ± 79.153 ops/s `` `
Exception when enabling unsupported TLSv1.3 ciphers __EoT__ This is maybe valid generally , but conflicts with this decision IMHO https : //github.com/google/conscrypt/pull/524 # discussion_r207832229 `` ` Exception in thread `` main '' java.lang.IllegalArgumentException : cipherSuite TLS_AES_128_CCM_SHA256 is not supported . at org.conscrypt.NativeCrypto.checkEnabledCipherSuites ( NativeCrypto.java:1083 ) at org.conscrypt.SSLParametersImpl.setEnabledCipherSuites ( SSLParametersImpl.java:221 ) at org.conscrypt.ConscryptEngine.setEnabledCipherSuites ( ConscryptEngine.java:649 ) at org.conscrypt.ConscryptEngineSocket.setEnabledCipherSuites ( ConscryptEngineSocket.java:289 ) at okhttp3.ConnectionSpec.apply ( ConnectionSpec.java:158 ) at okhttp3.OkHttpClient $ 1.apply ( OkHttpClient.java:176 ) at okhttp3.internal.connection.ConnectionSpecSelector.configureSecureSocket ( ConnectionSpecSelector.java:79 ) at okhttp3.internal.connection.RealConnection.connectTls ( RealConnection.java:311 ) at okhttp3.internal.connection.RealConnection.establishProtocol ( RealConnection.java:282 ) at okhttp3.internal.connection.RealConnection.connect ( RealConnection.java:167 ) at okhttp3.internal.connection.StreamAllocation.findConnection ( StreamAllocation.java:257 ) at okhttp3.internal.connection.StreamAllocation.findHealthyConnection ( StreamAllocation.java:135 ) at okhttp3.internal.connection.StreamAllocation.newStream ( StreamAllocation.java:114 ) at okhttp3.internal.connection.ConnectInterceptor.intercept ( ConnectInterceptor.java:42 ) at okhttp3.internal.http.RealInterceptorChain.proceed ( RealInterceptorChain.java:147 ) at okhttp3.internal.http.RealInterceptorChain.proceed ( RealInterceptorChain.java:121 ) at okhttp3.internal.cache.CacheInterceptor.intercept ( CacheInterceptor.java:93 ) at okhttp3.internal.http.RealInterceptorChain.proceed ( RealInterceptorChain.java:147 ) at okhttp3.internal.http.RealInterceptorChain.proceed ( RealInterceptorChain.java:121 ) at okhttp3.internal.http.BridgeInterceptor.intercept ( BridgeInterceptor.java:93 ) at okhttp3.internal.http.RealInterceptorChain.proceed ( RealInterceptorChain.java:147 ) at okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept ( RetryAndFollowUpInterceptor.java:126 ) at okhttp3.internal.http.RealInterceptorChain.proceed ( RealInterceptorChain.java:147 ) at okhttp3.internal.http.RealInterceptorChain.proceed ( RealInterceptorChain.java:121 ) at okhttp3.RealCall.getResponseWithInterceptorChain ( RealCall.java:200 ) at okhttp3.RealCall.execute ( RealCall.java:77 ) at okhttp3.TestTls13Request.sendRequest ( TestTls13Request.java:106 ) at okhttp3.TestTls13Request.testClient ( TestTls13Request.java:80 ) at okhttp3.TestTls13Request.main ( TestTls13Request.java:74 ) `` `
Enable logging in non-Android builds __EoT__ Conscrypt uses Android logging macros in native code ( ALOG , ALOGD , ALOGE , ALOGV ) . These are [ set to no-ops in non-Android builds ] ( https : //github.com/google/conscrypt/blob/0f225670ce56868975e376496b135fb89dc169ab/common/src/jni/main/include/conscrypt/macros.h # L183 ) , making it difficult to debug native errors on OpenJDK builds . Messages should be logged to stdout or stderr at least for the most severe log levels ( ALOGE ? ) .
Add versioning/flags mechanism to Conscrypt __EoT__ e.g . Conscrypt.getApiVersion ( ) It 's awkward to call new methods like ` Conscrypt.newProviderBuilder ( ) .provideTrustManager ( ) .build ( ) ` without being able to check user has upgraded the library . As a secondary issue , it would be useful if clients need to call this . Presumably in JDK 11 the default trust manager should be preferred ? Should all clienst encode the logic to check conscrypt and jdk versions before triggering that path ?
ConscryptEngineSocket.SSLInputStream does not pool direct buffers __EoT__ Direct buffers can be allocated at a much higher rate than they can be freed , resulting in direct memory OOMs .
Bundle native artifact for Android __EoT__
Remove conscrypt-android-stubs from Maven repo __EoT__ ` conscrypt-android-stubs ` is a build intermediate step . It should n't be compiled into any shipped library and probably should n't be in the Maven repo at all .
Issuers not passed thru chooseClientAlias __EoT__ Hi , I 'm trying to implement my own X509KeyManager and am running into an issue implementing the chooseClientAlias method . I 'm always getting null passed in for the issuers . This seems to be due to the conscrypt OpenSSLSocketImpl not passing the issuers through . https : //android.googlesource.com/platform/external/conscrypt/+/marshmallow-dev/src/main/java/org/conscrypt/OpenSSLSocketImpl.java @ Override public String chooseClientAlias ( X509KeyManager keyManager , X500Principal [ ] issuers , String [ ] keyTypes ) { return keyManager.chooseClientAlias ( keyTypes , null , this ) ; } In the debugger , I can step back up to the OpenSSLSocketImpl.chooseClientAlias method and see that the issuers are there . Is there a reason that issuers are n't passed through to the keyManager ? Is there another recommended way to implement KeyManagers for Android that work with conscrypt ? Thank you !
Conscrypt TrustManager does not provide feedback when certs are ignored due to invalidity __EoT__ ` ChainStrengthAnalyzer.checkCert ` is used to disallow potentially insecure certificates , unfortunately these failures manifest as ` java.security.cert.CertPathValidatorException : Trust anchor for certification path not found. ` without any mention of ignored certificates . Would it be reasonable to log a warning when the certificate validity check in TrustManagerImpl fails ? Eventually , pending java 8 target , we may want to use ` Throwable.addSuppressed ( Throwable ) ` instead .
Add conscrypt-android to Maven repo __EoT__ Currently it seems that ` conscrypt-android ` does n't deploy its ` .aar ` files to the Maven repository . This is what is needed for Android deployments , so it should be on there . It looks like a missing hook in the package/deploy steps .
Add JMH benchmarks for OpenJDK __EoT__
Add JMH benchmarks for OpenJDK __EoT__
Add integration tests __EoT__ We need a separate integration tests that will be run against platform ( i.e . bundled ) Conscrypt . Why : We 're currently using our unit tests for testing the native platform , but even though the packages are the same for the platform and tests ( i.e . org.conscrypt ) they are considered separate packages due to the fact that they came from separate class loaders ( platform is loaded by the boot classloader ) . In order to allow us to lockdown the API ( # 142 ) by making internal classes package-private , we 'll need to ensure that the tests run on Android do not attempt to access those classes . One option would be to always use java reflection in our unit tests , but this seems like overkill . Instead , we 're going to cherry-pick a few tests to a new ` integration-test ` module and have those be the tests that we run on Android .
No ellipsis for Amaranth __EoT__ Horizontal ellipsis ( U+2026 ) is missing for the beautiful Amaranth font .
Oswald : interpolation errors in ‘ : ’ and ‘ „ ’ __EoT__ : ( U+003A ) and „ ( U+201E ) have interpolation errors in Medium , Semibold and Bold . ! [ captura de pantalla de 2017-02-15 01-26-54 ] ( https : //cloud.githubusercontent.com/assets/554953/22964376/e9f0e230-f31d-11e6-8fbd-e519af366cc1.png )
Oswald Medium and Semibold colon spacing bug __EoT__ Colons are squashed together in font weights 500 ( Medium ) and 600 ( Semibold ) . https : //fonts.google.com/specimen/Oswald
Questions about glyph sets __EoT__ @ alexeiva and @ kalapi , I have some questions about the glyph sets : - [ x ] Latin Pro set : There are two sets of numbers for the same function , ` .subs ` and ` .inferior ` - should one be dropped from the definition ? - [ x ] Expert set : Glyphs suggests the name ` schwa-cy.sc ` instead of ` schwa.sc ` - and the Glyphs name allows for auto-generation of the ` smcp ` feature , so perhaps ` schwa.sc ` should be renamed in the definition ? - [ x ] Expert set : The glyph ` germandbls.sc ` is common , but there is also ` germandbls.alt.sc ` - what is this ? Any fonts with this as examples ? : )
caron alt __EoT__ caronalt should be changed to caroncomb.alt , otherwise Glyphs will not recognize it as a component .
caron alt __EoT__ caronalt should be changed to caroncomb.alt , otherwise Glyphs will not recognize it as a component .
Reformatting 'README.md ' in tools/encodings/GF 2016 Glyph Sets/ __EoT__ # # # # 1 . Re-Arrange Text Move the 'Language support ' list section to after the 'Structure and Heirarchy ' section . # # # # 2 . Update Some Character Set Descriptions **Note** : Instead of listing the Unicode Ranges under the 'Google Latin Plus ' section , it might be a useful to create a 'Unicode Ranges ' sub-section and list all the ranges that the largest set ( Expert ) covers . This is mainly because the unicode ranges list may be a bit misleading . For e.g . the Latin Extended Additional block supports things like 'Medievalist Additions ' . The Latin Extended B block supports 'African Linguistics ' , and 'Non-European and Historic Latin ' . So the new contents would be : # # # # # Google Latin Plus ( Total Glyphs : 600 ) - Western & Central European - Vietnamese - Currencies ( ₡ ₣ ₤ ₦ ₧ ₩ ₫ ₭ ₱ ₲ ₵ ₹ ₺ ₼ ₽ ) - Alternate Numerals : Proportonal Lining # # # # # Google Latin Pro ( Total Glyphs : 151 ) -
Maven Pro horizontal ellipsis __EoT__ The horizontal ellipsis in Maven Pro Bold and Medium has only 2 dots , not three in this repo . Looks good on [ GF ] ( https : //fonts.google.com/specimen/Maven+Pro ) , though . ! [ 20161207_google_fonts_maven_pro ] ( https : //cloud.githubusercontent.com/assets/802702/21029749/77cb0206-bd9b-11e6-94f8-62d28cb98b7c.PNG )
Update Muli to include full family __EoT__ Currently only two weights of the full family are represented , is it possible to get the remaining ones added ? [ vernnobile/MuliFont ] ( https : //github.com/vernnobile/MuliFont )
[ 2016 Glyph Sets ] oneinferior should be one.subs __EoT__ oneinferior should be one.subs twoinferior should be two.subs etc ... . As per https : //www.glyphsapp.com/tutorials/superscript-and-subscript-figures
Merriweather cyrillic uppercase Ы letter __EoT__ Chrome Mac os sierra 10.12.2 Google fonts website , lowecase letter ` ы ` is okay , uppercase ` Ы ` is not . < img width= '' 325 '' alt= '' screen shot 2017-01-22 at 11 11 51 am '' src= '' https : //cloud.githubusercontent.com/assets/6711845/22181039/ebc9f6f6-e093-11e6-9e97-baf577160dae.png '' >
Update Barlow font family ( 1.101-Pre Release - > 1.208 ) __EoT__ It will be nice if you can update Barlow font family to latest version , because current one have some issues with non english characters/languages . Git Repo : https : //github.com/jpt/barlow Detailed changelog : https : //github.com/jpt/barlow/releases Changes summary : - kerning fixed on a number of non-English characters - various spacing and other kerring tweaks - optimisation to Condensed Black variant - and more ... Refs # 1330
Neuton font 300 weight displays colon as middle dot __EoT__ Typing a ` : ` character results in a middle dot with Neuton 's 300 weight : < img width= '' 809 '' alt= '' neuton_-_google_fonts '' src= '' https : //cloud.githubusercontent.com/assets/647669/19959083/db8ad152-a19b-11e6-85c7-8355ed5dce17.png '' > I found a report of this in a repo at https : //github.com/vladpopa33/googlefontdirectory/issues/215 dating back to 2013 but could n't see anything in the official repo here .
https+insecure does n't extend to the symbolizer __EoT__ https : //go-review.googlesource.com/ # /c/33157/ changed ` httpGet ` to skip certificate verification on HTTP GET , but symbolization makes an extra request using HTTP POST , which silently fails . https : //github.com/google/pprof/blob/6addbe4/internal/symbolizer/symbolizer.go # L82 cc @ bdarnell EDIT : the real error is : `` ` shell $ pprof https+insecure : //127.0.0.1:8080/debug/pprof/profile Fetching profile over HTTP from https+insecure : //127.0.0.1:8080/debug/pprof/profile pprof : http post https+insecure : //127.0.0.1:8080/debug/pprof/symbol : Post https+insecure : //127.0.0.1:8080/debug/pprof/symbol : unsupported protocol scheme `` https+insecure '' `` `
Web UI should support flame graphs __EoT__ ! [ image ] ( http : //www.brendangregg.com/FlameGraphs/cpu-mysql-updated.svg ) [ Flame graphs ] ( http : //www.brendangregg.com/flamegraphs.html ) allow you to move in a specific ancestry path from a very compact UI and lets the users to zoom in/out specific paths easily . Flame graphs became one of the defato representation of profiling data in the industry in the recent years , e.g . [ go-torch ] ( https : //github.com/uber/go-torch ) is a highly successful tool and often user 's entry point to visualize pprof data in Go . Consider supporting flame graphs in addition to the dot visualization on the new web UI , so we can provide this representation out-of-the-box with minimal setup and external dependencies . ( If you need this feature as well , you can 👍 so the team knows about your interest . )
Web UI should support flame graphs __EoT__ ! [ image ] ( http : //www.brendangregg.com/FlameGraphs/cpu-mysql-updated.svg ) [ Flame graphs ] ( http : //www.brendangregg.com/flamegraphs.html ) allow you to move in a specific ancestry path from a very compact UI and lets the users to zoom in/out specific paths easily . Flame graphs became one of the defato representation of profiling data in the industry in the recent years , e.g . [ go-torch ] ( https : //github.com/uber/go-torch ) is a highly successful tool and often user 's entry point to visualize pprof data in Go . Consider supporting flame graphs in addition to the dot visualization on the new web UI , so we can provide this representation out-of-the-box with minimal setup and external dependencies . ( If you need this feature as well , you can 👍 so the team knows about your interest . )
Web UI should support flame graphs __EoT__ ! [ image ] ( http : //www.brendangregg.com/FlameGraphs/cpu-mysql-updated.svg ) [ Flame graphs ] ( http : //www.brendangregg.com/flamegraphs.html ) allow you to move in a specific ancestry path from a very compact UI and lets the users to zoom in/out specific paths easily . Flame graphs became one of the defato representation of profiling data in the industry in the recent years , e.g . [ go-torch ] ( https : //github.com/uber/go-torch ) is a highly successful tool and often user 's entry point to visualize pprof data in Go . Consider supporting flame graphs in addition to the dot visualization on the new web UI , so we can provide this representation out-of-the-box with minimal setup and external dependencies . ( If you need this feature as well , you can 👍 so the team knows about your interest . )
Web UI should support flame graphs __EoT__ ! [ image ] ( http : //www.brendangregg.com/FlameGraphs/cpu-mysql-updated.svg ) [ Flame graphs ] ( http : //www.brendangregg.com/flamegraphs.html ) allow you to move in a specific ancestry path from a very compact UI and lets the users to zoom in/out specific paths easily . Flame graphs became one of the defato representation of profiling data in the industry in the recent years , e.g . [ go-torch ] ( https : //github.com/uber/go-torch ) is a highly successful tool and often user 's entry point to visualize pprof data in Go . Consider supporting flame graphs in addition to the dot visualization on the new web UI , so we can provide this representation out-of-the-box with minimal setup and external dependencies . ( If you need this feature as well , you can 👍 so the team knows about your interest . )
Web UI should support flame graphs __EoT__ ! [ image ] ( http : //www.brendangregg.com/FlameGraphs/cpu-mysql-updated.svg ) [ Flame graphs ] ( http : //www.brendangregg.com/flamegraphs.html ) allow you to move in a specific ancestry path from a very compact UI and lets the users to zoom in/out specific paths easily . Flame graphs became one of the defato representation of profiling data in the industry in the recent years , e.g . [ go-torch ] ( https : //github.com/uber/go-torch ) is a highly successful tool and often user 's entry point to visualize pprof data in Go . Consider supporting flame graphs in addition to the dot visualization on the new web UI , so we can provide this representation out-of-the-box with minimal setup and external dependencies . ( If you need this feature as well , you can 👍 so the team knows about your interest . )
tagfocus needs better support for picking which tag to focus on __EoT__ Mentioned offhand in # 153 : A slightly different problem is that there 's no easy way to specify request ranges without having to write out full numbers . I 'd be very happy if I could specify , say : -tagfocus=/request/256kb:512kb to mean `` I want to see a tag , whose name matches `` request '' , whose value is between 256kb and 512kb . If we ca n't manage the unit conversion and I have to specify /request/262144:524288 , I can work with that for now ( though it 'd be very nice ... . ) This is becoming more important as we add more tags to generated profiles , some of which are n't really comparable ; they 're not tweaked values , they 're things with totally different meanings . ( think `` requested alignment '' ; I do not want tagfocus=:512 to pick up every unaligned alloc even if it was a 2 MiB request !
tagfocus needs better support for picking which tag to focus on __EoT__ Mentioned offhand in # 153 : A slightly different problem is that there 's no easy way to specify request ranges without having to write out full numbers . I 'd be very happy if I could specify , say : -tagfocus=/request/256kb:512kb to mean `` I want to see a tag , whose name matches `` request '' , whose value is between 256kb and 512kb . If we ca n't manage the unit conversion and I have to specify /request/262144:524288 , I can work with that for now ( though it 'd be very nice ... . ) This is becoming more important as we add more tags to generated profiles , some of which are n't really comparable ; they 're not tweaked values , they 're things with totally different meanings . ( think `` requested alignment '' ; I do not want tagfocus=:512 to pick up every unaligned alloc even if it was a 2 MiB request !
Fails with shared libraries because mapping is ignored on OSX __EoT__ See [ this issue ] ( https : //github.com/gperftools/gperftools/issues/956 ) for background and test case . Basically on OSX pprof does not subtract the mapped base address of shared libraries from code addresses before feeding them to ` llvm-symbolizer ` . I do n't know exactly how it is supposed to work , but replacing [ this line ] ( https : //github.com/google/pprof/blob/master/internal/symbolizer/symbolizer.go # L164 ) with this seems to make it work : `` ` addr : = l.Address if l.Mapping ! = nil { addr -= l.Mapping.Start } stack , err : = segment.SourceLine ( addr ) `` ` Also , for some very strange reason , with this patch , ` -web ` , ` -top ` and ` -list ' . * ' ` work but ` -weblist ' . * ' ` does not . However ` -weblist 'factorial ' ` *does* work . It seems like the web interface trips up on the first ` ? ? ` output : `` ` $ pprof -list `` . * '' out.prof Local symbolization failed for libdyld.dylib : unrecognized binary : /usr/lib/system/libdyld.dylib Some binary filenames
Drop frames with an anonymous namespace do n't get dropped __EoT__ If a profile has a drop frame matching against an anonymous namespace , the match never happens . I think the culprit is this line ( https : //github.com/google/pprof/blob/master/profile/prune.go # L39 ) , which checks for brackets before applying the drop frame RE , and only applies the RE to the symbol before the brackets . I 'm assuming this is for symbols that still have parameters . Here 's an example [ 1 ] , where I create a profile with a drop frame RE `` Foo : :.* : :Bar '' . If I comment that line out , things behave as expected [ 2 ] . Since this is already a hack , I 'll just special-case `` ( anonymous namespace ) '' , but there are probably other things with a legit `` ( `` as a part of the function name ( std : :function objects ? ) . [ 1 ] $ pprof -traces tmp.pb ... -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
internal/driver : test timed out after 3m0s on freebsd , openbsd , windows __EoT__ Encountered in https : //go-review.googlesource.com/c/go/+/89715 , after trying to update the vendored copy to pull https : //github.com/google/pprof/pull/298 . https : //storage.googleapis.com/go-build-log/b9cbc869/freebsd-amd64-11_1_dbe018c1.log Relevant bit of output : `` ` Generating report in /tmp/profile_proto401663026 Generating report in /tmp/profile_output286659561 Generating report in /tmp/profile_proto375996468 Generating report in /tmp/profile_output864397315 Generating report in /tmp/profile_proto039962502 Generating report in /tmp/profile_output187100461 Generating report in /tmp/profile_proto409183912 Ignoring local file /path/to/buildid : build-id mismatch ( buildid ! = abcdef ) Generating report in /tmp/profile_output785726951 Generating report in /tmp/profile_proto884247066 Ignoring local file /path/to/buildid : build-id mismatch ( buildid ! = abcdef ) Generating report in /tmp/profile_output637371569 Generating report in /tmp/profile_proto247173724 Ignoring local file /path/to/buildid : build-id mismatch ( buildid ! = abcdef ) Generating report in /tmp/profile_output097694731 Generating report in /tmp/profile_proto492795374 Ignoring local file /path/to/buildid : build-id mismatch ( buildid ! = abcdef ) Generating report in /tmp/profile_output034252917 Generating report in /tmp/profile_proto368253776 Generating report in /tmp/profile_output322350703 Generating report in /tmp/profile_proto977961218 Generating report in /tmp/profile_output894972025 Generating report in /tmp/profile_proto420701572 Generating report in /tmp/profile_output650228499 Generating report in /tmp/profile_proto732512086 Hide expression matched no samples Generating report in /tmp/profile_output521628349 Generating report in /tmp/profile_proto277399288 Ignoring local file /path/to/buildid : build-id mismatch (
internal/driver : test timed out after 3m0s on freebsd , openbsd , windows __EoT__ Encountered in https : //go-review.googlesource.com/c/go/+/89715 , after trying to update the vendored copy to pull https : //github.com/google/pprof/pull/298 . https : //storage.googleapis.com/go-build-log/b9cbc869/freebsd-amd64-11_1_dbe018c1.log Relevant bit of output : `` ` Generating report in /tmp/profile_proto401663026 Generating report in /tmp/profile_output286659561 Generating report in /tmp/profile_proto375996468 Generating report in /tmp/profile_output864397315 Generating report in /tmp/profile_proto039962502 Generating report in /tmp/profile_output187100461 Generating report in /tmp/profile_proto409183912 Ignoring local file /path/to/buildid : build-id mismatch ( buildid ! = abcdef ) Generating report in /tmp/profile_output785726951 Generating report in /tmp/profile_proto884247066 Ignoring local file /path/to/buildid : build-id mismatch ( buildid ! = abcdef ) Generating report in /tmp/profile_output637371569 Generating report in /tmp/profile_proto247173724 Ignoring local file /path/to/buildid : build-id mismatch ( buildid ! = abcdef ) Generating report in /tmp/profile_output097694731 Generating report in /tmp/profile_proto492795374 Ignoring local file /path/to/buildid : build-id mismatch ( buildid ! = abcdef ) Generating report in /tmp/profile_output034252917 Generating report in /tmp/profile_proto368253776 Generating report in /tmp/profile_output322350703 Generating report in /tmp/profile_proto977961218 Generating report in /tmp/profile_output894972025 Generating report in /tmp/profile_proto420701572 Generating report in /tmp/profile_output650228499 Generating report in /tmp/profile_proto732512086 Hide expression matched no samples Generating report in /tmp/profile_output521628349 Generating report in /tmp/profile_proto277399288 Ignoring local file /path/to/buildid : build-id mismatch (
internal/driver : test timed out after 3m0s on freebsd , openbsd , windows __EoT__ Encountered in https : //go-review.googlesource.com/c/go/+/89715 , after trying to update the vendored copy to pull https : //github.com/google/pprof/pull/298 . https : //storage.googleapis.com/go-build-log/b9cbc869/freebsd-amd64-11_1_dbe018c1.log Relevant bit of output : `` ` Generating report in /tmp/profile_proto401663026 Generating report in /tmp/profile_output286659561 Generating report in /tmp/profile_proto375996468 Generating report in /tmp/profile_output864397315 Generating report in /tmp/profile_proto039962502 Generating report in /tmp/profile_output187100461 Generating report in /tmp/profile_proto409183912 Ignoring local file /path/to/buildid : build-id mismatch ( buildid ! = abcdef ) Generating report in /tmp/profile_output785726951 Generating report in /tmp/profile_proto884247066 Ignoring local file /path/to/buildid : build-id mismatch ( buildid ! = abcdef ) Generating report in /tmp/profile_output637371569 Generating report in /tmp/profile_proto247173724 Ignoring local file /path/to/buildid : build-id mismatch ( buildid ! = abcdef ) Generating report in /tmp/profile_output097694731 Generating report in /tmp/profile_proto492795374 Ignoring local file /path/to/buildid : build-id mismatch ( buildid ! = abcdef ) Generating report in /tmp/profile_output034252917 Generating report in /tmp/profile_proto368253776 Generating report in /tmp/profile_output322350703 Generating report in /tmp/profile_proto977961218 Generating report in /tmp/profile_output894972025 Generating report in /tmp/profile_proto420701572 Generating report in /tmp/profile_output650228499 Generating report in /tmp/profile_proto732512086 Hide expression matched no samples Generating report in /tmp/profile_output521628349 Generating report in /tmp/profile_proto277399288 Ignoring local file /path/to/buildid : build-id mismatch (
internal/driver : test timed out after 3m0s on freebsd , openbsd , windows __EoT__ Encountered in https : //go-review.googlesource.com/c/go/+/89715 , after trying to update the vendored copy to pull https : //github.com/google/pprof/pull/298 . https : //storage.googleapis.com/go-build-log/b9cbc869/freebsd-amd64-11_1_dbe018c1.log Relevant bit of output : `` ` Generating report in /tmp/profile_proto401663026 Generating report in /tmp/profile_output286659561 Generating report in /tmp/profile_proto375996468 Generating report in /tmp/profile_output864397315 Generating report in /tmp/profile_proto039962502 Generating report in /tmp/profile_output187100461 Generating report in /tmp/profile_proto409183912 Ignoring local file /path/to/buildid : build-id mismatch ( buildid ! = abcdef ) Generating report in /tmp/profile_output785726951 Generating report in /tmp/profile_proto884247066 Ignoring local file /path/to/buildid : build-id mismatch ( buildid ! = abcdef ) Generating report in /tmp/profile_output637371569 Generating report in /tmp/profile_proto247173724 Ignoring local file /path/to/buildid : build-id mismatch ( buildid ! = abcdef ) Generating report in /tmp/profile_output097694731 Generating report in /tmp/profile_proto492795374 Ignoring local file /path/to/buildid : build-id mismatch ( buildid ! = abcdef ) Generating report in /tmp/profile_output034252917 Generating report in /tmp/profile_proto368253776 Generating report in /tmp/profile_output322350703 Generating report in /tmp/profile_proto977961218 Generating report in /tmp/profile_output894972025 Generating report in /tmp/profile_proto420701572 Generating report in /tmp/profile_output650228499 Generating report in /tmp/profile_proto732512086 Hide expression matched no samples Generating report in /tmp/profile_output521628349 Generating report in /tmp/profile_proto277399288 Ignoring local file /path/to/buildid : build-id mismatch (
internal/driver : test timed out after 3m0s on freebsd , openbsd , windows __EoT__ Encountered in https : //go-review.googlesource.com/c/go/+/89715 , after trying to update the vendored copy to pull https : //github.com/google/pprof/pull/298 . https : //storage.googleapis.com/go-build-log/b9cbc869/freebsd-amd64-11_1_dbe018c1.log Relevant bit of output : `` ` Generating report in /tmp/profile_proto401663026 Generating report in /tmp/profile_output286659561 Generating report in /tmp/profile_proto375996468 Generating report in /tmp/profile_output864397315 Generating report in /tmp/profile_proto039962502 Generating report in /tmp/profile_output187100461 Generating report in /tmp/profile_proto409183912 Ignoring local file /path/to/buildid : build-id mismatch ( buildid ! = abcdef ) Generating report in /tmp/profile_output785726951 Generating report in /tmp/profile_proto884247066 Ignoring local file /path/to/buildid : build-id mismatch ( buildid ! = abcdef ) Generating report in /tmp/profile_output637371569 Generating report in /tmp/profile_proto247173724 Ignoring local file /path/to/buildid : build-id mismatch ( buildid ! = abcdef ) Generating report in /tmp/profile_output097694731 Generating report in /tmp/profile_proto492795374 Ignoring local file /path/to/buildid : build-id mismatch ( buildid ! = abcdef ) Generating report in /tmp/profile_output034252917 Generating report in /tmp/profile_proto368253776 Generating report in /tmp/profile_output322350703 Generating report in /tmp/profile_proto977961218 Generating report in /tmp/profile_output894972025 Generating report in /tmp/profile_proto420701572 Generating report in /tmp/profile_output650228499 Generating report in /tmp/profile_proto732512086 Hide expression matched no samples Generating report in /tmp/profile_output521628349 Generating report in /tmp/profile_proto277399288 Ignoring local file /path/to/buildid : build-id mismatch (
internal/driver : test timed out after 3m0s on freebsd , openbsd , windows __EoT__ Encountered in https : //go-review.googlesource.com/c/go/+/89715 , after trying to update the vendored copy to pull https : //github.com/google/pprof/pull/298 . https : //storage.googleapis.com/go-build-log/b9cbc869/freebsd-amd64-11_1_dbe018c1.log Relevant bit of output : `` ` Generating report in /tmp/profile_proto401663026 Generating report in /tmp/profile_output286659561 Generating report in /tmp/profile_proto375996468 Generating report in /tmp/profile_output864397315 Generating report in /tmp/profile_proto039962502 Generating report in /tmp/profile_output187100461 Generating report in /tmp/profile_proto409183912 Ignoring local file /path/to/buildid : build-id mismatch ( buildid ! = abcdef ) Generating report in /tmp/profile_output785726951 Generating report in /tmp/profile_proto884247066 Ignoring local file /path/to/buildid : build-id mismatch ( buildid ! = abcdef ) Generating report in /tmp/profile_output637371569 Generating report in /tmp/profile_proto247173724 Ignoring local file /path/to/buildid : build-id mismatch ( buildid ! = abcdef ) Generating report in /tmp/profile_output097694731 Generating report in /tmp/profile_proto492795374 Ignoring local file /path/to/buildid : build-id mismatch ( buildid ! = abcdef ) Generating report in /tmp/profile_output034252917 Generating report in /tmp/profile_proto368253776 Generating report in /tmp/profile_output322350703 Generating report in /tmp/profile_proto977961218 Generating report in /tmp/profile_output894972025 Generating report in /tmp/profile_proto420701572 Generating report in /tmp/profile_output650228499 Generating report in /tmp/profile_proto732512086 Hide expression matched no samples Generating report in /tmp/profile_output521628349 Generating report in /tmp/profile_proto277399288 Ignoring local file /path/to/buildid : build-id mismatch (
pprof -web opens an svg image on my linux machine __EoT__ # # # What version of pprof are you using ? 3e1e08e68eaed0faefa67fe1c1e10e004cec2c9a ( latest commit at the time of posting this ) If you are using pprof via ` go tool pprof ` , what 's your ` go env ` output ? If you run pprof from GitHub , what 's the Git revision ? # # # What operating system and processor architecture are you using ? Linux ( ubuntu ) # # # What did you do ? I called ` pprof -web /path/to/profile.pb.gz ` # # # What did you expect to see ? I expected the graph to open in chrome . # # # What did you see instead ? An svg image opened . I think this is related to https : //github.com/google/pprof/pull/392
`` For tag bytes used unit bytes , also encountered unit ( s ) '' error message when opening a heap profile __EoT__ Steps to reproduce - see below . Note the `` For tag bytes used unit bytes , also encountered unit ( s ) '' message that should n't be printed . `` ` $ go run pprof.go -top ./profile/testdata/cppbench.heap Local symbolization failed for cppbench_server_main : stat /home/cppbench_server_main : no such file or directory Local symbolization failed for libpthread-2.15.so : stat /lib/libpthread-2.15.so : no such file or directory Local symbolization failed for libc-2.15.so : stat /lib/libc-2.15.so : no such file or directory Some binary filenames not available . Symbolization may be incomplete . Try setting PPROF_BINARY_PATH to the search path for local binaries . For tag bytes used unit bytes , also encountered unit ( s ) File : cppbench_server_main Type : space Showing nodes accounting for 77.19MB , 100 % of 77.19MB total flat flat % sum % cum cum % 77.19MB 100 % 100 % 77.19MB 100 % [ cppbench_server_main ] 0 0 % 100 % 2MB 2.59 % [ libc-2.15.so ] 0 0 % 100 % 73.18MB 94.81 % [ libpthread-2.15.so ] `` `
In the web UI the Refine menu does not enable after a search string is typed __EoT__ Please answer these questions before submitting your issue . Thanks ! # # # What version of pprof are you using ? Master at 9cd381157e8842177d38d881690003a328e9c3ba . # # # What operating system and processor architecture are you using ? Linux # # # What did you do ? I run ` go run pprof.go -http :8080 profile.pb.gz ` . I then enter something like `` foo '' in the search box . # # # What did you expect to see ? I expected the Refine menu to enable . It used to work . # # # What did you see instead ? The refine menu does not enable .
wireshark build broken __EoT__ @ jwzawadzki @ moshekaplan - can you please take a look . https : //oss-fuzz-build-logs.storage.googleapis.com/index.html `` ` Step # 4 : CXXFLAGS=-O1 -fno-omit-frame-pointer -gline-tables-only -DFUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION -fsanitize=address -fsanitize-address-use-after-scope -fsanitize=fuzzer-no-link -stdlib=libc++ Step # 4 : -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Step # 4 : + export WIRESHARK_INSTALL_PATH=/work/install Step # 4 : + WIRESHARK_INSTALL_PATH=/work/install Step # 4 : + mkdir -p /work/install Step # 4 : + export SAMPLES_DIR=/work/samples Step # 4 : + SAMPLES_DIR=/work/samples Step # 4 : + mkdir -p /work/samples Step # 4 : + cp -a /src/wireshark-fuzzdb/samples/ip_proto-ospf /src/wireshark-fuzzdb/samples/media_type-json /src/wireshark-fuzzdb/samples/tcp_port-bgp /src/wireshark-fuzzdb/samples/tcp_port-bzr /src/wireshark-fuzzdb/samples/udp_port-bfd /src/wireshark-fuzzdb/samples/udp_port-bootp /src/wireshark-fuzzdb/samples/udp_port-dns /src/wireshark-fuzzdb/samples/udp_port-sigcomp /src/wireshark-fuzzdb/samples/udp_port-wsp /work/samples Step # 4 : + CONFOPTS= ' -- disable-shared -- enable-static -- disable-plugins' Step # 4 : + CONFOPTS= ' -- disable-shared -- enable-static -- disable-plugins -- without-pcap -- without-ssl -- without-gnutls' Step # 4 : + CONFOPTS= ' -- disable-shared -- enable-static -- disable-plugins -- without-pcap -- without-ssl -- without-gnutls -- disable-wireshark -- disable-tshark -- disable-sharkd -- disable-dumpcap -- disable-capinfos -- disable-captype -- disable-randpkt -- disable-dftest -- disable-editcap -- disable-mergecap --
NSS startup crash __EoT__ Weird that this is only reproducing on ClusterFuzz and not on Docker container locally with fuzzer hash in nss . @ ttaubert - any clues ? hash : ../../fuzz/shared.h:21 : NSSDatabase : :NSSDatabase ( ) : Assertion ` NSS_NoDB_Init ( nullptr ) == SECSuccess failed . ASAN : DEADLYSIGNAL ==1==ERROR : AddressSanitizer : ABRT on unknown address 0x000000000001 ( pc 0x7f8fb75f9418 bp 0x000000c06760 sp 0x7ffcca718ea8 T0 ) SCARINESS : 10 ( signal ) # 0 0x7f8fb75f9417 in gsignal # 1 0x7f8fb75fb019 in abort # 2 0x7f8fb75f1bd6 in libc.so.6 # 3 0x7f8fb75f1c81 in __assert_fail # 4 0x51502a in NSSDatabase : :NSSDatabase ( ) /src/nss/fuzz/shared.h:21:19 # 5 0x51502a in LLVMFuzzerTestOneInput /src/nss/fuzz/hash_target.cc:19 # 6 0x969ee8 in fuzzer : :Fuzzer : :ExecuteCallback ( unsigned char const* , unsigned long ) /src/libfuzzer/FuzzerLoop.cpp:550:13 # 7 0x96906d in fuzzer : :Fuzzer : :ShuffleAndMinimize ( std : :__1 : :vector < std : :__1 : :vector < unsigned char , std : :__1 : :allocator > , std : :__1 : :allocator < std : :__1 : :vector < unsigned char , std : :__1 : :allocator > > > ) /src/libfuzzer/FuzzerLoop.cpp:477:3 # 8 0xa4790a in fuzzer : :FuzzerDriver ( int , char*** ,
afl fuzzer builds out of date compared to libFuzzer builds ? __EoT__ There are two afl detected bugs in bugs.chromium.org for libreoffice , ( https : //bugs.chromium.org/p/oss-fuzz/issues/detail ? id=5542 and https : //bugs.chromium.org/p/oss-fuzz/issues/detail ? id=4946 ) which link to https : //oss-fuzz.com/v2/testcase-detail/5685654865313792 and https : //oss-fuzz.com/v2/testcase-detail/5741892999315456 I know the bugs are fixed , but the latest afl fuzzer build seems stuck at revision 401a3d2336bf46b38235c498ff86ff675d0de759 ( Nov 27 , 2017 ) while the libFuzzer builds have definitely progressed to contemporary HEAD Are they intentionally disabled or is there some afl specific buildfailure ?
FFmpeg 's configure script is not compatible with code coverage __EoT__ While testing # 1547 with ffmpeg , I 've accidentally noticed ASan crashes . Apparently , [ configure script ] ( https : //github.com/FFmpeg/FFmpeg/blob/5423fe29da6bbf28f5253800cace133ac8343a7e/configure # L3968 ) appends ASan and UBSan flags when there is n't any ` -fsanitize= ` `` ` enabled ossfuzz & & ! echo $ CFLAGS | grep -q -- `` -fsanitize= '' & & { add_cflags -fsanitize=address , undefined -fsanitize-coverage=trace-pc-guard , trace-cmp -fno-omit-frame-pointer add_ldflags -fsanitize=address , undefined -fsanitize-coverage=trace-pc-guard , trace-cmp } `` ` Code coverage configurations uses the following flags : https : //github.com/google/oss-fuzz/blob/0fe45f59a73153b083c53cb3113c42c961286cb6/infra/base-images/base-builder/Dockerfile # L27 //cc @ michaelni
FFmpeg 's configure script is not compatible with code coverage __EoT__ While testing # 1547 with ffmpeg , I 've accidentally noticed ASan crashes . Apparently , [ configure script ] ( https : //github.com/FFmpeg/FFmpeg/blob/5423fe29da6bbf28f5253800cace133ac8343a7e/configure # L3968 ) appends ASan and UBSan flags when there is n't any ` -fsanitize= ` `` ` enabled ossfuzz & & ! echo $ CFLAGS | grep -q -- `` -fsanitize= '' & & { add_cflags -fsanitize=address , undefined -fsanitize-coverage=trace-pc-guard , trace-cmp -fno-omit-frame-pointer add_ldflags -fsanitize=address , undefined -fsanitize-coverage=trace-pc-guard , trace-cmp } `` ` Code coverage configurations uses the following flags : https : //github.com/google/oss-fuzz/blob/0fe45f59a73153b083c53cb3113c42c961286cb6/infra/base-images/base-builder/Dockerfile # L27 //cc @ michaelni
Need some way to check project is built with coverage properly ( post build time ) . __EoT__
Need some way to check project is built with coverage properly ( post build time ) . __EoT__
Fuzzer that needs to link against cpython to fuzz __EoT__ For the Mercurial fuzzer we 've got a couple of pieces of code that are complicated C , but for performance reasons are directly implemented as C extensions . I 've done some wiring up locally that seems to work in the local-test docker containers , but I 'm nervous because I had to hardcode the location of ` /out/ ` in at least one place . I think I can work around that , but I 'm curious what I should do . So : where should I put a full install of Python that was built with -- without-pymalloc , and is there anything I should do to make sure the linked-against Python has the right sanitizers enabled ? ( I 'm not interested in bugs in Python itself , really - I 'm mostly worried about a couple of adhoc parsers we 've got that feel like low-hanging ASAN targets . ) Sounds related to # 837 and # 731 based on my reading of those .
h2o : startup crash __EoT__ Has been detected after lading # 838 https : //oss-fuzz-build-logs.storage.googleapis.com/index.html `` ` Step # 5 : INFO : performing bad build checks for /workspace/out/address/h2o-fuzzer-url . Step # 5 : INFO : performing bad build checks for /workspace/out/address/h2o-fuzzer-http1 . Step # 5 : INFO : performing bad build checks for /workspace/out/address/h2o-fuzzer-http2 . Step # 5 : Broken fuzz targets ( 1 ) : Step # 5 : h2o-fuzzer-http2 : Step # 5 : BAD BUILD : the fuzzer seems to have either startup crash or exit . Step # 5 : ERROR : 33 % of fuzz targets seem to be broken . See the list above for a detailed information . `` `
Use standalone main in reproduce __EoT__ https : //reviews.llvm.org/rL285135 adds standalone main Reproduce command should link with that one instead of full libfuzzer .
Add way to run reproduce with valgrind in infra/helper.py __EoT__ This is needed for msan experimental and make validations easier .
openssl build is broken __EoT__ https : //oss-fuzz-build-logs.storage.googleapis.com/index.html `` ` ... Step # 3 : + for f in ' $ fuzzers' Step # 3 : ++ basename fuzz/mkfuzzoids.pl Step # 3 : + fuzzer=mkfuzzoids.pl Step # 3 : + cp fuzz/mkfuzzoids.pl /workspace/out/address/ Step # 3 : + zip -j /workspace/out/address/mkfuzzoids.pl_seed_corpus.zip 'fuzz/corpora/mkfuzzoids.pl/*' Step # 3 : zip warning : name not matched : fuzz/corpora/mkfuzzoids.pl/* Step # 3 : Step # 3 : zip error : Nothing to do ! ( /workspace/out/address/mkfuzzoids.pl_seed_corpus.zip ) Finished Step # 3 ERROR `` `
Files created by images are root-owned __EoT__ python script should specify current user id : ` docker run -- user= < uid > : < gid > `
h2o : fuzz target crashes on startup . __EoT__ https : //oss-fuzz-build-logs.storage.googleapis.com/index.html `` ` Step # 5 : INFO : performing bad build checks for /workspace/out/address/h2o-fuzzer-url . Step # 5 : INFO : performing bad build checks for /workspace/out/address/h2o-fuzzer-http1 . Step # 5 : INFO : performing bad build checks for /workspace/out/address/h2o-fuzzer-http2 . Step # 5 : Broken fuzz targets ( 1 ) : Step # 5 : h2o-fuzzer-http1 : Step # 5 : h2o-fuzzer-http1 has a crash input in its seed coprpus : Step # 5 : Using seed corpus : h2o-fuzzer-http1_seed_corpus.zip Step # 5 : /workspace/out/address/h2o-fuzzer-http1 -rss_limit_mb=2048 -timeout=25 -runs=0 /tmp/h2o-fuzzer-http1_corpus -close_fd_mask=3 -max_len=16384 -dict=http.dict < /dev/null Step # 5 : Dictionary : 126 entries Step # 5 : INFO : Seed : 2301698242 Step # 5 : INFO : Loaded 1 modules ( 9426 inline 8-bit counters ) : 9426 [ 0xc0e350 , 0xc10822 ) , Step # 5 : INFO : Loaded 1 PC tables ( 9426 PCs ) : 9426 [ 0x933450,0x958170 ) , Step # 5 : INFO : 1709 files found in /tmp/h2o-fuzzer-http1_corpus Step # 5 : ALARM : working on the last Unit for 25 seconds Step # 5 : and the timeout value
running out of space in $ OUT ? __EoT__ LibreOffice builds have been failing for a while with ... Step # 12 : cp : error writing '/workspace/out/undefined/qpwfuzzer ' : No space left on device I ( hopefully temporarily ) trimmed down the number of fuzzers we build from to 46 to 18 and my local du for the default asan build is 8G which is below the mentioned 10G in https : //github.com/google/oss-fuzz/blob/master/docs/new_project_guide.md but it still seems to be running out of space . What is my size limit there ? Since https : //github.com/google/oss-fuzz/issues/1188 `` we now only copy a particular fuzz target binary and their needed dependencies '' I 'd hoped that there was no real buildtime $ OUT size limit , just individual fuzz-target limit as each is transported individually around
Tracking of projects that fail to run coverage job __EoT__ - [ x ] **bad_example** Failed to get list of targets from `` https : //storage.googleapis.com/clusterfuzz-builds/bad_example/targets.list.address '' . as the target is not being run on CF - [ x ] **chakra** The same root cause : Failed to get list of targets from `` https : //storage.googleapis.com/clusterfuzz-builds/chakra/targets.list.address '' . - [ x ] **dlplibs** the disk is full , can not unpack the corpus : `` ` I Step # 4 : /corpus/mswksfuzzer/5baeffd6604290d2cbea9a2112181f6c7b91ea0c : write error ( disk full ? ) . Continue ? ( y/n/^C ) I Step # 4 : warning : /corpus/mswksfuzzer/5baeffd6604290d2cbea9a2112181f6c7b91ea0c is probably truncated I Step # 4 : checkdir : can not create extraction directory : /corpus/mswrdfuzzer I Step # 4 : No space left on device I Step # 4 : checkdir : can not create extraction directory : /corpus/multiplanfuzzer I Step # 4 : No space left on device `` ` - [ x ] **example** download_corpus failed , target is disabled on CF - [ x ] **firefox** the disk is full , can not finish compilation `` ` I Step # 2 : + find media/webrtc/trunk/webrtc/test/fuzzers/corpora/stun-corpus -type f -exec zip
Tracking of projects that fail to run coverage job __EoT__ - [ x ] **bad_example** Failed to get list of targets from `` https : //storage.googleapis.com/clusterfuzz-builds/bad_example/targets.list.address '' . as the target is not being run on CF - [ x ] **chakra** The same root cause : Failed to get list of targets from `` https : //storage.googleapis.com/clusterfuzz-builds/chakra/targets.list.address '' . - [ x ] **dlplibs** the disk is full , can not unpack the corpus : `` ` I Step # 4 : /corpus/mswksfuzzer/5baeffd6604290d2cbea9a2112181f6c7b91ea0c : write error ( disk full ? ) . Continue ? ( y/n/^C ) I Step # 4 : warning : /corpus/mswksfuzzer/5baeffd6604290d2cbea9a2112181f6c7b91ea0c is probably truncated I Step # 4 : checkdir : can not create extraction directory : /corpus/mswrdfuzzer I Step # 4 : No space left on device I Step # 4 : checkdir : can not create extraction directory : /corpus/multiplanfuzzer I Step # 4 : No space left on device `` ` - [ x ] **example** download_corpus failed , target is disabled on CF - [ x ] **firefox** the disk is full , can not finish compilation `` ` I Step # 2 : + find media/webrtc/trunk/webrtc/test/fuzzers/corpora/stun-corpus -type f -exec zip
Tracking of projects that fail to run coverage job __EoT__ - [ x ] **bad_example** Failed to get list of targets from `` https : //storage.googleapis.com/clusterfuzz-builds/bad_example/targets.list.address '' . as the target is not being run on CF - [ x ] **chakra** The same root cause : Failed to get list of targets from `` https : //storage.googleapis.com/clusterfuzz-builds/chakra/targets.list.address '' . - [ x ] **dlplibs** the disk is full , can not unpack the corpus : `` ` I Step # 4 : /corpus/mswksfuzzer/5baeffd6604290d2cbea9a2112181f6c7b91ea0c : write error ( disk full ? ) . Continue ? ( y/n/^C ) I Step # 4 : warning : /corpus/mswksfuzzer/5baeffd6604290d2cbea9a2112181f6c7b91ea0c is probably truncated I Step # 4 : checkdir : can not create extraction directory : /corpus/mswrdfuzzer I Step # 4 : No space left on device I Step # 4 : checkdir : can not create extraction directory : /corpus/multiplanfuzzer I Step # 4 : No space left on device `` ` - [ x ] **example** download_corpus failed , target is disabled on CF - [ x ] **firefox** the disk is full , can not finish compilation `` ` I Step # 2 : + find media/webrtc/trunk/webrtc/test/fuzzers/corpora/stun-corpus -type f -exec zip
recognize div-by-zero as such , instead of Crash Type : UNKNOWN __EoT__ https : //bugs.chromium.org/p/oss-fuzz/issues/detail ? id=1374 Crash Type : UNKNOWN Crash Address : But the error report is : ... c:1245:49 : runtime error : division by zero
Do not report coverage build failures in case of a missing corpus for a new target __EoT__ Reported in https : //github.com/google/oss-fuzz/issues/1743 # issuecomment-419050479 I was hoping that we wo n't get 2 consecutive failures with a new fuzz target without corpus backup , but looks like it 's still possible . Potential solutions to the problem : 1 ) parse the log and , if there is corpus download failure , behave differently 2 ) ignore corpus download failures , in that case we 'll still be getting the reports 3 ) increase consecutive build failure threshold to 3 for coverage builds 1 ) is hacky and fragile , 3 ) would make everything even slower I 'll give this another thought and probably would go with 2 ) .
Do not report coverage build failures in case of a missing corpus for a new target __EoT__ Reported in https : //github.com/google/oss-fuzz/issues/1743 # issuecomment-419050479 I was hoping that we wo n't get 2 consecutive failures with a new fuzz target without corpus backup , but looks like it 's still possible . Potential solutions to the problem : 1 ) parse the log and , if there is corpus download failure , behave differently 2 ) ignore corpus download failures , in that case we 'll still be getting the reports 3 ) increase consecutive build failure threshold to 3 for coverage builds 1 ) is hacky and fragile , 3 ) would make everything even slower I 'll give this another thought and probably would go with 2 ) .
Bad Build Checks check libraries __EoT__ I saw this output on a local branch of skia : `` ` Broken fuzz targets ( 2 ) : libEGL.so : BAD BUILD : /out/libEGL.so seems to have either startup crash or exit : libGLESv2.so : BAD BUILD : /out/libGLESv2.so seems to have either startup crash or exit : 22 fuzzers total , 2 seem to be broken ( 9 % ) . Check build passed `` `
Bad Build Checks check libraries __EoT__ I saw this output on a local branch of skia : `` ` Broken fuzz targets ( 2 ) : libEGL.so : BAD BUILD : /out/libEGL.so seems to have either startup crash or exit : libGLESv2.so : BAD BUILD : /out/libGLESv2.so seems to have either startup crash or exit : 22 fuzzers total , 2 seem to be broken ( 9 % ) . Check build passed `` `
openssl does not build with ubsan __EoT__ https : //oss-fuzz-build-logs.storage.googleapis.com/build_logs/openssl/latest.txt @ mikea , @ kroeckx - any idea why ? coverage=edge , indirect-calls,8bit-counters -fno-sanitize=alignment -o fuzz/bndiv-test fuzz/bndiv.o fuzz/test-corpus.o -L. -lcrypto -ldl /usr/local/lib/libc++.a ../../usr/lib/libFuzzingEngine.a ( FuzzerIO.o ) : In function ` basic_ifstream ' : /usr/local/bin/../include/c++/v1/fstream:1067 : undefined reference to ` __ubsan_vptr_type_cache' /usr/local/bin/../include/c++/v1/fstream:1067 : undefined reference to ` __ubsan_handle_dynamic_type_cache_miss_abort' ../../usr/lib/libFuzzingEngine.a ( FuzzerIO.o ) : In function ` basic_istream ' : /usr/local/bin/../include/c++/v1/istream:189 : undefined reference to ` __ubsan_vptr_type_cache' /usr/local/bin/../include/c++/v1/istream:189 : undefined reference to ` __ubsan_handle_dynamic_type_cache_miss_abort' /usr/local/bin/../include/c++/v1/istream:189 : undefined reference to ` __ubsan_vptr_type_cache' /usr/local/bin/../include/c++/v1/istream:189 : undefined reference to ` __ubsan_handle_dynamic_type_cache_miss_abort' ../../usr/lib/libFuzzingEngine.a ( FuzzerIO.o ) : In function ` std : :__1 : :basic_ios < char , std : :__1 : :char_traits < char > > : :init ( std : :__1 : :basic_streambuf < char , std : :__1 : :char_traits < char > > * ) ' : /usr/local/bin/../include/c++/v1/ios:673 : undefined reference to ` __ubsan_vptr_type_cache' /usr/local/bin/../include/c++/v1/ios:673 : undefined reference to ` __ubsan_handle_dynamic_type_cache_miss_abort' /usr/local/bin/../include/c++/v1/ios:674 : undefined reference to ` __ubsan_vptr_type_cache' /usr/local/bin/../include/c++/v1/ios:674 : undefined reference to ` __ubsan_handle_dynamic_type_cache_miss_abort' /usr/local/bin/../include/c++/v1/ios:675 : undefined reference to ` __ubsan_vptr_type_cache' /usr/local/bin/../include/c++/v1/ios:675 : undefined reference to ` __ubsan_handle_dynamic_type_cache_miss_abort' ../../usr/lib/libFuzzingEngine.a ( FuzzerIO.o ) : In function ` basic_ifstream ' : /usr/local/bin/../include/c++/v1/fstream:1069 :
experimentally enable msan for more projects __EoT__ Based on looking at the apt-get dependencies ( build dependencies only ) and linking command , here 's a list of candidates that we can try turning on MSan for : - arduinojson - botan - brotli - c-ares - expat - file - icu - libjpeg-turbo - libplist - libteken - libtsm - libxml2 - libyaml - llvm_libcxxabi - nghttp2 - opus - pcre2 - re2 - sqlite3 - woff2 - zlib
firefox micro-targets __EoT__ I 'd like to know what Googlers ( and Mozillians ) think of this . There are currently two Firefox projects here : ` qcms ` and ` spidermonkey ` . Both build stand-alone and are also useful separate from Firefox . Firefox _core_ does however have many more potential targets . I 'll give a simple example : the FTP LIST parser . [ Please take a look . ] ( https : //dxr.mozilla.org/mozilla-central/source/netwerk/streamconv/converters/ParseFTPList.cpp ) For such targets Firefox has a native fuzzing interface , which is integrated into its normal build system , and produces a binary that can be run with the normal libFuzzer flags . ( ASAN-only currently . ) Quite similar to how ` spidermonkey ` seems to be integrated here right now . Is this still within the scope of ` oss-fuzz ` ?
Fuzz Chakra , SpiderMonkey , JavascriptCore __EoT__ Not with libFuzzer , but with our custom JS fuzzing engine and minimizer .
Coverage : add a way for projects to specify additional arguments __EoT__ The script ( https : //github.com/google/oss-fuzz/blob/master/docs/code_coverage.md # additional-arguments-for-llvm-cov ) already allows that , we just need to add some field to project.yaml that will be automatically taken and passed to the coverage job . Requested in https : //github.com/google/oss-fuzz/issues/1720 # issuecomment-413901806
Coverage : add a way for projects to specify additional arguments __EoT__ The script ( https : //github.com/google/oss-fuzz/blob/master/docs/code_coverage.md # additional-arguments-for-llvm-cov ) already allows that , we just need to add some field to project.yaml that will be automatically taken and passed to the coverage job . Requested in https : //github.com/google/oss-fuzz/issues/1720 # issuecomment-413901806
Coverage : add a way for projects to specify additional arguments __EoT__ The script ( https : //github.com/google/oss-fuzz/blob/master/docs/code_coverage.md # additional-arguments-for-llvm-cov ) already allows that , we just need to add some field to project.yaml that will be automatically taken and passed to the coverage job . Requested in https : //github.com/google/oss-fuzz/issues/1720 # issuecomment-413901806
There does n't seem to be a way to download all the corpora for a project and pass them to the fuzzers __EoT__ According to https : //github.com/google/oss-fuzz/issues/1989 # issuecomment-443368973 > If you want to use the previously-crashed inputs for regression testing , our recommendation is to use the entire corpus generated by fuzzing ( you can get that from the GCS bucket ) , as opposed to select inputs that caused crashes in the past . This is going to be both stronger and simpler solution . It would be great if ` helper.py ` provided a way to do that natively , that is running something like `` ` helper.py regression -- sanitizer= $ sanitizer -- engine= $ engine project `` ` led to downloading all the corpora and passing them to the fuzzers . It could be used in scripts like `` ` sh # ! /bin/bash set -euo pipefail git clone https : //github.com/google/oss-fuzz cd oss-fuzz ./infra/helper.py pull_images ./infra/helper.py build_image -- no-pull systemd # Now it 's not possible to build and check fuzzers in parallel : https : //github.com/google/oss-fuzz/pull/1521 for sanitizer in address undefined memory ; do for engine in libfuzzer afl ; do ./infra/helper.py
Large control file does not seem to get deleted on error during merging __EoT__ We are seeing out of disk space errors in one case , and could be resulting from large control file ( ~4gb ) not cleared after merge fails with global corpus . Kostya , can you check libFuzzer code to confirm that control file is cleared in all cases , even when oom occurs and exits out . If not , can you add the clear call . { insertId : `` r7ea82f3a9kh6 '' jsonPayload : { error : { … } created : `` 2017-03-09T21:24:11.138876Z '' extras : { … } message : `` Corpus pruning failed to merge global corpus : INFO : Seed : 3099641220 INFO : Loaded 1 modules ( 42932 guards ) : [ 0xe82e10 , 0xeacce0 ) , INFO : -max_len is not provided , using 1048576 MERGE-OUTER : 320733 files , 1662 in the initial corpus MERGE-OUTER : attempt 1 INFO : Seed : 3100510032 INFO : Loaded 1 modules ( 42932 guards ) : [ 0xe82e10 , 0xeacce0 ) , INFO : -max_len is not provided , using 1048576 MERGE-INNER : using the control file '/tmp/libFuzzerTemp.1.txt' MERGE-INNER :
Add libaom project / establish fuzzing for AV1 __EoT__ //cc @ jzern This should be pretty straightforward , just follow https : //github.com/google/oss-fuzz/blob/master/docs/new_project_guide.md and / or look at some projects that are already integrated : https : //github.com/google/oss-fuzz/tree/master/projects
openssl fuzzer does n't build anymore __EoT__ The openssl build fails with : `` ` *** No rule to make target '../../usr/lib/libfuzzer.a ' , needed by 'fuzz/asn1 ' . Stop . `` `` Did /usr/lib/libfuzzer.a move ?
Leak detection does n't work __EoT__ Leak detection is disabled because it always gives a false positive . To reproduce : `` ` bash docker run -ti -e ASAN_OPTIONS=detect_leaks=1 ossfuzz/expat run expat_parse_fuzzer -runs=100 `` ` This will generate lots of leaks like : `` ` bash Indirect leak of 1 byte ( s ) in 1 object ( s ) allocated from : # 0 0x50d000 in operator new ( unsigned long ) /src/llvm/projects/compiler-rt/lib/asan/asan_new_delete.cc:82 # 1 0x53fd89 in std : :__1 : :__allocate ( unsigned long ) /usr/local/bin/../include/c++/v1/new:171:10 # 2 0x53fd89 in std : :__1 : :allocator < unsigned char > : :allocate ( unsigned long , void const* ) /usr/local/bin/../include/c++/v1/memory:1771 # 3 0x53fd89 in std : :__1 : :allocator_traits < std : :__1 : :allocator < unsigned char > > : :allocate ( std : :__1 : :allocator < unsigned char > & , unsigned long ) /usr/local/bin/../include/c++/v1/memory:1526 # 4 0x53fd89 in std : :__1 : :vector < unsigned char , std : :__1 : :allocator < unsigned char > > : :allocate ( unsigned long ) /usr/local/bin/../include/c++/v1/vector:923 # 5 0x53f2a4 in _ZNSt3__16vectorIhNS_9allocatorIhEEE6assignIPhEENS_9enable_ifIXaasr21__is_forward_iteratorIT_EE5valuesr16is_constructibleIhNS_15iterator_traitsIS7_E9referenceEEE5valueEvE4typeES7_S7_ /usr/local/bin/../include/c++/v1/vector:1403:9 # 6 0x52d096 in std : :__1 : :vector < unsigned char , std :
wpantund : bad build , partial instrumentation __EoT__ Has been detected after lading # 838 `` ` root @ 2d14f453ace2 : /out # ./ncp-spinel-fuzz INFO : Seed : 4186706769 INFO : Loaded 1 modules ( 3 inline 8-bit counters ) : 3 [ 0x8a16e8 , 0x8a16eb ) , INFO : Loaded 1 PC tables ( 3 PCs ) : 3 [ 0x64e2b0,0x64e2e0 ) , INFO : -max_len is not provided ; libFuzzer will not generate inputs larger than 4096 bytes INFO : A corpus is not provided , starting from an empty corpus # 2 INITED cov : 2 ft : 2 corp : 1/1b lim : 4 exec/s : 0 rss : 34Mb ^C==17== libFuzzer : run interrupted ; exiting `` ` > INFO : Loaded 1 PC tables ( 3 PCs ) : 3 [ 0x64e2b0,0x64e2e0 ) , 3 PCs does n't look right . Compare it with another fuzz target from the same project : `` ` root @ 2d14f453ace2 : /out # ./wpantund-fuzz INFO : Seed : 4189433736 INFO : Loaded 1 modules ( 18217 inline 8-bit counters ) : 18217 [ 0xb10348 , 0xb14a71 ) , INFO : Loaded 1 PC tables ( 18217 PCs
Broken openssl build __EoT__ https : //oss-fuzz-build-logs.storage.googleapis.com/index.html `` ` Step # 4 : + check_startup_crash /workspace/out/address/bndiv Step # 4 : + local FUZZER=/workspace/out/address/bndiv Step # 4 : + local CHECK_PASSED=0 Step # 4 : + [ [ libfuzzer = libfuzzer ] ] Step # 4 : ++ /workspace/out/address/bndiv -runs=4 Step # 4 : ++ egrep 'Done 4 runs ' -c Step # 4 : BAD BUILD : the fuzzer seems to have either startup crash or exit . Step # 4 : + CHECK_PASSED=0 Step # 4 : + ( ( 0 == 0 ) ) Step # 4 : + echo 'BAD BUILD : the fuzzer seems to have either startup crash or exit . ' Step # 4 : + exit 1 `` `
coreutils : need to add fuzz target and seed corpus , dictionary __EoT__ @ pixelb - Tracking bug to add fuzz target in coreutils repo and then reland part of https : //github.com/google/oss-fuzz/pull/118 .
curl build broken __EoT__ Has been detected after lading # 838 https : //oss-fuzz-build-logs.storage.googleapis.com/index.html `` ` Step # 4 : Running : /src/curl_fuzzer/corpora/curl_fuzzer/oss-fuzz-gen-6638d89b27ceef13e985ca5a1d9f64cc9d78f579 Step # 4 : Step # 4 : ================================================================= Step # 4 : ==8599==ERROR : LeakSanitizer : detected memory leaks Step # 4 : Step # 4 : Direct leak of 168 byte ( s ) in 1 object ( s ) allocated from : Step # 4 : # 0 0x4ea4e0 in calloc /src/llvm/projects/compiler-rt/lib/asan/asan_malloc_linux.cc:97 Step # 4 : # 1 0x532680 in curl_docalloc /src/curl/lib/memdebug.c:206:9 Step # 4 : # 2 0x63d284 in Curl_ftp_parselist /src/curl/lib/ftplistparser.c:356:27 Step # 4 : # 3 0x59c267 in chop_write /src/curl/lib/sendf.c:585:22 Step # 4 : # 4 0x5c96a8 in readwrite_data /src/curl/lib/transfer.c:796:26 Step # 4 : # 5 0x5c703c in Curl_readwrite /src/curl/lib/transfer.c:1125:14 Step # 4 : # 6 0x540672 in multi_runsingle /src/curl/lib/multi.c:1893:16 Step # 4 : # 7 0x53e5c3 in curl_multi_perform /src/curl/lib/multi.c:2160:14 Step # 4 : # 8 0x5298be in fuzz_handle_transfer ( fuzz_data* ) /src/curl_fuzzer/curl_fuzzer.cc:382:5 Step # 4 : # 9 0x528acb in LLVMFuzzerTestOneInput /src/curl_fuzzer/curl_fuzzer.cc:93:3 Step # 4 : # 10 0xb0e3d1 in fuzzer : :Fuzzer : :ExecuteCallback ( unsigned char const* , unsigned long ) /src/libfuzzer/FuzzerLoop.cpp:517:13 Step # 4 : # 11 0xae547a
Re-enable optimization of priming UNDEFINED image layers . __EoT__ See : https : //github.com/google/gapid/pull/1935 If an image layer was marked as undefined , then we tried to elide the priming . This was causing some issues with some applications . Remove this for now , and figure out how to re-add the optimization .
Investigate encoder weirdness in armv7 more . __EoT__ It looks like LLVM is optimizing incorrectly , so let us rule out anything else , and try a new verison of LLVM . I put up https : //github.com/google/gapid/pull/1729 so that we can unblock tracing on armv7 while I investigate further .
[ installer.cpp:140 ] Could n't resolve the interceptor library . __EoT__ ! [ qq 20170830210819 ] ( https : //user-images.githubusercontent.com/6738727/29873839-63c6b92e-8dc7-11e7-9197-89d3c40bb423.png ) the main error is： I/GAPID ( 32404 ) : [ installer.cpp:132 ] Installing GAPII hooks ... F/GAPID ( 32404 ) : [ installer.cpp:140 ] Could n't resolve the interceptor library . my device is Redmi 3 related logcat : [ logcat.txt ] ( https : //github.com/google/gapid/files/1263660/logcat.txt )
We do not show framebuffers when the present is done on a compute only queue . __EoT__ Since we use vkCmdBlitImage we ignore the case where we are on a compute only queue . Instead we should copy the image , and do the downsampling operation in software .
Refresh device list for tracing __EoT__ I often connect a device after I 've launched GAPIC . I go to take a trace and find there 's no device in the list , and have to restart GAPIC . We should periodically poll for a new device list .
OSX build broken . __EoT__ Due to # 2014 OSX fails to build : `` ` ERROR : /Volumes/BuildData/tmpfs/src/github/gapid/cmd/gapit/BUILD.bazel:81:1 : GoLink cmd/gapit/darwin_amd64/gapit_unstripped failed ( Exit 1 ) : link failed : error executing command ( cd /Volumes/BuildData/tmpfs/tmp/bazel/829807fe32249b8848a6e84ae1e9b8e6/execroot/gapid & & \ exec env - \ CGO_ENABLED=1 \ GOARCH=amd64 \ GOOS=darwin \ GOROOT='bazel-out/darwin-opt/bin/external/io_bazel_rules_go/darwin_amd64/stdlib~ ' \ GOROOT_FINAL=GOROOT \ PATH=/usr/bin \ bazel-out/host/bin/external/io_bazel_rules_go/go/tools/builders/darwin_amd64_stripped/link [ ... ... ] external/go_sdk/pkg/tool/darwin_amd64/link : external/go_sdk/pkg/tool/darwin_amd64/link : combining dwarf failed : open bazel-out/darwin-opt/bin/cmd/gapit/darwin_amd64/gapit_unstripped~ : is a directory GoLink : error running subcommand : exit status 2 `` ` Probably another rules_go issue .
Group render-passes and subpassess within command-buffers __EoT__ The tree-view would be much easier to navigate if it grouped render-passes , subpasses and draw calls within a submission .
Group render-passes and subpassess within command-buffers __EoT__ The tree-view would be much easier to navigate if it grouped render-passes , subpasses and draw calls within a submission .
Better efficiency for priming image data by rendering __EoT__ Creating pipeline may spend a lot of time for some devices , and the 'priming data pipeline ' actually can be reused for nearly all images . So we should cache the created pipelines , and reuse them for priming data for multiple images . This is specifically for PR : # 1656 Once the depth/stencil logic is fixed , this issue should be resolved .
Better efficiency for priming image data by rendering __EoT__ Creating pipeline may spend a lot of time for some devices , and the 'priming data pipeline ' actually can be reused for nearly all images . So we should cache the created pipelines , and reuse them for priming data for multiple images . This is specifically for PR : # 1656 Once the depth/stencil logic is fixed , this issue should be resolved .
Add support for VK_EXT_DEBUG_MARKER __EoT__ This will let us assign readable names to Vulkan Objects , as well as group commands in command-buffers .
glGenXXX functions can corrupt default objects . __EoT__ As discussed in # 492 , there 's an unusual case where ` glGenXXX ` functions can corrupt the default objects ( 0 ) if the pointer read returns 0 . In # 492 , this happened due to a context cancellation - but this can also occur if the user modifies the pointer or memory . I believe that if # 18 is fixed , then this becomes a non-issue , as the ` glGenXXX ` functions should n't actually touch the underlying object . Given that context cancellation should now stop state mutation , I 'm giving this a low priority .
Texture with format B10G11R11_UFLOAT_PACK32 showing up black in the UI __EoT__ The texture does have data , it ranges from very small ( 0.001 ) to fairly large ( 90+ ) , but the texture view shows uniformly black . The UI shows `` correct '' values on the text during the hover-over . It might be nice for the user to be able to specify a curve for the texture ( This one would need to be exponential in order for the data to be correctly visualized ) . I am not 100 % sure if this is a bug , or just a byproduct of how we display Float textures .
Selecting a vulkan shader in GAPIC causes a crash __EoT__ As above , GAPIS crashes when this happens .
Could n't obtain GVR library handle __EoT__ Built on commit e843f4e7 . VR libraries being missing seems to cause the device under test to not connect properly . `` ` Press enter to stop capturing ... 12:57:51.034 I : [ gapidapk.EnsureInstalled ] < gapit > Examining gapid.apk on host ... 12:57:51.064 I : [ gapidapk.EnsureInstalled ] < gapit > Looking at installed packages ... 12:57:53.472 I : [ gapidapk.EnsureInstalled ] < gapit > Found gapid package ... 12:57:55.314 I : < gapit > Adding new device 12:57:55.314 I : < gapit > Device list : 12:57:55.314 I : < gapit > 172.16.1.1:5555 12:57:55.856 I : < gapit > action : android.intent.action.MAIN : com.johnathongoss.libgdxtests/com.johnathongoss.testing.MainActivity 12:57:55.866 I : < gapit > Device is rooted 12:57:55.964 I : [ start ] < gapit > Turning device screen on 12:57:56.065 I : [ start ] < gapit > Checking for lockscreen 12:57:56.166 I : [ start ] < gapit > Checking gapid.apk is installed 12:57:56.166 I : [ gapidapk.EnsureInstalled⇒start ] < gapit > Examining gapid.apk on host ... 12:57:56.194 I : [ gapidapk.EnsureInstalled⇒start ] < gapit > Looking at installed packages ... 12:57:58.632 I : [ gapidapk.EnsureInstalled⇒start ] < gapit > Found gapid package
ADB devices missing from getDevices ( ) __EoT__ When launching GAPID from Finder , gapis does n't return adb devices . Launching from terminal is OK. To reproduce : 1 . Download .dmg image from kokoro macos release build . 2 . Open the dmg and drag GAPID to Applications . 3 . ` vi /Applications/GAPID.app/Contents/MacOS/gapid ` and add ` -logLevel ALL ` just before the ` $ @ ` on the last line . 4 . Launch GAPID from finder with a phone attached and open the trace dialog . 5 . Observe that no device shows up in the dropdown . 6 . Close the dialog and look at the log tab , only one device ( the local host ) is returned and then queried : look at the RPC- > get ( device ... ) calls . 7 . Close and re-launch the app from the terminal with ` /Applications/GAPID.app/Contents/MacOS/gapid ` . 8 . Again , open the trace dialog . 9 . Observe that the OS asks to allow incoming connections to gapis ( unlike when launching from finder ) - Do n't know if this is a red herring . 10 . Observe in
Re-add observations to gapit commands __EoT__ Context : https : //github.com/google/gapid/pull/352 # discussion_r116492047
Re-add observations to gapit commands __EoT__ Context : https : //github.com/google/gapid/pull/352 # discussion_r116492047
Re-add observations to gapit commands __EoT__ Context : https : //github.com/google/gapid/pull/352 # discussion_r116492047
Re-add observations to gapit commands __EoT__ Context : https : //github.com/google/gapid/pull/352 # discussion_r116492047
Re-add observations to gapit commands __EoT__ Context : https : //github.com/google/gapid/pull/352 # discussion_r116492047
Re-add observations to gapit commands __EoT__ Context : https : //github.com/google/gapid/pull/352 # discussion_r116492047
Re-add observations to gapit commands __EoT__ Context : https : //github.com/google/gapid/pull/352 # discussion_r116492047
Re-add observations to gapit commands __EoT__ Context : https : //github.com/google/gapid/pull/352 # discussion_r116492047
Re-add observations to gapit commands __EoT__ Context : https : //github.com/google/gapid/pull/352 # discussion_r116492047
Re-add observations to gapit commands __EoT__ Context : https : //github.com/google/gapid/pull/352 # discussion_r116492047
Re-add observations to gapit commands __EoT__ Context : https : //github.com/google/gapid/pull/352 # discussion_r116492047
Re-add observations to gapit commands __EoT__ Context : https : //github.com/google/gapid/pull/352 # discussion_r116492047
Re-add observations to gapit commands __EoT__ Context : https : //github.com/google/gapid/pull/352 # discussion_r116492047
Re-add observations to gapit commands __EoT__ Context : https : //github.com/google/gapid/pull/352 # discussion_r116492047
Vulkan : Data mismatch for Depth+Stencil images __EoT__ This is introduced by # 1724 .
Vulkan : Data mismatch for Depth+Stencil images __EoT__ This is introduced by # 1724 .
Map : CONIKS map hasher __EoT__ To provide l bits of security in the map we need the following : - A collision and pre-image resistant hash function H of 2l bits . - A tree specific nonce to prevent multi-tree attacks [ 1 ] [ 2 ] - An index specific value for both leafs and empty branches to prevent attacking multiple locations within the same tree [ 1 ] [ 2 ] In particular , this means that the HStar2 algorithm does not provide the full l bits of security in a multi-tree setting , and we need to adjust the tree hasher interface to support supplying location and tree specific values during the computation of leaves and empty branches . [ 1 ] : https : //eprint.iacr.org/2016/683.pdf Section 5.1 [ 2 ] : https : //eprint.iacr.org/2014/1004.pdf Section 3.1 Proposal for MapHasher : `` ` go type MapHasher interface { HashEmpty ( treeNonce int64 , index [ ] byte , height int ) [ ] byte HashLeaf ( treeNonce int64 , index [ ] byte , depth int , leaf [ ] byte ) [ ] byte HashChildren ( l , r [ ] byte ) [ ]
Map : CONIKS map hasher __EoT__ To provide l bits of security in the map we need the following : - A collision and pre-image resistant hash function H of 2l bits . - A tree specific nonce to prevent multi-tree attacks [ 1 ] [ 2 ] - An index specific value for both leafs and empty branches to prevent attacking multiple locations within the same tree [ 1 ] [ 2 ] In particular , this means that the HStar2 algorithm does not provide the full l bits of security in a multi-tree setting , and we need to adjust the tree hasher interface to support supplying location and tree specific values during the computation of leaves and empty branches . [ 1 ] : https : //eprint.iacr.org/2016/683.pdf Section 5.1 [ 2 ] : https : //eprint.iacr.org/2014/1004.pdf Section 3.1 Proposal for MapHasher : `` ` go type MapHasher interface { HashEmpty ( treeNonce int64 , index [ ] byte , height int ) [ ] byte HashLeaf ( treeNonce int64 , index [ ] byte , depth int , leaf [ ] byte ) [ ] byte HashChildren ( l , r [ ] byte ) [ ]
Map : CONIKS map hasher __EoT__ To provide l bits of security in the map we need the following : - A collision and pre-image resistant hash function H of 2l bits . - A tree specific nonce to prevent multi-tree attacks [ 1 ] [ 2 ] - An index specific value for both leafs and empty branches to prevent attacking multiple locations within the same tree [ 1 ] [ 2 ] In particular , this means that the HStar2 algorithm does not provide the full l bits of security in a multi-tree setting , and we need to adjust the tree hasher interface to support supplying location and tree specific values during the computation of leaves and empty branches . [ 1 ] : https : //eprint.iacr.org/2016/683.pdf Section 5.1 [ 2 ] : https : //eprint.iacr.org/2014/1004.pdf Section 3.1 Proposal for MapHasher : `` ` go type MapHasher interface { HashEmpty ( treeNonce int64 , index [ ] byte , height int ) [ ] byte HashLeaf ( treeNonce int64 , index [ ] byte , depth int , leaf [ ] byte ) [ ] byte HashChildren ( l , r [ ] byte ) [ ]
Map : CONIKS map hasher __EoT__ To provide l bits of security in the map we need the following : - A collision and pre-image resistant hash function H of 2l bits . - A tree specific nonce to prevent multi-tree attacks [ 1 ] [ 2 ] - An index specific value for both leafs and empty branches to prevent attacking multiple locations within the same tree [ 1 ] [ 2 ] In particular , this means that the HStar2 algorithm does not provide the full l bits of security in a multi-tree setting , and we need to adjust the tree hasher interface to support supplying location and tree specific values during the computation of leaves and empty branches . [ 1 ] : https : //eprint.iacr.org/2016/683.pdf Section 5.1 [ 2 ] : https : //eprint.iacr.org/2014/1004.pdf Section 3.1 Proposal for MapHasher : `` ` go type MapHasher interface { HashEmpty ( treeNonce int64 , index [ ] byte , height int ) [ ] byte HashLeaf ( treeNonce int64 , index [ ] byte , depth int , leaf [ ] byte ) [ ] byte HashChildren ( l , r [ ] byte ) [ ]
Map : CONIKS map hasher __EoT__ To provide l bits of security in the map we need the following : - A collision and pre-image resistant hash function H of 2l bits . - A tree specific nonce to prevent multi-tree attacks [ 1 ] [ 2 ] - An index specific value for both leafs and empty branches to prevent attacking multiple locations within the same tree [ 1 ] [ 2 ] In particular , this means that the HStar2 algorithm does not provide the full l bits of security in a multi-tree setting , and we need to adjust the tree hasher interface to support supplying location and tree specific values during the computation of leaves and empty branches . [ 1 ] : https : //eprint.iacr.org/2016/683.pdf Section 5.1 [ 2 ] : https : //eprint.iacr.org/2014/1004.pdf Section 3.1 Proposal for MapHasher : `` ` go type MapHasher interface { HashEmpty ( treeNonce int64 , index [ ] byte , height int ) [ ] byte HashLeaf ( treeNonce int64 , index [ ] byte , depth int , leaf [ ] byte ) [ ] byte HashChildren ( l , r [ ] byte ) [ ]
Map : CONIKS map hasher __EoT__ To provide l bits of security in the map we need the following : - A collision and pre-image resistant hash function H of 2l bits . - A tree specific nonce to prevent multi-tree attacks [ 1 ] [ 2 ] - An index specific value for both leafs and empty branches to prevent attacking multiple locations within the same tree [ 1 ] [ 2 ] In particular , this means that the HStar2 algorithm does not provide the full l bits of security in a multi-tree setting , and we need to adjust the tree hasher interface to support supplying location and tree specific values during the computation of leaves and empty branches . [ 1 ] : https : //eprint.iacr.org/2016/683.pdf Section 5.1 [ 2 ] : https : //eprint.iacr.org/2014/1004.pdf Section 3.1 Proposal for MapHasher : `` ` go type MapHasher interface { HashEmpty ( treeNonce int64 , index [ ] byte , height int ) [ ] byte HashLeaf ( treeNonce int64 , index [ ] byte , depth int , leaf [ ] byte ) [ ] byte HashChildren ( l , r [ ] byte ) [ ]
Map : CONIKS map hasher __EoT__ To provide l bits of security in the map we need the following : - A collision and pre-image resistant hash function H of 2l bits . - A tree specific nonce to prevent multi-tree attacks [ 1 ] [ 2 ] - An index specific value for both leafs and empty branches to prevent attacking multiple locations within the same tree [ 1 ] [ 2 ] In particular , this means that the HStar2 algorithm does not provide the full l bits of security in a multi-tree setting , and we need to adjust the tree hasher interface to support supplying location and tree specific values during the computation of leaves and empty branches . [ 1 ] : https : //eprint.iacr.org/2016/683.pdf Section 5.1 [ 2 ] : https : //eprint.iacr.org/2014/1004.pdf Section 3.1 Proposal for MapHasher : `` ` go type MapHasher interface { HashEmpty ( treeNonce int64 , index [ ] byte , height int ) [ ] byte HashLeaf ( treeNonce int64 , index [ ] byte , depth int , leaf [ ] byte ) [ ] byte HashChildren ( l , r [ ] byte ) [ ]
Map : CONIKS map hasher __EoT__ To provide l bits of security in the map we need the following : - A collision and pre-image resistant hash function H of 2l bits . - A tree specific nonce to prevent multi-tree attacks [ 1 ] [ 2 ] - An index specific value for both leafs and empty branches to prevent attacking multiple locations within the same tree [ 1 ] [ 2 ] In particular , this means that the HStar2 algorithm does not provide the full l bits of security in a multi-tree setting , and we need to adjust the tree hasher interface to support supplying location and tree specific values during the computation of leaves and empty branches . [ 1 ] : https : //eprint.iacr.org/2016/683.pdf Section 5.1 [ 2 ] : https : //eprint.iacr.org/2014/1004.pdf Section 3.1 Proposal for MapHasher : `` ` go type MapHasher interface { HashEmpty ( treeNonce int64 , index [ ] byte , height int ) [ ] byte HashLeaf ( treeNonce int64 , index [ ] byte , depth int , leaf [ ] byte ) [ ] byte HashChildren ( l , r [ ] byte ) [ ]
Map : CONIKS map hasher __EoT__ To provide l bits of security in the map we need the following : - A collision and pre-image resistant hash function H of 2l bits . - A tree specific nonce to prevent multi-tree attacks [ 1 ] [ 2 ] - An index specific value for both leafs and empty branches to prevent attacking multiple locations within the same tree [ 1 ] [ 2 ] In particular , this means that the HStar2 algorithm does not provide the full l bits of security in a multi-tree setting , and we need to adjust the tree hasher interface to support supplying location and tree specific values during the computation of leaves and empty branches . [ 1 ] : https : //eprint.iacr.org/2016/683.pdf Section 5.1 [ 2 ] : https : //eprint.iacr.org/2014/1004.pdf Section 3.1 Proposal for MapHasher : `` ` go type MapHasher interface { HashEmpty ( treeNonce int64 , index [ ] byte , height int ) [ ] byte HashLeaf ( treeNonce int64 , index [ ] byte , depth int , leaf [ ] byte ) [ ] byte HashChildren ( l , r [ ] byte ) [ ]
Map : CONIKS map hasher __EoT__ To provide l bits of security in the map we need the following : - A collision and pre-image resistant hash function H of 2l bits . - A tree specific nonce to prevent multi-tree attacks [ 1 ] [ 2 ] - An index specific value for both leafs and empty branches to prevent attacking multiple locations within the same tree [ 1 ] [ 2 ] In particular , this means that the HStar2 algorithm does not provide the full l bits of security in a multi-tree setting , and we need to adjust the tree hasher interface to support supplying location and tree specific values during the computation of leaves and empty branches . [ 1 ] : https : //eprint.iacr.org/2016/683.pdf Section 5.1 [ 2 ] : https : //eprint.iacr.org/2014/1004.pdf Section 3.1 Proposal for MapHasher : `` ` go type MapHasher interface { HashEmpty ( treeNonce int64 , index [ ] byte , height int ) [ ] byte HashLeaf ( treeNonce int64 , index [ ] byte , depth int , leaf [ ] byte ) [ ] byte HashChildren ( l , r [ ] byte ) [ ]
Map : CONIKS map hasher __EoT__ To provide l bits of security in the map we need the following : - A collision and pre-image resistant hash function H of 2l bits . - A tree specific nonce to prevent multi-tree attacks [ 1 ] [ 2 ] - An index specific value for both leafs and empty branches to prevent attacking multiple locations within the same tree [ 1 ] [ 2 ] In particular , this means that the HStar2 algorithm does not provide the full l bits of security in a multi-tree setting , and we need to adjust the tree hasher interface to support supplying location and tree specific values during the computation of leaves and empty branches . [ 1 ] : https : //eprint.iacr.org/2016/683.pdf Section 5.1 [ 2 ] : https : //eprint.iacr.org/2014/1004.pdf Section 3.1 Proposal for MapHasher : `` ` go type MapHasher interface { HashEmpty ( treeNonce int64 , index [ ] byte , height int ) [ ] byte HashLeaf ( treeNonce int64 , index [ ] byte , depth int , leaf [ ] byte ) [ ] byte HashChildren ( l , r [ ] byte ) [ ]
Map : CONIKS map hasher __EoT__ To provide l bits of security in the map we need the following : - A collision and pre-image resistant hash function H of 2l bits . - A tree specific nonce to prevent multi-tree attacks [ 1 ] [ 2 ] - An index specific value for both leafs and empty branches to prevent attacking multiple locations within the same tree [ 1 ] [ 2 ] In particular , this means that the HStar2 algorithm does not provide the full l bits of security in a multi-tree setting , and we need to adjust the tree hasher interface to support supplying location and tree specific values during the computation of leaves and empty branches . [ 1 ] : https : //eprint.iacr.org/2016/683.pdf Section 5.1 [ 2 ] : https : //eprint.iacr.org/2014/1004.pdf Section 3.1 Proposal for MapHasher : `` ` go type MapHasher interface { HashEmpty ( treeNonce int64 , index [ ] byte , height int ) [ ] byte HashLeaf ( treeNonce int64 , index [ ] byte , depth int , leaf [ ] byte ) [ ] byte HashChildren ( l , r [ ] byte ) [ ]
Map : CONIKS map hasher __EoT__ To provide l bits of security in the map we need the following : - A collision and pre-image resistant hash function H of 2l bits . - A tree specific nonce to prevent multi-tree attacks [ 1 ] [ 2 ] - An index specific value for both leafs and empty branches to prevent attacking multiple locations within the same tree [ 1 ] [ 2 ] In particular , this means that the HStar2 algorithm does not provide the full l bits of security in a multi-tree setting , and we need to adjust the tree hasher interface to support supplying location and tree specific values during the computation of leaves and empty branches . [ 1 ] : https : //eprint.iacr.org/2016/683.pdf Section 5.1 [ 2 ] : https : //eprint.iacr.org/2014/1004.pdf Section 3.1 Proposal for MapHasher : `` ` go type MapHasher interface { HashEmpty ( treeNonce int64 , index [ ] byte , height int ) [ ] byte HashLeaf ( treeNonce int64 , index [ ] byte , depth int , leaf [ ] byte ) [ ] byte HashChildren ( l , r [ ] byte ) [ ]
Map : CONIKS map hasher __EoT__ To provide l bits of security in the map we need the following : - A collision and pre-image resistant hash function H of 2l bits . - A tree specific nonce to prevent multi-tree attacks [ 1 ] [ 2 ] - An index specific value for both leafs and empty branches to prevent attacking multiple locations within the same tree [ 1 ] [ 2 ] In particular , this means that the HStar2 algorithm does not provide the full l bits of security in a multi-tree setting , and we need to adjust the tree hasher interface to support supplying location and tree specific values during the computation of leaves and empty branches . [ 1 ] : https : //eprint.iacr.org/2016/683.pdf Section 5.1 [ 2 ] : https : //eprint.iacr.org/2014/1004.pdf Section 3.1 Proposal for MapHasher : `` ` go type MapHasher interface { HashEmpty ( treeNonce int64 , index [ ] byte , height int ) [ ] byte HashLeaf ( treeNonce int64 , index [ ] byte , depth int , leaf [ ] byte ) [ ] byte HashChildren ( l , r [ ] byte ) [ ]
trillian_map_server : MySQL error 'Could not start tree TX : Error 1040 : Too many connections ' when running examples/ct_mapper __EoT__ The example instructions for setting up a CT mapper do n't seem to work . https : //github.com/google/trillian/tree/master/examples/ct/ctmapper I had to use these commands instead to get it to start : `` ` # Ensure you have your MySQL DB set up correctly , with tables created by the # contents of storage/mysql/storage.sql yes | scripts/resetdb.sh go build ./server/trillian_map_server go build ./examples/ct/ctmapper/mapper go build ./examples/ct/ctmapper/lookup # in one terminal : ./trillian_map_server -- logtostderr # in another ( leaving the trillian_map_server running ) : go build ./cmd/createtree/ tree_id= $ ( ./createtree \ -- admin_server=localhost:8090 \ -- signature_algorithm=ECDSA -tree_type MAP -hash_strategy TEST_MAP_HASHER ) ./mapper \ -source http : //ct.googleapis.com/pilot \ -map_id= $ { tree_id } \ -map_server=localhost:8090 \ -- logtostderr `` ` The problems are : 1 . You need to specify the tree type to ./createtree . 2 . MapHasher ( RFC6962_SHA256 ) is an unknown hasher . Only TEST_MAP_HASHER works . 3 . The HTTP server on port 8091 does not work ; you get ` rpc error : code = Unavailable desc = transport is closing ` .
Logs can not be safely frozen at the moment __EoT__ Setting the frozen state blocks both queuing and signing leaving anything queued unable to be integrated . The proposed solution is : * Expose metrics on integration rate / errors to allow inspection that queue is empty for a tree * Add a new tree state that will allow signing but block queueing * Various new tests and stuff that it works correctly Will need a minor MySQL schema change to the tree states ENUM but this does not need to be done immediately .
Logs can not be safely frozen at the moment __EoT__ Setting the frozen state blocks both queuing and signing leaving anything queued unable to be integrated . The proposed solution is : * Expose metrics on integration rate / errors to allow inspection that queue is empty for a tree * Add a new tree state that will allow signing but block queueing * Various new tests and stuff that it works correctly Will need a minor MySQL schema change to the tree states ENUM but this does not need to be done immediately .
Updates to signer / key management API + implementation __EoT__ + storage of keys in protos .
Better unit tests of inclusion / consistency proofs __EoT__ Existing tests are at a small tree size ( up to 7 leaves ) . This is not enough to be sure about the code . Needs more extensive tests .
Use gRPC status to return errors __EoT__ Consider removing TrillianStatusCode so that clients have a single place to check for errors .
Log integration tests are not running __EoT__ https : //travis-ci.org/google/trillian/jobs/182679607 # L1061 `` ` Running test ( s ) # github.com/google/certificate-transparency/go/merkletree cc1plus : error : unrecognized command line option ‘ -std=c++11 ’ FAIL github.com/google/trillian/integration [ build failed ] `` `
Ensure Trillian Log core allows dupes __EoT__ Some storage layers may guarantee no two leaves can share the same ` LeafIdentityHash ` or ` MerkleLeafHash ` by enforcing this at ` QueueLeaves ` time , but there are other classes of storage implementation we 'd like Trillian to support where this would be hard , if not impossible , to implement . Trillian Log APIs and core code should not assume that dupes will be disallowed by storage , and must cope with cases where duplicate entries exist both within the to-be-sequenced queue as well as the sequenced leaves .
Cloud Spanner support __EoT__ My understanding is that Google runs Trillian internally on top of Spanner . Would it make sense for Trillian to support Cloud Spanner for use on GCP ?
Cloud Spanner support __EoT__ My understanding is that Google runs Trillian internally on top of Spanner . Would it make sense for Trillian to support Cloud Spanner for use on GCP ?
Cloud Spanner support __EoT__ My understanding is that Google runs Trillian internally on top of Spanner . Would it make sense for Trillian to support Cloud Spanner for use on GCP ?
Resend original SCT if a client submits a certificate that already exists in the log __EoT__ Currently , if a client sends the same certificate twice , on the second time , it will fail with ` AddChain handler error : backend QueueLeaves request failed : rpc error : code = AlreadyExists desc = Leaf hash already exists ` due to https : //github.com/google/trillian/blob/master/examples/ct/handlers.go # L308 . To comply with the RFC , any valid submission must yield an SCT .
Resend original SCT if a client submits a certificate that already exists in the log __EoT__ Currently , if a client sends the same certificate twice , on the second time , it will fail with ` AddChain handler error : backend QueueLeaves request failed : rpc error : code = AlreadyExists desc = Leaf hash already exists ` due to https : //github.com/google/trillian/blob/master/examples/ct/handlers.go # L308 . To comply with the RFC , any valid submission must yield an SCT .
Resend original SCT if a client submits a certificate that already exists in the log __EoT__ Currently , if a client sends the same certificate twice , on the second time , it will fail with ` AddChain handler error : backend QueueLeaves request failed : rpc error : code = AlreadyExists desc = Leaf hash already exists ` due to https : //github.com/google/trillian/blob/master/examples/ct/handlers.go # L308 . To comply with the RFC , any valid submission must yield an SCT .
flatc JSON serialisation of sorted tables __EoT__ Hi , I think that the current implementation of Parser : :ParseVector serializes tables with key in the order of the JSON file and do n't sorts the element by their keys . I clearly do n't know enough of the internals to know if I 'm right , but it looks like it just PushElement offset to tables in the order of parsing , and do n't arrange the entries before serializing the offsets . Bye , JB .
flatc JSON serialisation of sorted tables __EoT__ Hi , I think that the current implementation of Parser : :ParseVector serializes tables with key in the order of the JSON file and do n't sorts the element by their keys . I clearly do n't know enough of the internals to know if I 'm right , but it looks like it just PushElement offset to tables in the order of parsing , and do n't arrange the entries before serializing the offsets . Bye , JB .
VS2015 warning `` conditional expression is constant '' __EoT__ Visual Studio 2015 Revision : 640b525e830f84060f0d43923e4fffdb40d5676a Message : optimization of FlatBufferBuilder : :CreateVector ( ) ( # 4198 ) After this commit an annoying warning appears with the code fragment `` if ( sizeof ( T ) == 1 ) { `` flatbuffers\include\flatbuffers/flatbuffers.h ( 1060 ) : warning C4127 : conditional expression is constant
Python bindings __EoT__ I need to be able to create plugins for blender and some conversion tools , thus Python binding is required . I am currently working on it but just want to make sure someone else is not doing it already and I 'm going in the right direction . Ideas : - structs are represented as namedtuple , whole struct is read at once - offsets and vectors of scalar types are wrapped array module - all subregions are mapped using memoryview , deserialization of scalars is done using struct module - there is optional extension module which registers itself as import hook and allows to directly import schema without compiling it
BufferRef is quite broken [ C++ , grpc ] __EoT__ I have been playing around a bit with gRPC and Flatbuffers lately . In doing so , I have found some issues with the ` BufferRef < T > ` type , varying from nitpicky to severe fundamental flaws . 1 . ` BufferRef : :Verify ( ) ` is not a const method . Fixing is easy : Just add the ` const ` keyword . 2 . The docs for ` BufferRef ` claims that it does n't own its buffer , but it still sometimes does ( it calls ` free ` from the destructor sometimes ) . A fix could be to correct the documentation . 3 . ` BufferRef ` sometimes calls ` free ` from its destructor , but it does n't delete or override the copy constructor . This means that if you have an owning ( ` must_free == true ` ) ` BufferRef ` and pass it by value to a function or in some other way copy it , you have a double free memory corruption/crash . This makes that type very difficult to use correctly . A fix for
BufferRef is quite broken [ C++ , grpc ] __EoT__ I have been playing around a bit with gRPC and Flatbuffers lately . In doing so , I have found some issues with the ` BufferRef < T > ` type , varying from nitpicky to severe fundamental flaws . 1 . ` BufferRef : :Verify ( ) ` is not a const method . Fixing is easy : Just add the ` const ` keyword . 2 . The docs for ` BufferRef ` claims that it does n't own its buffer , but it still sometimes does ( it calls ` free ` from the destructor sometimes ) . A fix could be to correct the documentation . 3 . ` BufferRef ` sometimes calls ` free ` from its destructor , but it does n't delete or override the copy constructor . This means that if you have an owning ( ` must_free == true ` ) ` BufferRef ` and pass it by value to a function or in some other way copy it , you have a double free memory corruption/crash . This makes that type very difficult to use correctly . A fix for
BufferRef is quite broken [ C++ , grpc ] __EoT__ I have been playing around a bit with gRPC and Flatbuffers lately . In doing so , I have found some issues with the ` BufferRef < T > ` type , varying from nitpicky to severe fundamental flaws . 1 . ` BufferRef : :Verify ( ) ` is not a const method . Fixing is easy : Just add the ` const ` keyword . 2 . The docs for ` BufferRef ` claims that it does n't own its buffer , but it still sometimes does ( it calls ` free ` from the destructor sometimes ) . A fix could be to correct the documentation . 3 . ` BufferRef ` sometimes calls ` free ` from its destructor , but it does n't delete or override the copy constructor . This means that if you have an owning ( ` must_free == true ` ) ` BufferRef ` and pass it by value to a function or in some other way copy it , you have a double free memory corruption/crash . This makes that type very difficult to use correctly . A fix for
BufferRef is quite broken [ C++ , grpc ] __EoT__ I have been playing around a bit with gRPC and Flatbuffers lately . In doing so , I have found some issues with the ` BufferRef < T > ` type , varying from nitpicky to severe fundamental flaws . 1 . ` BufferRef : :Verify ( ) ` is not a const method . Fixing is easy : Just add the ` const ` keyword . 2 . The docs for ` BufferRef ` claims that it does n't own its buffer , but it still sometimes does ( it calls ` free ` from the destructor sometimes ) . A fix could be to correct the documentation . 3 . ` BufferRef ` sometimes calls ` free ` from its destructor , but it does n't delete or override the copy constructor . This means that if you have an owning ( ` must_free == true ` ) ` BufferRef ` and pass it by value to a function or in some other way copy it , you have a double free memory corruption/crash . This makes that type very difficult to use correctly . A fix for
BufferRef is quite broken [ C++ , grpc ] __EoT__ I have been playing around a bit with gRPC and Flatbuffers lately . In doing so , I have found some issues with the ` BufferRef < T > ` type , varying from nitpicky to severe fundamental flaws . 1 . ` BufferRef : :Verify ( ) ` is not a const method . Fixing is easy : Just add the ` const ` keyword . 2 . The docs for ` BufferRef ` claims that it does n't own its buffer , but it still sometimes does ( it calls ` free ` from the destructor sometimes ) . A fix could be to correct the documentation . 3 . ` BufferRef ` sometimes calls ` free ` from its destructor , but it does n't delete or override the copy constructor . This means that if you have an owning ( ` must_free == true ` ) ` BufferRef ` and pass it by value to a function or in some other way copy it , you have a double free memory corruption/crash . This makes that type very difficult to use correctly . A fix for
Cache UTF8 Charset from Table __EoT__ Please consider caching the UTF8 char codec from the Java ` Table # __string ` method . This access a hash table which causes needless indirection . The codec is already cached from ` FlatBufferBuilder ` as a package private method .
Flexbuffer or bytearray support for python [ python 3.x , flatc 1.7.1 , Ubuntu 16.04 LTS , flatbuffers 2015.5.14 ] __EoT__ - python 3.X - flatc version 1.7.1 ( Sep 27 2017 ) - Ubuntu 16.04.2 LTS - flatbuffers serialization format for Python : INSTALLED : 2015.5.14.0 I would like to store a python bytearray in a flatbuffer vector of type byte _without_ having to iterate over the incoming bytearray invoking ` builder.PrependByte ` , e.g . : for j in reversed ( range ( 0 , len ( buf ) ) ) : builder.PrependByte ( buf [ j ] ) Is this possible via Flexbuffers _or other_ given the current implementation ( s ) ? Otherwise , for serialization performance reasons the above prevents usage within my use case . Many thanks in advance .
Enable verification by default for gRPC __EoT__ ( This issue is a continuation of a discussion in # 4310 ) Flatbuffers does not verify its input by default , not in gRPC and not when not using gRPC . I think Flatbuffers is a great project . It 's a great serialization format : it 's very efficient and it is useful in a wide variety of situations . In the non-RPC use case I think it makes sense to not do verification unless explicitly asked for , because in many situations a program will have just created the buffer and then it would be nonsensical to verify it . However , for gRPC and networking I think the situation is different . @ aardappel says in # 4310 that : * `` While typically you 'd want to use [ verification ] for untrusted network traffic , it should still be a conscious decision to use it , '' * `` and what you 'd want to do if it fails will differ by the use-case . '' * `` People also use GRPC between trusted servers in the same internal network / data center , and may not
flatc : Add option to generate json schema __EoT__ I use JSON to define my flatbuffer data structures and therefore I edit a lot of json . In Visual Studio ( and other IDEs ) there is a possiblity to use json schema for this . http : //json-schema.org/example1.html This is very handy since it avoids typos . The information for this is contained in the idl so flatc should be able to generate a json schema with a new option . Also the documentation would be very easy : http : //jlblcc.github.io/json-schema-viewer/ See also here : https : //spacetelescope.github.io/understanding-json-schema/index.html
binary schemas are different on different platforms / build types __EoT__ As seen here https : //ci.appveyor.com/project/gwvo/flatbuffers/builds/19502965 windows release and debug builds create different in the ` monster_test.bfbs ` binary schema files . There might be an underlying problem that extends to binary serialization in general which should be investigated . Step to reproduce : `` ` # build flatc on Windows in Debug and Release modes first md 1 md 2 Release\flatc.exe '' -o 1 -- binary -- schema -I tests\include_test tests\monster_test.fbs Release\flatc.exe '' -o 2 -- binary -- schema -I tests\include_test tests\monster_test.fbs `` ` There also seem to be differences between macos and Linux as seen here : https : //travis-ci.org/google/flatbuffers/builds/441411876
binary schemas are different on different platforms / build types __EoT__ As seen here https : //ci.appveyor.com/project/gwvo/flatbuffers/builds/19502965 windows release and debug builds create different in the ` monster_test.bfbs ` binary schema files . There might be an underlying problem that extends to binary serialization in general which should be investigated . Step to reproduce : `` ` # build flatc on Windows in Debug and Release modes first md 1 md 2 Release\flatc.exe '' -o 1 -- binary -- schema -I tests\include_test tests\monster_test.fbs Release\flatc.exe '' -o 2 -- binary -- schema -I tests\include_test tests\monster_test.fbs `` ` There also seem to be differences between macos and Linux as seen here : https : //travis-ci.org/google/flatbuffers/builds/441411876
Allocator Improvements __EoT__ The flatbuffers allocator currently has a few limitations/bugs : - The allocator must be copy-constructible , because the deleter in ` vector_downward : :release ( ) ` copies the allocator into the bind . This means we ca n't have a stateful allocator where the allocator manages its own memory regions . I hit this in # 4310 , and while I was debugging I found some other related issues so filing here separately . - The deleter currently does not support subclasses ( ! ) , because it will call ` simple_allocator : :deallocate ` regardless of the type of the allocator . - If we switch the bind to use a pointer to the allocator , then it will use the correct ( derived ) ` deallocate ` function , fixing the above point . However , the bind then refers to an allocator whose lifetime is not managed by the ` unique_ptr_t ` . If the ` unique_ptr_t ` outlives the allocator , it will point to an invalid allocator and could segfault . My proposal is this : - Templatize ` vector_downward < Allocator > ` , making it look more like the
Allocator Improvements __EoT__ The flatbuffers allocator currently has a few limitations/bugs : - The allocator must be copy-constructible , because the deleter in ` vector_downward : :release ( ) ` copies the allocator into the bind . This means we ca n't have a stateful allocator where the allocator manages its own memory regions . I hit this in # 4310 , and while I was debugging I found some other related issues so filing here separately . - The deleter currently does not support subclasses ( ! ) , because it will call ` simple_allocator : :deallocate ` regardless of the type of the allocator . - If we switch the bind to use a pointer to the allocator , then it will use the correct ( derived ) ` deallocate ` function , fixing the above point . However , the bind then refers to an allocator whose lifetime is not managed by the ` unique_ptr_t ` . If the ` unique_ptr_t ` outlives the allocator , it will point to an invalid allocator and could segfault . My proposal is this : - Templatize ` vector_downward < Allocator > ` , making it look more like the
Allocator Improvements __EoT__ The flatbuffers allocator currently has a few limitations/bugs : - The allocator must be copy-constructible , because the deleter in ` vector_downward : :release ( ) ` copies the allocator into the bind . This means we ca n't have a stateful allocator where the allocator manages its own memory regions . I hit this in # 4310 , and while I was debugging I found some other related issues so filing here separately . - The deleter currently does not support subclasses ( ! ) , because it will call ` simple_allocator : :deallocate ` regardless of the type of the allocator . - If we switch the bind to use a pointer to the allocator , then it will use the correct ( derived ) ` deallocate ` function , fixing the above point . However , the bind then refers to an allocator whose lifetime is not managed by the ` unique_ptr_t ` . If the ` unique_ptr_t ` outlives the allocator , it will point to an invalid allocator and could segfault . My proposal is this : - Templatize ` vector_downward < Allocator > ` , making it look more like the
Conan package __EoT__ Hello , Do you know about [ Conan ] ( https : //github.com/conan-io/conan ) ? [ Conan ] ( http : //docs.conan.io/en/latest/ ) is modern dependency manager for C++ . And will be great if your library will be available via package manager for other developers . [ Here ] ( https : //github.com/bincrafters/conan-templates ) you can find example , how you can create package for the library . If you have any questions , just ask : - )
Dart support __EoT__ I 'm wondering if there are plans for supporting Dart too ( especially after the JS serialization is done , I imagine it would n't be that much different to do the codegen ) .
DetachedBuffer can not be rewrapped [ C++ , clang 6.0 , OSX 10.11.6 , master ] __EoT__ *affected language* : cpp *compiler version* : clang 6.0 *operating system version* : macOs 10.11.6 *flatBuffers version* : current ` DetachedBuffer ` automatically manages the memory , which is good . However , it is kind of hard ( rather impossible ) to permanently release the memory from the ` DetachedBuffer ` . I would like a functionality to get the pointers ` _cur ` and the size ` _reserved ` out from the detached buffer such that I can **rewrap** this memory somewhere else ( without having to use type from the library ) . I have given FlatBufferBuilder a proxy object which forwards to my special allocator . Something along the line : `` ` cpp AllocatorProxyFlatBuffer allocator ( mySpecialAllocator ) ; flatbuffers : :FlatBufferBuilder builder ( 1024 , & allocator ) ; ... . auto detachedBuffer = builder.Release ( ) ; uint8_t data = detachedBuffer.data ( ) // pointer to the start of the data . std : size_t size = detachedBuffer.size ( ) // size of the data . std : :pair < uint8_t* , std : :size_t >
DetachedBuffer can not be rewrapped [ C++ , clang 6.0 , OSX 10.11.6 , master ] __EoT__ *affected language* : cpp *compiler version* : clang 6.0 *operating system version* : macOs 10.11.6 *flatBuffers version* : current ` DetachedBuffer ` automatically manages the memory , which is good . However , it is kind of hard ( rather impossible ) to permanently release the memory from the ` DetachedBuffer ` . I would like a functionality to get the pointers ` _cur ` and the size ` _reserved ` out from the detached buffer such that I can **rewrap** this memory somewhere else ( without having to use type from the library ) . I have given FlatBufferBuilder a proxy object which forwards to my special allocator . Something along the line : `` ` cpp AllocatorProxyFlatBuffer allocator ( mySpecialAllocator ) ; flatbuffers : :FlatBufferBuilder builder ( 1024 , & allocator ) ; ... . auto detachedBuffer = builder.Release ( ) ; uint8_t data = detachedBuffer.data ( ) // pointer to the start of the data . std : size_t size = detachedBuffer.size ( ) // size of the data . std : :pair < uint8_t* , std : :size_t >
-- ts generate invalid code if namespace is n't declared __EoT__ Given JS / TS modules provide name-spacing using namespaces is kind of redundant . But then if you generate typescript code for file without namespace generate looks like : `` ` ts export namespace { export class ChangeProperty { // ... } } `` ` which is invalid typescript code given that namespace name is omitted .
-- ts generate invalid code if namespace is n't declared __EoT__ Given JS / TS modules provide name-spacing using namespaces is kind of redundant . But then if you generate typescript code for file without namespace generate looks like : `` ` ts export namespace { export class ChangeProperty { // ... } } `` ` which is invalid typescript code given that namespace name is omitted .
Support Pool Allocation __EoT__ @ gwvo The byte array allocated in `` ` protected String __string ( int offset ) { offset += bb.getInt ( offset ) ; if ( bb.hasArray ( ) ) { return new String ( bb.array ( ) , offset + SIZEOF_INT , bb.getInt ( offset ) , FlatBufferBuilder.utf8charset ) ; } else { // We ca n't access .array ( ) , since the ByteBuffer is read-only , // off-heap or a memory map ByteBuffer bb = this.bb.duplicate ( ) .order ( ByteOrder.LITTLE_ENDIAN ) ; // We 're forced to make an extra copy : byte [ ] copy = new byte [ bb.getInt ( offset ) ] ; bb.position ( offset + SIZEOF_INT ) ; bb.get ( copy ) ; return new String ( copy , 0 , copy.length , FlatBufferBuilder.utf8charset ) ; } } `` ` is really killing me . if i were to make a patch that would add the following modifications - An interfaced called ` Allocator ` - Add a default ` Allocator ` that just implements the current behavior - The requirement would only be that for the function to return with a minimum number of bytes requested
Java `` enums '' should be final classes with private default constructors __EoT__ currently they are non-final , and have no source for a default constructor , so anyone can instantiate them .
Generated Rust is invalid for nested tables [ Rust , OSX , Flatbuffers 1.9.0 ] __EoT__ Minimal example : `` ` table Foo { bar : float ; } table Baz { bux : Foo ; } `` ` This generates an undeclared lifetime specifier ` 'a ` here : `` ` pub struct BazArgs { pub bux : Option < flatbuffers : :WIPOffset < Foo < 'a > > > , } `` `
ORM like encoding/decoding for C++ ? __EoT__ Because currently is not easy to change a message ( e.g . set a longer string , change a vector size , etc . ) , when it has lots of ( nested ) tables , I 'd like to propose a new optional way ( probably slower ) of encoding/decoding flatbuffers messages in C++ . Flatc will generate ( separate ) classes which will have set/get methods for every field , this way it will be very easy to change any fields . They will also have two methods : - ` bool encode ( void **ptr , size_t *sz ) ` which encodes the message ( it will go through all nested tables ) , sets **_ptr**_ and **_sz**_ values and returns **_true**_ on success . - ` size_t decoding ( void *ptr ) ; ` decodes the message ( fills all the classes/nested classes fields ) and returns the size of the message in bytes ( **_0**_ of course means it needs more data ) . I know it will be a little bit slower , but the code will be much more readable and maintainable .
[ C++ ] Enum Safety of generated code __EoT__ Given an enum : `` ` enum compression_flags { none , disabled , zstd , lz4 } `` ` The flatc generator outputs this : `` ` enum compression_flags { compression_flags_none = 0 , compression_flags_disabled = 1 , compression_flags_zstd = 2 , compression_flags_lz4 = 3 , compression_flags_MIN = compression_flags_none , compression_flags_MAX = compression_flags_lz4 } ; inline const compression_flags ( & EnumValuescompression_flags ( ) ) [ 4 ] { static const compression_flags values [ ] = { compression_flags_none , compression_flags_disabled , compression_flags_zstd , compression_flags_lz4 } ; return values ; } inline const char * const *EnumNamescompression_flags ( ) { static const char * const names [ ] = { `` none '' , `` disabled '' , `` zstd '' , `` lz4 '' , nullptr } ; return names ; } inline const char *EnumNamecompression_flags ( compression_flags e ) { const size_t index = static_cast < int > ( e ) ; return EnumNamescompression_flags ( ) [ index ] ; } `` ` However , if you have some debugging code - say you are printing the C++ Object API via the `` ` flatbuffers : :TypeTable * `` ` then
Improve Javadoc of FlatBufferBuilder __EoT__ PR incoming
Improve Javadoc of FlatBufferBuilder __EoT__ PR incoming
[ help needed ] JSON 2D arrays and JSON lists schema __EoT__ Hi , I wonder if it 's possible to represent the below JSON as a scheme in Flatbuffers : ( It 's output from InfluxDB ; I was using boost : :property_tree ( spirit ) to parse it , but it was too slow at about 15k rows/core/sec ) - The `` result '' array can contain 0+ `` results '' - A `` result '' can contain 0+ `` series '' - A `` series '' can contain a `` name '' string and a `` columns '' string-array So far I was able to create a scheme . But how to handle : - `` tags '' list with variable key-strings ? ( host/type/type_instance , ... ) - `` values '' array of arrays of unions ( strings/floats/integers/bools ) `` ` { `` results '' : [ { `` series '' : [ { `` name '' : `` chrony_value '' , `` tags '' : { `` host '' : `` apar.example.com '' , `` type '' : `` clock_skew_ppm '' , `` type_instance '' : `` chrony '' } , `` columns '' :
Why flatbuffers instead of capnp ? __EoT__ You mention memory efficiency as the main selling point of flatbuffers . However , [ capnp ] ( http : //kentonv.github.io/capnproto/ ) already exists , with zero copy `` serialization , '' and a range of compelling features ( RPC , etc ) . Still you saw the need to build flatbuffers , which means this library either ( a ) does something capnp does n't , or ( b ) beats capnp in some respect . Could you please describe somewhere what this rationale is ? Why should someone pick flatbuffers instead of capnp ?
Java : public default constructor is error-prone __EoT__ Hi ! I am currently using flatbuffers to both hold default values and allow to sync user themes ( for an Android Wear Watch Face ) between the handset and the wearables . I always need a ` Theme ` object ( which is a flatbuffers ` Table ` ) available to get the default values if the user has n't set any theme . How to create an empty object is not clear . From the code ( and I tried to confirm ) , creating a new object initially ( ` new Theme ( ) ` ) is possible , and creates one with a ` null `` ByteBuffer ` , leading to a ` NullPointerException ` as soon as you call an accessor . `` ` java private final Theme mTheme = new Theme ( ) ; // Should work but leads to NPEs on accesses . `` ` I tried calling this : `` ` java private final Theme mTheme = Theme.getRootAsTheme ( ByteBuffer.allocate ( 0 ) ) ; // Does n't work `` ` But got an ` IndexOutOfBoundsException ` when accessing a property . Finally ,
Java : public default constructor is error-prone __EoT__ Hi ! I am currently using flatbuffers to both hold default values and allow to sync user themes ( for an Android Wear Watch Face ) between the handset and the wearables . I always need a ` Theme ` object ( which is a flatbuffers ` Table ` ) available to get the default values if the user has n't set any theme . How to create an empty object is not clear . From the code ( and I tried to confirm ) , creating a new object initially ( ` new Theme ( ) ` ) is possible , and creates one with a ` null `` ByteBuffer ` , leading to a ` NullPointerException ` as soon as you call an accessor . `` ` java private final Theme mTheme = new Theme ( ) ; // Should work but leads to NPEs on accesses . `` ` I tried calling this : `` ` java private final Theme mTheme = Theme.getRootAsTheme ( ByteBuffer.allocate ( 0 ) ) ; // Does n't work `` ` But got an ` IndexOutOfBoundsException ` when accessing a property . Finally ,
flatc fails to verify required fields are present . __EoT__ I am trying to use FlatBuffers , and we found out that flatc does not verify if the required fields are present . this is a stackoverflow question I asked about this : http : //stackoverflow.com/questions/43597831 please let me know if you needed any more info .
flatc uses wrong type for reflection of ulong enum __EoT__ Given a schema `` ` fbs namespace foo ; /// Invalid value used by IndexItem enum Index : ulong { Invalid = 0xfffffffffffffff } table IndexItem { id : ulong ( key ) ; /// optional , use Index : :Invalid to indicate no value index : ulong ; somethingelsewecareabout : string ; } table Association { index : [ IndexItem ] ; items : [ string ] ; } `` ` and running ` flatc ` with arguments ` -- cpp -- gen-object-api -- reflect-types ` generates the following reflection ` TypeTable ` : `` ` cpp inline const flatbuffers : :TypeTable *IndexTypeTable ( ) { static const flatbuffers : :TypeCode type_codes [ ] = { { flatbuffers : :ET_ULONG , 0 , 0 } } ; static const flatbuffers : :TypeFunction type_refs [ ] = { IndexTypeTable } ; // int32_t instead of uint64_t array static const int32_t values [ ] = { 1152921504606846975 } ; static const flatbuffers : :TypeTable tt = { flatbuffers : :ST_ENUM , 1 , type_codes , type_refs , values , nullptr } ; return & tt ; } `` ` ` values
Returned types not consistent [ Python , master ] __EoT__ When using Flatbuffers with Python , default return types are always 0 , even if the field type is ` bool ` . ( due to the fact that ` field.value.constant ` is 0 by default and it is this value which is returned [ here ] ( https : //github.com/google/flatbuffers/blob/master/src/idl_gen_python.cpp # L146 ) ) In addition , get a field `` XAsNumpy '' also returns 0 ( or empty string ) if the field is not set : it should be [ ] ( empty list ) to be logical , no ? ( [ here ] ( https : //github.com/google/flatbuffers/blob/master/src/idl_gen_python.cpp # L294 ) ) Example : - ` test.fbs ` `` ` namespace MyGame.Sample ; table Monster { alive : bool ; inventory : [ int ] } `` ` - ` test.py ` `` ` [ ... ] monster = MyGame.Sample.Monster.Monster.GetRootAsMonster ( buf , 0 ) print ( monster.Alive ( ) ) # Will print `` True '' if set to `` True '' , 0 if set to false ( which is the default value ) or if not set . print ( monster.InventoryAsNumpy ( )
Include service data in reflection schema __EoT__ My use-case is that I have a custom RPC layer and I would like to use flatbuffers as the serialization format for wire transfer . I may be missing something , but I do n't see an easy way to write my own `` plugin generator '' to generate custom code based on a flatbuffer schema . Similarly to how I could write a plugin for ` protoc ` to generate my custom RPC code . To work around the fact that there does n't seem to be an analogous plugin API for ` flatc ` I was going to try generating the reflection schema data and then writing an application to parse that reflection data to output generated RPC code . Unfortunately , rpc_service reflection data does n't seem to make it into the reflection schema binary . Is there a recommended way of doing what I am trying to do ? Thanks !
Go : Enums should have type __EoT__ Enums in generated Go code should have it 's own type . Consider the following example : `` ` enum Color : byte { Red = 1 , Green , Blue } `` ` Currently , this generates : `` ` go const ( ColorRed = 1 ColorGreen = 2 ColorBlue = 3 ) `` ` It would be better if the following was generated : `` ` go type Color byte const ( Red Color = 1 Green = 2 Blue = 3 ) `` ` This way , the generated enums can be used with the type : `` ` go type MyColorContainer struct { color Color } `` `
compare_bench fails if Complexity was computed __EoT__ When the benchmarks are are run with ` Complexity ( ) ` enabled , the output JSON file contains additional members that cause the ` compare_bench.py ` script to fail : > Invalid input file : 'out.json ' does not name a valid benchmark executable or JSON file The offending lines in the JSON look like : { `` name '' : `` ComputationBenchmarks_BigO '' , `` cpu_coefficient '' : 672 , `` real_coefficient '' : 672 , `` big_o '' : `` N '' , `` time_unit '' : `` ns '' } , { `` name '' : `` ComputationBenchmarks_RMS '' , `` rms '' : 2 % } Removing them manually works fine , so presumably the script should just ignore complexity-related entries in the JSON .
compare_bench fails if Complexity was computed __EoT__ When the benchmarks are are run with ` Complexity ( ) ` enabled , the output JSON file contains additional members that cause the ` compare_bench.py ` script to fail : > Invalid input file : 'out.json ' does not name a valid benchmark executable or JSON file The offending lines in the JSON look like : { `` name '' : `` ComputationBenchmarks_BigO '' , `` cpu_coefficient '' : 672 , `` real_coefficient '' : 672 , `` big_o '' : `` N '' , `` time_unit '' : `` ns '' } , { `` name '' : `` ComputationBenchmarks_RMS '' , `` rms '' : 2 % } Removing them manually works fine , so presumably the script should just ignore complexity-related entries in the JSON .
compare_bench fails if Complexity was computed __EoT__ When the benchmarks are are run with ` Complexity ( ) ` enabled , the output JSON file contains additional members that cause the ` compare_bench.py ` script to fail : > Invalid input file : 'out.json ' does not name a valid benchmark executable or JSON file The offending lines in the JSON look like : { `` name '' : `` ComputationBenchmarks_BigO '' , `` cpu_coefficient '' : 672 , `` real_coefficient '' : 672 , `` big_o '' : `` N '' , `` time_unit '' : `` ns '' } , { `` name '' : `` ComputationBenchmarks_RMS '' , `` rms '' : 2 % } Removing them manually works fine , so presumably the script should just ignore complexity-related entries in the JSON .
compare_bench fails if Complexity was computed __EoT__ When the benchmarks are are run with ` Complexity ( ) ` enabled , the output JSON file contains additional members that cause the ` compare_bench.py ` script to fail : > Invalid input file : 'out.json ' does not name a valid benchmark executable or JSON file The offending lines in the JSON look like : { `` name '' : `` ComputationBenchmarks_BigO '' , `` cpu_coefficient '' : 672 , `` real_coefficient '' : 672 , `` big_o '' : `` N '' , `` time_unit '' : `` ns '' } , { `` name '' : `` ComputationBenchmarks_RMS '' , `` rms '' : 2 % } Removing them manually works fine , so presumably the script should just ignore complexity-related entries in the JSON .
compare_bench fails if Complexity was computed __EoT__ When the benchmarks are are run with ` Complexity ( ) ` enabled , the output JSON file contains additional members that cause the ` compare_bench.py ` script to fail : > Invalid input file : 'out.json ' does not name a valid benchmark executable or JSON file The offending lines in the JSON look like : { `` name '' : `` ComputationBenchmarks_BigO '' , `` cpu_coefficient '' : 672 , `` real_coefficient '' : 672 , `` big_o '' : `` N '' , `` time_unit '' : `` ns '' } , { `` name '' : `` ComputationBenchmarks_RMS '' , `` rms '' : 2 % } Removing them manually works fine , so presumably the script should just ignore complexity-related entries in the JSON .
compare_bench fails if Complexity was computed __EoT__ When the benchmarks are are run with ` Complexity ( ) ` enabled , the output JSON file contains additional members that cause the ` compare_bench.py ` script to fail : > Invalid input file : 'out.json ' does not name a valid benchmark executable or JSON file The offending lines in the JSON look like : { `` name '' : `` ComputationBenchmarks_BigO '' , `` cpu_coefficient '' : 672 , `` real_coefficient '' : 672 , `` big_o '' : `` N '' , `` time_unit '' : `` ns '' } , { `` name '' : `` ComputationBenchmarks_RMS '' , `` rms '' : 2 % } Removing them manually works fine , so presumably the script should just ignore complexity-related entries in the JSON .
compare_bench fails if Complexity was computed __EoT__ When the benchmarks are are run with ` Complexity ( ) ` enabled , the output JSON file contains additional members that cause the ` compare_bench.py ` script to fail : > Invalid input file : 'out.json ' does not name a valid benchmark executable or JSON file The offending lines in the JSON look like : { `` name '' : `` ComputationBenchmarks_BigO '' , `` cpu_coefficient '' : 672 , `` real_coefficient '' : 672 , `` big_o '' : `` N '' , `` time_unit '' : `` ns '' } , { `` name '' : `` ComputationBenchmarks_RMS '' , `` rms '' : 2 % } Removing them manually works fine , so presumably the script should just ignore complexity-related entries in the JSON .
compare_bench fails if Complexity was computed __EoT__ When the benchmarks are are run with ` Complexity ( ) ` enabled , the output JSON file contains additional members that cause the ` compare_bench.py ` script to fail : > Invalid input file : 'out.json ' does not name a valid benchmark executable or JSON file The offending lines in the JSON look like : { `` name '' : `` ComputationBenchmarks_BigO '' , `` cpu_coefficient '' : 672 , `` real_coefficient '' : 672 , `` big_o '' : `` N '' , `` time_unit '' : `` ns '' } , { `` name '' : `` ComputationBenchmarks_RMS '' , `` rms '' : 2 % } Removing them manually works fine , so presumably the script should just ignore complexity-related entries in the JSON .
compare_bench fails if Complexity was computed __EoT__ When the benchmarks are are run with ` Complexity ( ) ` enabled , the output JSON file contains additional members that cause the ` compare_bench.py ` script to fail : > Invalid input file : 'out.json ' does not name a valid benchmark executable or JSON file The offending lines in the JSON look like : { `` name '' : `` ComputationBenchmarks_BigO '' , `` cpu_coefficient '' : 672 , `` real_coefficient '' : 672 , `` big_o '' : `` N '' , `` time_unit '' : `` ns '' } , { `` name '' : `` ComputationBenchmarks_RMS '' , `` rms '' : 2 % } Removing them manually works fine , so presumably the script should just ignore complexity-related entries in the JSON .
compare_bench fails if Complexity was computed __EoT__ When the benchmarks are are run with ` Complexity ( ) ` enabled , the output JSON file contains additional members that cause the ` compare_bench.py ` script to fail : > Invalid input file : 'out.json ' does not name a valid benchmark executable or JSON file The offending lines in the JSON look like : { `` name '' : `` ComputationBenchmarks_BigO '' , `` cpu_coefficient '' : 672 , `` real_coefficient '' : 672 , `` big_o '' : `` N '' , `` time_unit '' : `` ns '' } , { `` name '' : `` ComputationBenchmarks_RMS '' , `` rms '' : 2 % } Removing them manually works fine , so presumably the script should just ignore complexity-related entries in the JSON .
compare_bench fails if Complexity was computed __EoT__ When the benchmarks are are run with ` Complexity ( ) ` enabled , the output JSON file contains additional members that cause the ` compare_bench.py ` script to fail : > Invalid input file : 'out.json ' does not name a valid benchmark executable or JSON file The offending lines in the JSON look like : { `` name '' : `` ComputationBenchmarks_BigO '' , `` cpu_coefficient '' : 672 , `` real_coefficient '' : 672 , `` big_o '' : `` N '' , `` time_unit '' : `` ns '' } , { `` name '' : `` ComputationBenchmarks_RMS '' , `` rms '' : 2 % } Removing them manually works fine , so presumably the script should just ignore complexity-related entries in the JSON .
compare_bench fails if Complexity was computed __EoT__ When the benchmarks are are run with ` Complexity ( ) ` enabled , the output JSON file contains additional members that cause the ` compare_bench.py ` script to fail : > Invalid input file : 'out.json ' does not name a valid benchmark executable or JSON file The offending lines in the JSON look like : { `` name '' : `` ComputationBenchmarks_BigO '' , `` cpu_coefficient '' : 672 , `` real_coefficient '' : 672 , `` big_o '' : `` N '' , `` time_unit '' : `` ns '' } , { `` name '' : `` ComputationBenchmarks_RMS '' , `` rms '' : 2 % } Removing them manually works fine , so presumably the script should just ignore complexity-related entries in the JSON .
compare_bench fails if Complexity was computed __EoT__ When the benchmarks are are run with ` Complexity ( ) ` enabled , the output JSON file contains additional members that cause the ` compare_bench.py ` script to fail : > Invalid input file : 'out.json ' does not name a valid benchmark executable or JSON file The offending lines in the JSON look like : { `` name '' : `` ComputationBenchmarks_BigO '' , `` cpu_coefficient '' : 672 , `` real_coefficient '' : 672 , `` big_o '' : `` N '' , `` time_unit '' : `` ns '' } , { `` name '' : `` ComputationBenchmarks_RMS '' , `` rms '' : 2 % } Removing them manually works fine , so presumably the script should just ignore complexity-related entries in the JSON .
compare_bench fails if Complexity was computed __EoT__ When the benchmarks are are run with ` Complexity ( ) ` enabled , the output JSON file contains additional members that cause the ` compare_bench.py ` script to fail : > Invalid input file : 'out.json ' does not name a valid benchmark executable or JSON file The offending lines in the JSON look like : { `` name '' : `` ComputationBenchmarks_BigO '' , `` cpu_coefficient '' : 672 , `` real_coefficient '' : 672 , `` big_o '' : `` N '' , `` time_unit '' : `` ns '' } , { `` name '' : `` ComputationBenchmarks_RMS '' , `` rms '' : 2 % } Removing them manually works fine , so presumably the script should just ignore complexity-related entries in the JSON .
compare_bench fails if Complexity was computed __EoT__ When the benchmarks are are run with ` Complexity ( ) ` enabled , the output JSON file contains additional members that cause the ` compare_bench.py ` script to fail : > Invalid input file : 'out.json ' does not name a valid benchmark executable or JSON file The offending lines in the JSON look like : { `` name '' : `` ComputationBenchmarks_BigO '' , `` cpu_coefficient '' : 672 , `` real_coefficient '' : 672 , `` big_o '' : `` N '' , `` time_unit '' : `` ns '' } , { `` name '' : `` ComputationBenchmarks_RMS '' , `` rms '' : 2 % } Removing them manually works fine , so presumably the script should just ignore complexity-related entries in the JSON .
compare_bench fails if Complexity was computed __EoT__ When the benchmarks are are run with ` Complexity ( ) ` enabled , the output JSON file contains additional members that cause the ` compare_bench.py ` script to fail : > Invalid input file : 'out.json ' does not name a valid benchmark executable or JSON file The offending lines in the JSON look like : { `` name '' : `` ComputationBenchmarks_BigO '' , `` cpu_coefficient '' : 672 , `` real_coefficient '' : 672 , `` big_o '' : `` N '' , `` time_unit '' : `` ns '' } , { `` name '' : `` ComputationBenchmarks_RMS '' , `` rms '' : 2 % } Removing them manually works fine , so presumably the script should just ignore complexity-related entries in the JSON .
compare_bench fails if Complexity was computed __EoT__ When the benchmarks are are run with ` Complexity ( ) ` enabled , the output JSON file contains additional members that cause the ` compare_bench.py ` script to fail : > Invalid input file : 'out.json ' does not name a valid benchmark executable or JSON file The offending lines in the JSON look like : { `` name '' : `` ComputationBenchmarks_BigO '' , `` cpu_coefficient '' : 672 , `` real_coefficient '' : 672 , `` big_o '' : `` N '' , `` time_unit '' : `` ns '' } , { `` name '' : `` ComputationBenchmarks_RMS '' , `` rms '' : 2 % } Removing them manually works fine , so presumably the script should just ignore complexity-related entries in the JSON .
Repetition is too large on MAC OSX and does not seem controllable __EoT__ Hi I am running some very simple benchmark ( similar to the first example given in the README ) . And I have encountered these errors : `` ` The number of inputs is very large . BM_UFTree will be repeated at least 18446744072586632936 times . benchmark ( 15824,0x7fffaed953c0 ) malloc : *** mach_vm_map ( size=18446743903025913856 ) failed ( error code=3 ) *** error : ca n't allocate region *** set a breakpoint in malloc_error_break to debug libc++abi.dylib : terminating with uncaught exception of type std : :bad_alloc : std : :bad_alloc [ 1 ] 15824 abort ./benchmark -- benchmark_repetitions=100 `` ` As you can see the command line argument does not seems to have any effect here . Am I missing something ? Thanks ! I am testing with clang++ on Mac .
Add tests to user counters __EoT__ Now that user counters were integrated , tests should be added . During review of # 262 , # 276 was suggested which added some tests . @ EricWF Could you add your tests so that I pick up from there ?
Add tests to user counters __EoT__ Now that user counters were integrated , tests should be added . During review of # 262 , # 276 was suggested which added some tests . @ EricWF Could you add your tests so that I pick up from there ?
Add tests to user counters __EoT__ Now that user counters were integrated , tests should be added . During review of # 262 , # 276 was suggested which added some tests . @ EricWF Could you add your tests so that I pick up from there ?
Add tests to user counters __EoT__ Now that user counters were integrated , tests should be added . During review of # 262 , # 276 was suggested which added some tests . @ EricWF Could you add your tests so that I pick up from there ?
Add tests to user counters __EoT__ Now that user counters were integrated , tests should be added . During review of # 262 , # 276 was suggested which added some tests . @ EricWF Could you add your tests so that I pick up from there ?
Add tests to user counters __EoT__ Now that user counters were integrated , tests should be added . During review of # 262 , # 276 was suggested which added some tests . @ EricWF Could you add your tests so that I pick up from there ?
Add tests to user counters __EoT__ Now that user counters were integrated , tests should be added . During review of # 262 , # 276 was suggested which added some tests . @ EricWF Could you add your tests so that I pick up from there ?
Add tests to user counters __EoT__ Now that user counters were integrated , tests should be added . During review of # 262 , # 276 was suggested which added some tests . @ EricWF Could you add your tests so that I pick up from there ?
Add tests to user counters __EoT__ Now that user counters were integrated , tests should be added . During review of # 262 , # 276 was suggested which added some tests . @ EricWF Could you add your tests so that I pick up from there ?
Add tests to user counters __EoT__ Now that user counters were integrated , tests should be added . During review of # 262 , # 276 was suggested which added some tests . @ EricWF Could you add your tests so that I pick up from there ?
Add tests to user counters __EoT__ Now that user counters were integrated , tests should be added . During review of # 262 , # 276 was suggested which added some tests . @ EricWF Could you add your tests so that I pick up from there ?
Add tests to user counters __EoT__ Now that user counters were integrated , tests should be added . During review of # 262 , # 276 was suggested which added some tests . @ EricWF Could you add your tests so that I pick up from there ?
Add tests to user counters __EoT__ Now that user counters were integrated , tests should be added . During review of # 262 , # 276 was suggested which added some tests . @ EricWF Could you add your tests so that I pick up from there ?
Add tests to user counters __EoT__ Now that user counters were integrated , tests should be added . During review of # 262 , # 276 was suggested which added some tests . @ EricWF Could you add your tests so that I pick up from there ?
Add tests to user counters __EoT__ Now that user counters were integrated , tests should be added . During review of # 262 , # 276 was suggested which added some tests . @ EricWF Could you add your tests so that I pick up from there ?
Re-add support for per-thread timing and run results . __EoT__ Issue # 86 is going to remove this functionality . In # 86 tests that are run on more than one thread report as a single result . Dominic would like to see this functionality readded .
Re-add support for per-thread timing and run results . __EoT__ Issue # 86 is going to remove this functionality . In # 86 tests that are run on more than one thread report as a single result . Dominic would like to see this functionality readded .
Impossible to specify number of iterations and number of repetitions at the same time . __EoT__ I have created a simple test : `` ` c++ # include < benchmark/benchmark.h > static void BM_Test ( benchmark : :State & state ) { std : :string x ( 1024 , 'x ' ) ; while ( state.KeepRunning ( ) ) { std : :string copy ( x ) ; } } BENCHMARK ( BM_Test ) ; int main ( int argc , const char ** argv ) { benchmark : :Initialize ( & argc , argv ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; return 0 ; } `` ` And now I try to launch it with specified number of iterations and repetitions : - when I specify only number of iterations it works good : `` ` $ ./test -- benchmark_iterations=10000 Reading /proc/self/cputime_ns failed . Using getrusage ( ) . Benchmarking on 8 X 2401 MHz CPUs 2014/10/04-11:12:01 CPU scaling is enabled : Benchmark timings may be noisy . DEBUG : Benchmark Time ( ns ) CPU ( ns ) Iterations -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
Impossible to specify number of iterations and number of repetitions at the same time . __EoT__ I have created a simple test : `` ` c++ # include < benchmark/benchmark.h > static void BM_Test ( benchmark : :State & state ) { std : :string x ( 1024 , 'x ' ) ; while ( state.KeepRunning ( ) ) { std : :string copy ( x ) ; } } BENCHMARK ( BM_Test ) ; int main ( int argc , const char ** argv ) { benchmark : :Initialize ( & argc , argv ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; return 0 ; } `` ` And now I try to launch it with specified number of iterations and repetitions : - when I specify only number of iterations it works good : `` ` $ ./test -- benchmark_iterations=10000 Reading /proc/self/cputime_ns failed . Using getrusage ( ) . Benchmarking on 8 X 2401 MHz CPUs 2014/10/04-11:12:01 CPU scaling is enabled : Benchmark timings may be noisy . DEBUG : Benchmark Time ( ns ) CPU ( ns ) Iterations -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
Impossible to specify number of iterations and number of repetitions at the same time . __EoT__ I have created a simple test : `` ` c++ # include < benchmark/benchmark.h > static void BM_Test ( benchmark : :State & state ) { std : :string x ( 1024 , 'x ' ) ; while ( state.KeepRunning ( ) ) { std : :string copy ( x ) ; } } BENCHMARK ( BM_Test ) ; int main ( int argc , const char ** argv ) { benchmark : :Initialize ( & argc , argv ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; return 0 ; } `` ` And now I try to launch it with specified number of iterations and repetitions : - when I specify only number of iterations it works good : `` ` $ ./test -- benchmark_iterations=10000 Reading /proc/self/cputime_ns failed . Using getrusage ( ) . Benchmarking on 8 X 2401 MHz CPUs 2014/10/04-11:12:01 CPU scaling is enabled : Benchmark timings may be noisy . DEBUG : Benchmark Time ( ns ) CPU ( ns ) Iterations -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
Undefined reference to UseRealTime __EoT__ Hello , I tried to use the function to use realtime instead of CPU time : `` ` c++ // If a particular benchmark is I/O bound , or if for some reason CPU // timings are not representative , call this method from within the // benchmark routine . If called , the elapsed time will be used to // control how many iterations are run , and in the printing of // items/second or MB/seconds values . If not called , the cpu time // used by the benchmark will be used . void UseRealTime ( ) ; `` ` like this : `` ` c++ static void BM_foo ( benchmark : :State & state ) { benchmark : :UseRealTime ( ) ; while ( state.KeepRunning ( ) ) { // Stuff } } `` ` But this wo n't link . The function is actually defined in the ` benchmark : :internal ` namespace . Simply moving it to the ` benchmark ` namespace _seems_ to work ( at least to compile ) . Is this function supposed to be used like this ? Also the function ` void SetLabel ( const
[ RFC ] *Display* aggregates only . __EoT__ There is a flag https : //github.com/google/benchmark/blob/d9cab612e40017af10bddaa5b60c7067032a9e1c/src/benchmark.cc # L75-L78 and a call https : //github.com/google/benchmark/blob/d9cab612e40017af10bddaa5b60c7067032a9e1c/include/benchmark/benchmark.h # L837-L840 But that affects everything , every reporter , destination : https : //github.com/google/benchmark/blob/d9cab612e40017af10bddaa5b60c7067032a9e1c/src/benchmark.cc # L316 It would be quite useful to have an ability to be more picky . More specifically , i would like to be able to only see the aggregates in the on-screen output , but for the file output to still contain everything . The former is useful in case of a lot of repetition ( or even more so if every iteration is reported separately ) , while the former is **great** for tooling . Now the problem . I 'm not sure how best to do it . The most straight-forward solution i can think of is to filter the results here : https : //github.com/google/benchmark/blob/d9cab612e40017af10bddaa5b60c7067032a9e1c/src/benchmark.cc # L466-L471 So the file reporter will still receive the full ` std : :vector < BenchmarkReporter : :Run > reports ` , while ` display_reporter ` may ( depending on the configuration ) only receive the aggregates . Alternative solutions could include modifying the API of ` BenchmarkReporter ` , and every
[ RFC ] *Display* aggregates only . __EoT__ There is a flag https : //github.com/google/benchmark/blob/d9cab612e40017af10bddaa5b60c7067032a9e1c/src/benchmark.cc # L75-L78 and a call https : //github.com/google/benchmark/blob/d9cab612e40017af10bddaa5b60c7067032a9e1c/include/benchmark/benchmark.h # L837-L840 But that affects everything , every reporter , destination : https : //github.com/google/benchmark/blob/d9cab612e40017af10bddaa5b60c7067032a9e1c/src/benchmark.cc # L316 It would be quite useful to have an ability to be more picky . More specifically , i would like to be able to only see the aggregates in the on-screen output , but for the file output to still contain everything . The former is useful in case of a lot of repetition ( or even more so if every iteration is reported separately ) , while the former is **great** for tooling . Now the problem . I 'm not sure how best to do it . The most straight-forward solution i can think of is to filter the results here : https : //github.com/google/benchmark/blob/d9cab612e40017af10bddaa5b60c7067032a9e1c/src/benchmark.cc # L466-L471 So the file reporter will still receive the full ` std : :vector < BenchmarkReporter : :Run > reports ` , while ` display_reporter ` may ( depending on the configuration ) only receive the aggregates . Alternative solutions could include modifying the API of ` BenchmarkReporter ` , and every
Build fails with ICC17u2 __EoT__ The build fails with ICC17 because of warnings and Werror . What is the correct solution to fix it ? Should a patch 1 ) disable Werror for ICC ( or maybe all non known compilers ) 2 ) disable the false postive warnings for all files . This could be done using : add_cxx_compiler_flag ( -wd2102 ) # ICC17u2 : Many false positives for Wstrict-aliasing add_cxx_compiler_flag ( -wd2259 ) # ICC17u2 : non-pointer conversion from `` long '' to `` int '' may lose significant bits ( even for explicit static cast , sleep.cc ( 44 ) ) add_cxx_compiler_flag ( -wd654 ) # ICC17u2 : overloaded virtual function `` benchmark : :Fixture : :SetUp '' is only partially overridden ( because of deprecated overload ) 3 ) disable warnings at file level or some other granularity Another warning which is n't clearly a false positive is ../test/output_test_helper.cc ( 34 ) : error # 3280 : declaration hides variable `` < unnamed > : :dec_re '' ( declared at line 67 of `` ../test/output_test.h '' ) Should the local dec_re be renamed or should that warning be suppressed too for 2/3 ? If I know
Build fails with ICC17u2 __EoT__ The build fails with ICC17 because of warnings and Werror . What is the correct solution to fix it ? Should a patch 1 ) disable Werror for ICC ( or maybe all non known compilers ) 2 ) disable the false postive warnings for all files . This could be done using : add_cxx_compiler_flag ( -wd2102 ) # ICC17u2 : Many false positives for Wstrict-aliasing add_cxx_compiler_flag ( -wd2259 ) # ICC17u2 : non-pointer conversion from `` long '' to `` int '' may lose significant bits ( even for explicit static cast , sleep.cc ( 44 ) ) add_cxx_compiler_flag ( -wd654 ) # ICC17u2 : overloaded virtual function `` benchmark : :Fixture : :SetUp '' is only partially overridden ( because of deprecated overload ) 3 ) disable warnings at file level or some other granularity Another warning which is n't clearly a false positive is ../test/output_test_helper.cc ( 34 ) : error # 3280 : declaration hides variable `` < unnamed > : :dec_re '' ( declared at line 67 of `` ../test/output_test.h '' ) Should the local dec_re be renamed or should that warning be suppressed too for 2/3 ? If I know
Build fails with ICC17u2 __EoT__ The build fails with ICC17 because of warnings and Werror . What is the correct solution to fix it ? Should a patch 1 ) disable Werror for ICC ( or maybe all non known compilers ) 2 ) disable the false postive warnings for all files . This could be done using : add_cxx_compiler_flag ( -wd2102 ) # ICC17u2 : Many false positives for Wstrict-aliasing add_cxx_compiler_flag ( -wd2259 ) # ICC17u2 : non-pointer conversion from `` long '' to `` int '' may lose significant bits ( even for explicit static cast , sleep.cc ( 44 ) ) add_cxx_compiler_flag ( -wd654 ) # ICC17u2 : overloaded virtual function `` benchmark : :Fixture : :SetUp '' is only partially overridden ( because of deprecated overload ) 3 ) disable warnings at file level or some other granularity Another warning which is n't clearly a false positive is ../test/output_test_helper.cc ( 34 ) : error # 3280 : declaration hides variable `` < unnamed > : :dec_re '' ( declared at line 67 of `` ../test/output_test.h '' ) Should the local dec_re be renamed or should that warning be suppressed too for 2/3 ? If I know
Build fails with ICC17u2 __EoT__ The build fails with ICC17 because of warnings and Werror . What is the correct solution to fix it ? Should a patch 1 ) disable Werror for ICC ( or maybe all non known compilers ) 2 ) disable the false postive warnings for all files . This could be done using : add_cxx_compiler_flag ( -wd2102 ) # ICC17u2 : Many false positives for Wstrict-aliasing add_cxx_compiler_flag ( -wd2259 ) # ICC17u2 : non-pointer conversion from `` long '' to `` int '' may lose significant bits ( even for explicit static cast , sleep.cc ( 44 ) ) add_cxx_compiler_flag ( -wd654 ) # ICC17u2 : overloaded virtual function `` benchmark : :Fixture : :SetUp '' is only partially overridden ( because of deprecated overload ) 3 ) disable warnings at file level or some other granularity Another warning which is n't clearly a false positive is ../test/output_test_helper.cc ( 34 ) : error # 3280 : declaration hides variable `` < unnamed > : :dec_re '' ( declared at line 67 of `` ../test/output_test.h '' ) Should the local dec_re be renamed or should that warning be suppressed too for 2/3 ? If I know
Build fails with ICC17u2 __EoT__ The build fails with ICC17 because of warnings and Werror . What is the correct solution to fix it ? Should a patch 1 ) disable Werror for ICC ( or maybe all non known compilers ) 2 ) disable the false postive warnings for all files . This could be done using : add_cxx_compiler_flag ( -wd2102 ) # ICC17u2 : Many false positives for Wstrict-aliasing add_cxx_compiler_flag ( -wd2259 ) # ICC17u2 : non-pointer conversion from `` long '' to `` int '' may lose significant bits ( even for explicit static cast , sleep.cc ( 44 ) ) add_cxx_compiler_flag ( -wd654 ) # ICC17u2 : overloaded virtual function `` benchmark : :Fixture : :SetUp '' is only partially overridden ( because of deprecated overload ) 3 ) disable warnings at file level or some other granularity Another warning which is n't clearly a false positive is ../test/output_test_helper.cc ( 34 ) : error # 3280 : declaration hides variable `` < unnamed > : :dec_re '' ( declared at line 67 of `` ../test/output_test.h '' ) Should the local dec_re be renamed or should that warning be suppressed too for 2/3 ? If I know
Build fails with ICC17u2 __EoT__ The build fails with ICC17 because of warnings and Werror . What is the correct solution to fix it ? Should a patch 1 ) disable Werror for ICC ( or maybe all non known compilers ) 2 ) disable the false postive warnings for all files . This could be done using : add_cxx_compiler_flag ( -wd2102 ) # ICC17u2 : Many false positives for Wstrict-aliasing add_cxx_compiler_flag ( -wd2259 ) # ICC17u2 : non-pointer conversion from `` long '' to `` int '' may lose significant bits ( even for explicit static cast , sleep.cc ( 44 ) ) add_cxx_compiler_flag ( -wd654 ) # ICC17u2 : overloaded virtual function `` benchmark : :Fixture : :SetUp '' is only partially overridden ( because of deprecated overload ) 3 ) disable warnings at file level or some other granularity Another warning which is n't clearly a false positive is ../test/output_test_helper.cc ( 34 ) : error # 3280 : declaration hides variable `` < unnamed > : :dec_re '' ( declared at line 67 of `` ../test/output_test.h '' ) Should the local dec_re be renamed or should that warning be suppressed too for 2/3 ? If I know
Build fails with ICC17u2 __EoT__ The build fails with ICC17 because of warnings and Werror . What is the correct solution to fix it ? Should a patch 1 ) disable Werror for ICC ( or maybe all non known compilers ) 2 ) disable the false postive warnings for all files . This could be done using : add_cxx_compiler_flag ( -wd2102 ) # ICC17u2 : Many false positives for Wstrict-aliasing add_cxx_compiler_flag ( -wd2259 ) # ICC17u2 : non-pointer conversion from `` long '' to `` int '' may lose significant bits ( even for explicit static cast , sleep.cc ( 44 ) ) add_cxx_compiler_flag ( -wd654 ) # ICC17u2 : overloaded virtual function `` benchmark : :Fixture : :SetUp '' is only partially overridden ( because of deprecated overload ) 3 ) disable warnings at file level or some other granularity Another warning which is n't clearly a false positive is ../test/output_test_helper.cc ( 34 ) : error # 3280 : declaration hides variable `` < unnamed > : :dec_re '' ( declared at line 67 of `` ../test/output_test.h '' ) Should the local dec_re be renamed or should that warning be suppressed too for 2/3 ? If I know
Build fails with ICC17u2 __EoT__ The build fails with ICC17 because of warnings and Werror . What is the correct solution to fix it ? Should a patch 1 ) disable Werror for ICC ( or maybe all non known compilers ) 2 ) disable the false postive warnings for all files . This could be done using : add_cxx_compiler_flag ( -wd2102 ) # ICC17u2 : Many false positives for Wstrict-aliasing add_cxx_compiler_flag ( -wd2259 ) # ICC17u2 : non-pointer conversion from `` long '' to `` int '' may lose significant bits ( even for explicit static cast , sleep.cc ( 44 ) ) add_cxx_compiler_flag ( -wd654 ) # ICC17u2 : overloaded virtual function `` benchmark : :Fixture : :SetUp '' is only partially overridden ( because of deprecated overload ) 3 ) disable warnings at file level or some other granularity Another warning which is n't clearly a false positive is ../test/output_test_helper.cc ( 34 ) : error # 3280 : declaration hides variable `` < unnamed > : :dec_re '' ( declared at line 67 of `` ../test/output_test.h '' ) Should the local dec_re be renamed or should that warning be suppressed too for 2/3 ? If I know
Build fails with ICC17u2 __EoT__ The build fails with ICC17 because of warnings and Werror . What is the correct solution to fix it ? Should a patch 1 ) disable Werror for ICC ( or maybe all non known compilers ) 2 ) disable the false postive warnings for all files . This could be done using : add_cxx_compiler_flag ( -wd2102 ) # ICC17u2 : Many false positives for Wstrict-aliasing add_cxx_compiler_flag ( -wd2259 ) # ICC17u2 : non-pointer conversion from `` long '' to `` int '' may lose significant bits ( even for explicit static cast , sleep.cc ( 44 ) ) add_cxx_compiler_flag ( -wd654 ) # ICC17u2 : overloaded virtual function `` benchmark : :Fixture : :SetUp '' is only partially overridden ( because of deprecated overload ) 3 ) disable warnings at file level or some other granularity Another warning which is n't clearly a false positive is ../test/output_test_helper.cc ( 34 ) : error # 3280 : declaration hides variable `` < unnamed > : :dec_re '' ( declared at line 67 of `` ../test/output_test.h '' ) Should the local dec_re be renamed or should that warning be suppressed too for 2/3 ? If I know
Build fails with ICC17u2 __EoT__ The build fails with ICC17 because of warnings and Werror . What is the correct solution to fix it ? Should a patch 1 ) disable Werror for ICC ( or maybe all non known compilers ) 2 ) disable the false postive warnings for all files . This could be done using : add_cxx_compiler_flag ( -wd2102 ) # ICC17u2 : Many false positives for Wstrict-aliasing add_cxx_compiler_flag ( -wd2259 ) # ICC17u2 : non-pointer conversion from `` long '' to `` int '' may lose significant bits ( even for explicit static cast , sleep.cc ( 44 ) ) add_cxx_compiler_flag ( -wd654 ) # ICC17u2 : overloaded virtual function `` benchmark : :Fixture : :SetUp '' is only partially overridden ( because of deprecated overload ) 3 ) disable warnings at file level or some other granularity Another warning which is n't clearly a false positive is ../test/output_test_helper.cc ( 34 ) : error # 3280 : declaration hides variable `` < unnamed > : :dec_re '' ( declared at line 67 of `` ../test/output_test.h '' ) Should the local dec_re be renamed or should that warning be suppressed too for 2/3 ? If I know
Build fails with ICC17u2 __EoT__ The build fails with ICC17 because of warnings and Werror . What is the correct solution to fix it ? Should a patch 1 ) disable Werror for ICC ( or maybe all non known compilers ) 2 ) disable the false postive warnings for all files . This could be done using : add_cxx_compiler_flag ( -wd2102 ) # ICC17u2 : Many false positives for Wstrict-aliasing add_cxx_compiler_flag ( -wd2259 ) # ICC17u2 : non-pointer conversion from `` long '' to `` int '' may lose significant bits ( even for explicit static cast , sleep.cc ( 44 ) ) add_cxx_compiler_flag ( -wd654 ) # ICC17u2 : overloaded virtual function `` benchmark : :Fixture : :SetUp '' is only partially overridden ( because of deprecated overload ) 3 ) disable warnings at file level or some other granularity Another warning which is n't clearly a false positive is ../test/output_test_helper.cc ( 34 ) : error # 3280 : declaration hides variable `` < unnamed > : :dec_re '' ( declared at line 67 of `` ../test/output_test.h '' ) Should the local dec_re be renamed or should that warning be suppressed too for 2/3 ? If I know
Build fails with ICC17u2 __EoT__ The build fails with ICC17 because of warnings and Werror . What is the correct solution to fix it ? Should a patch 1 ) disable Werror for ICC ( or maybe all non known compilers ) 2 ) disable the false postive warnings for all files . This could be done using : add_cxx_compiler_flag ( -wd2102 ) # ICC17u2 : Many false positives for Wstrict-aliasing add_cxx_compiler_flag ( -wd2259 ) # ICC17u2 : non-pointer conversion from `` long '' to `` int '' may lose significant bits ( even for explicit static cast , sleep.cc ( 44 ) ) add_cxx_compiler_flag ( -wd654 ) # ICC17u2 : overloaded virtual function `` benchmark : :Fixture : :SetUp '' is only partially overridden ( because of deprecated overload ) 3 ) disable warnings at file level or some other granularity Another warning which is n't clearly a false positive is ../test/output_test_helper.cc ( 34 ) : error # 3280 : declaration hides variable `` < unnamed > : :dec_re '' ( declared at line 67 of `` ../test/output_test.h '' ) Should the local dec_re be renamed or should that warning be suppressed too for 2/3 ? If I know
Fine-grained iteration count control __EoT__ Is there a way to control the iteration count directly instead of -- benchmark_min_time ?
Fine-grained iteration count control __EoT__ Is there a way to control the iteration count directly instead of -- benchmark_min_time ?
Fine-grained iteration count control __EoT__ Is there a way to control the iteration count directly instead of -- benchmark_min_time ?
Fine-grained iteration count control __EoT__ Is there a way to control the iteration count directly instead of -- benchmark_min_time ?
Fine-grained iteration count control __EoT__ Is there a way to control the iteration count directly instead of -- benchmark_min_time ?
Fine-grained iteration count control __EoT__ Is there a way to control the iteration count directly instead of -- benchmark_min_time ?
Fine-grained iteration count control __EoT__ Is there a way to control the iteration count directly instead of -- benchmark_min_time ?
Fine-grained iteration count control __EoT__ Is there a way to control the iteration count directly instead of -- benchmark_min_time ?
Fine-grained iteration count control __EoT__ Is there a way to control the iteration count directly instead of -- benchmark_min_time ?
Fine-grained iteration count control __EoT__ Is there a way to control the iteration count directly instead of -- benchmark_min_time ?
Fine-grained iteration count control __EoT__ Is there a way to control the iteration count directly instead of -- benchmark_min_time ?
Fine-grained iteration count control __EoT__ Is there a way to control the iteration count directly instead of -- benchmark_min_time ?
Fine-grained iteration count control __EoT__ Is there a way to control the iteration count directly instead of -- benchmark_min_time ?
Fine-grained iteration count control __EoT__ Is there a way to control the iteration count directly instead of -- benchmark_min_time ?
Fine-grained iteration count control __EoT__ Is there a way to control the iteration count directly instead of -- benchmark_min_time ?
Fine-grained iteration count control __EoT__ Is there a way to control the iteration count directly instead of -- benchmark_min_time ?
Fine-grained iteration count control __EoT__ Is there a way to control the iteration count directly instead of -- benchmark_min_time ?
Fine-grained iteration count control __EoT__ Is there a way to control the iteration count directly instead of -- benchmark_min_time ?
User Counters ignore Flag parameter __EoT__ When creating a user counter it display the count as if ` kDefaults ` flag was used even if explictly setting a different flag ` st.counters [ `` Foo '' ] = benchmark : :Counter ( FooCount , benchmark : :Counter : :kIsRate ) ; `
User Counters ignore Flag parameter __EoT__ When creating a user counter it display the count as if ` kDefaults ` flag was used even if explictly setting a different flag ` st.counters [ `` Foo '' ] = benchmark : :Counter ( FooCount , benchmark : :Counter : :kIsRate ) ; `
User Counters ignore Flag parameter __EoT__ When creating a user counter it display the count as if ` kDefaults ` flag was used even if explictly setting a different flag ` st.counters [ `` Foo '' ] = benchmark : :Counter ( FooCount , benchmark : :Counter : :kIsRate ) ; `
User Counters ignore Flag parameter __EoT__ When creating a user counter it display the count as if ` kDefaults ` flag was used even if explictly setting a different flag ` st.counters [ `` Foo '' ] = benchmark : :Counter ( FooCount , benchmark : :Counter : :kIsRate ) ; `
User Counters ignore Flag parameter __EoT__ When creating a user counter it display the count as if ` kDefaults ` flag was used even if explictly setting a different flag ` st.counters [ `` Foo '' ] = benchmark : :Counter ( FooCount , benchmark : :Counter : :kIsRate ) ; `
User Counters ignore Flag parameter __EoT__ When creating a user counter it display the count as if ` kDefaults ` flag was used even if explictly setting a different flag ` st.counters [ `` Foo '' ] = benchmark : :Counter ( FooCount , benchmark : :Counter : :kIsRate ) ; `
User Counters ignore Flag parameter __EoT__ When creating a user counter it display the count as if ` kDefaults ` flag was used even if explictly setting a different flag ` st.counters [ `` Foo '' ] = benchmark : :Counter ( FooCount , benchmark : :Counter : :kIsRate ) ; `
User Counters ignore Flag parameter __EoT__ When creating a user counter it display the count as if ` kDefaults ` flag was used even if explictly setting a different flag ` st.counters [ `` Foo '' ] = benchmark : :Counter ( FooCount , benchmark : :Counter : :kIsRate ) ; `
User Counters ignore Flag parameter __EoT__ When creating a user counter it display the count as if ` kDefaults ` flag was used even if explictly setting a different flag ` st.counters [ `` Foo '' ] = benchmark : :Counter ( FooCount , benchmark : :Counter : :kIsRate ) ; `
User Counters ignore Flag parameter __EoT__ When creating a user counter it display the count as if ` kDefaults ` flag was used even if explictly setting a different flag ` st.counters [ `` Foo '' ] = benchmark : :Counter ( FooCount , benchmark : :Counter : :kIsRate ) ; `
User Counters ignore Flag parameter __EoT__ When creating a user counter it display the count as if ` kDefaults ` flag was used even if explictly setting a different flag ` st.counters [ `` Foo '' ] = benchmark : :Counter ( FooCount , benchmark : :Counter : :kIsRate ) ; `
User Counters ignore Flag parameter __EoT__ When creating a user counter it display the count as if ` kDefaults ` flag was used even if explictly setting a different flag ` st.counters [ `` Foo '' ] = benchmark : :Counter ( FooCount , benchmark : :Counter : :kIsRate ) ; `
CXXFeatureCheck for regex for cross compiling failing __EoT__ CXXFeature testing is not possible for cross compiling to another platform . Specifying -DRUN_HAVE_POSIX_REGEX=flase -DHAVE_POSIX_REGEX=true does not pass these definitions to the makefiles .
CXXFeatureCheck for regex for cross compiling failing __EoT__ CXXFeature testing is not possible for cross compiling to another platform . Specifying -DRUN_HAVE_POSIX_REGEX=flase -DHAVE_POSIX_REGEX=true does not pass these definitions to the makefiles .
CXXFeatureCheck for regex for cross compiling failing __EoT__ CXXFeature testing is not possible for cross compiling to another platform . Specifying -DRUN_HAVE_POSIX_REGEX=flase -DHAVE_POSIX_REGEX=true does not pass these definitions to the makefiles .
CXXFeatureCheck for regex for cross compiling failing __EoT__ CXXFeature testing is not possible for cross compiling to another platform . Specifying -DRUN_HAVE_POSIX_REGEX=flase -DHAVE_POSIX_REGEX=true does not pass these definitions to the makefiles .
CXXFeatureCheck for regex for cross compiling failing __EoT__ CXXFeature testing is not possible for cross compiling to another platform . Specifying -DRUN_HAVE_POSIX_REGEX=flase -DHAVE_POSIX_REGEX=true does not pass these definitions to the makefiles .
CXXFeatureCheck for regex for cross compiling failing __EoT__ CXXFeature testing is not possible for cross compiling to another platform . Specifying -DRUN_HAVE_POSIX_REGEX=flase -DHAVE_POSIX_REGEX=true does not pass these definitions to the makefiles .
CXXFeatureCheck for regex for cross compiling failing __EoT__ CXXFeature testing is not possible for cross compiling to another platform . Specifying -DRUN_HAVE_POSIX_REGEX=flase -DHAVE_POSIX_REGEX=true does not pass these definitions to the makefiles .
CXXFeatureCheck for regex for cross compiling failing __EoT__ CXXFeature testing is not possible for cross compiling to another platform . Specifying -DRUN_HAVE_POSIX_REGEX=flase -DHAVE_POSIX_REGEX=true does not pass these definitions to the makefiles .
CXXFeatureCheck for regex for cross compiling failing __EoT__ CXXFeature testing is not possible for cross compiling to another platform . Specifying -DRUN_HAVE_POSIX_REGEX=flase -DHAVE_POSIX_REGEX=true does not pass these definitions to the makefiles .
CXXFeatureCheck for regex for cross compiling failing __EoT__ CXXFeature testing is not possible for cross compiling to another platform . Specifying -DRUN_HAVE_POSIX_REGEX=flase -DHAVE_POSIX_REGEX=true does not pass these definitions to the makefiles .
CXXFeatureCheck for regex for cross compiling failing __EoT__ CXXFeature testing is not possible for cross compiling to another platform . Specifying -DRUN_HAVE_POSIX_REGEX=flase -DHAVE_POSIX_REGEX=true does not pass these definitions to the makefiles .
CXXFeatureCheck for regex for cross compiling failing __EoT__ CXXFeature testing is not possible for cross compiling to another platform . Specifying -DRUN_HAVE_POSIX_REGEX=flase -DHAVE_POSIX_REGEX=true does not pass these definitions to the makefiles .
CXXFeatureCheck for regex for cross compiling failing __EoT__ CXXFeature testing is not possible for cross compiling to another platform . Specifying -DRUN_HAVE_POSIX_REGEX=flase -DHAVE_POSIX_REGEX=true does not pass these definitions to the makefiles .
CXXFeatureCheck for regex for cross compiling failing __EoT__ CXXFeature testing is not possible for cross compiling to another platform . Specifying -DRUN_HAVE_POSIX_REGEX=flase -DHAVE_POSIX_REGEX=true does not pass these definitions to the makefiles .
CXXFeatureCheck for regex for cross compiling failing __EoT__ CXXFeature testing is not possible for cross compiling to another platform . Specifying -DRUN_HAVE_POSIX_REGEX=flase -DHAVE_POSIX_REGEX=true does not pass these definitions to the makefiles .
CXXFeatureCheck for regex for cross compiling failing __EoT__ CXXFeature testing is not possible for cross compiling to another platform . Specifying -DRUN_HAVE_POSIX_REGEX=flase -DHAVE_POSIX_REGEX=true does not pass these definitions to the makefiles .
CXXFeatureCheck for regex for cross compiling failing __EoT__ CXXFeature testing is not possible for cross compiling to another platform . Specifying -DRUN_HAVE_POSIX_REGEX=flase -DHAVE_POSIX_REGEX=true does not pass these definitions to the makefiles .
CXXFeatureCheck for regex for cross compiling failing __EoT__ CXXFeature testing is not possible for cross compiling to another platform . Specifying -DRUN_HAVE_POSIX_REGEX=flase -DHAVE_POSIX_REGEX=true does not pass these definitions to the makefiles .
Eliminate regex . { h , cc } and use the C++11 version __EoT__ This came up as part of the discussion on # 29 , see https : //github.com/google/benchmark/pull/29 # commitcomment-6593215 .
Custom Stats __EoT__ From the README and the API header , it looks like the supported metrics are bytes processed and items processed . Is it possible to extend these with custom metrics ? For example , one might be interested in GFLOPS , or other derived metrics such as raw bandwidth v/s effective bandwidth .
Custom Stats __EoT__ From the README and the API header , it looks like the supported metrics are bytes processed and items processed . Is it possible to extend these with custom metrics ? For example , one might be interested in GFLOPS , or other derived metrics such as raw bandwidth v/s effective bandwidth .
Custom Stats __EoT__ From the README and the API header , it looks like the supported metrics are bytes processed and items processed . Is it possible to extend these with custom metrics ? For example , one might be interested in GFLOPS , or other derived metrics such as raw bandwidth v/s effective bandwidth .
Custom Stats __EoT__ From the README and the API header , it looks like the supported metrics are bytes processed and items processed . Is it possible to extend these with custom metrics ? For example , one might be interested in GFLOPS , or other derived metrics such as raw bandwidth v/s effective bandwidth .
Custom Stats __EoT__ From the README and the API header , it looks like the supported metrics are bytes processed and items processed . Is it possible to extend these with custom metrics ? For example , one might be interested in GFLOPS , or other derived metrics such as raw bandwidth v/s effective bandwidth .
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
Build fails in benchmark.cc __EoT__ Hello , I 've got to these two errors , while trying to build google benchmarks for the project I 'm on : `` ` /home/ucapgui/BICO/benchmark/src/benchmark.cc ( 177 ) : error # 2259 : non-pointer conversion from `` long '' to `` int '' may lose significant bits results.complexity_n += st.complexity_length_n ( ) ; ^ /home/ucapgui/BICO/benchmark/src/benchmark.cc ( 326 ) : error # 1875 : offsetof applied to non-POD ( Plain Old Data ) types is nonstandard static_assert ( offsetof ( State , error_occurred_ ) < = ^ `` ` Any idea how to fix it ? Thank you .
What is the critera/timing for creating a new release ? __EoT__ I recently update ponylang to an arbitrary commit on master as they were at pre v1.1.0 . I 'd like to update ponylang to a formal release if one might be coming soonish , hence my question .
What is the critera/timing for creating a new release ? __EoT__ I recently update ponylang to an arbitrary commit on master as they were at pre v1.1.0 . I 'd like to update ponylang to a formal release if one might be coming soonish , hence my question .
What is the critera/timing for creating a new release ? __EoT__ I recently update ponylang to an arbitrary commit on master as they were at pre v1.1.0 . I 'd like to update ponylang to a formal release if one might be coming soonish , hence my question .
What is the critera/timing for creating a new release ? __EoT__ I recently update ponylang to an arbitrary commit on master as they were at pre v1.1.0 . I 'd like to update ponylang to a formal release if one might be coming soonish , hence my question .
RMS calculation seems broken , takes time Unit into account ? __EoT__ Hi . Consider the [ ` BM_Complexity_O_N ` ] ( https : //github.com/google/benchmark/blob/36a251ab3ee4e6dc668129e0ef5fa62b1b8d5d49/test/complexity_test.cc # L80-L113 ) Vanilla version outputs : `` ` Benchmark Time CPU Iterations -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - BM_Complexity_O_N/1024 441 ns 441 ns 1583329 BM_Complexity_O_N/2k 881 ns 881 ns 797104 BM_Complexity_O_N/4k 1767 ns 1767 ns 395988 BM_Complexity_O_N/8k 3691 ns 3690 ns 189567 BM_Complexity_O_N/16k 7377 ns 7376 ns 95031 BM_Complexity_O_N/32k 14813 ns 14801 ns 47507 BM_Complexity_O_N/64k 29605 ns 29602 ns 23608 BM_Complexity_O_N_BigO 0.45 N 0.45 N BM_Complexity_O_N_RMS 0 % 0 % BM_Complexity_O_N/1024 446 ns 446 ns 1587672 BM_Complexity_O_N/2k 884 ns 884 ns 792614 BM_Complexity_O_N/4k 1787 ns 1786 ns 393225 BM_Complexity_O_N/8k 3684 ns 3683 ns 189791 BM_Complexity_O_N/16k 7373 ns 7369 ns 95181 BM_Complexity_O_N/32k 14716 ns 14711 ns 47732 BM_Complexity_O_N/64k 29714 ns 29707 ns 23656 BM_Complexity_O_N_BigO 0.45 f ( N ) 0.45 f ( N ) BM_Complexity_O_N_RMS 1 % 1 % BM_Complexity_O_N/1024 442 ns 442 ns 1589204 BM_Complexity_O_N/2k 885 ns 884 ns 788984 BM_Complexity_O_N/4k 1781 ns 1781 ns 390535 BM_Complexity_O_N/8k 3691 ns
RMS calculation seems broken , takes time Unit into account ? __EoT__ Hi . Consider the [ ` BM_Complexity_O_N ` ] ( https : //github.com/google/benchmark/blob/36a251ab3ee4e6dc668129e0ef5fa62b1b8d5d49/test/complexity_test.cc # L80-L113 ) Vanilla version outputs : `` ` Benchmark Time CPU Iterations -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - BM_Complexity_O_N/1024 441 ns 441 ns 1583329 BM_Complexity_O_N/2k 881 ns 881 ns 797104 BM_Complexity_O_N/4k 1767 ns 1767 ns 395988 BM_Complexity_O_N/8k 3691 ns 3690 ns 189567 BM_Complexity_O_N/16k 7377 ns 7376 ns 95031 BM_Complexity_O_N/32k 14813 ns 14801 ns 47507 BM_Complexity_O_N/64k 29605 ns 29602 ns 23608 BM_Complexity_O_N_BigO 0.45 N 0.45 N BM_Complexity_O_N_RMS 0 % 0 % BM_Complexity_O_N/1024 446 ns 446 ns 1587672 BM_Complexity_O_N/2k 884 ns 884 ns 792614 BM_Complexity_O_N/4k 1787 ns 1786 ns 393225 BM_Complexity_O_N/8k 3684 ns 3683 ns 189791 BM_Complexity_O_N/16k 7373 ns 7369 ns 95181 BM_Complexity_O_N/32k 14716 ns 14711 ns 47732 BM_Complexity_O_N/64k 29714 ns 29707 ns 23656 BM_Complexity_O_N_BigO 0.45 f ( N ) 0.45 f ( N ) BM_Complexity_O_N_RMS 1 % 1 % BM_Complexity_O_N/1024 442 ns 442 ns 1589204 BM_Complexity_O_N/2k 885 ns 884 ns 788984 BM_Complexity_O_N/4k 1781 ns 1781 ns 390535 BM_Complexity_O_N/8k 3691 ns
RMS calculation seems broken , takes time Unit into account ? __EoT__ Hi . Consider the [ ` BM_Complexity_O_N ` ] ( https : //github.com/google/benchmark/blob/36a251ab3ee4e6dc668129e0ef5fa62b1b8d5d49/test/complexity_test.cc # L80-L113 ) Vanilla version outputs : `` ` Benchmark Time CPU Iterations -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - BM_Complexity_O_N/1024 441 ns 441 ns 1583329 BM_Complexity_O_N/2k 881 ns 881 ns 797104 BM_Complexity_O_N/4k 1767 ns 1767 ns 395988 BM_Complexity_O_N/8k 3691 ns 3690 ns 189567 BM_Complexity_O_N/16k 7377 ns 7376 ns 95031 BM_Complexity_O_N/32k 14813 ns 14801 ns 47507 BM_Complexity_O_N/64k 29605 ns 29602 ns 23608 BM_Complexity_O_N_BigO 0.45 N 0.45 N BM_Complexity_O_N_RMS 0 % 0 % BM_Complexity_O_N/1024 446 ns 446 ns 1587672 BM_Complexity_O_N/2k 884 ns 884 ns 792614 BM_Complexity_O_N/4k 1787 ns 1786 ns 393225 BM_Complexity_O_N/8k 3684 ns 3683 ns 189791 BM_Complexity_O_N/16k 7373 ns 7369 ns 95181 BM_Complexity_O_N/32k 14716 ns 14711 ns 47732 BM_Complexity_O_N/64k 29714 ns 29707 ns 23656 BM_Complexity_O_N_BigO 0.45 f ( N ) 0.45 f ( N ) BM_Complexity_O_N_RMS 1 % 1 % BM_Complexity_O_N/1024 442 ns 442 ns 1589204 BM_Complexity_O_N/2k 885 ns 884 ns 788984 BM_Complexity_O_N/4k 1781 ns 1781 ns 390535 BM_Complexity_O_N/8k 3691 ns
RMS calculation seems broken , takes time Unit into account ? __EoT__ Hi . Consider the [ ` BM_Complexity_O_N ` ] ( https : //github.com/google/benchmark/blob/36a251ab3ee4e6dc668129e0ef5fa62b1b8d5d49/test/complexity_test.cc # L80-L113 ) Vanilla version outputs : `` ` Benchmark Time CPU Iterations -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - BM_Complexity_O_N/1024 441 ns 441 ns 1583329 BM_Complexity_O_N/2k 881 ns 881 ns 797104 BM_Complexity_O_N/4k 1767 ns 1767 ns 395988 BM_Complexity_O_N/8k 3691 ns 3690 ns 189567 BM_Complexity_O_N/16k 7377 ns 7376 ns 95031 BM_Complexity_O_N/32k 14813 ns 14801 ns 47507 BM_Complexity_O_N/64k 29605 ns 29602 ns 23608 BM_Complexity_O_N_BigO 0.45 N 0.45 N BM_Complexity_O_N_RMS 0 % 0 % BM_Complexity_O_N/1024 446 ns 446 ns 1587672 BM_Complexity_O_N/2k 884 ns 884 ns 792614 BM_Complexity_O_N/4k 1787 ns 1786 ns 393225 BM_Complexity_O_N/8k 3684 ns 3683 ns 189791 BM_Complexity_O_N/16k 7373 ns 7369 ns 95181 BM_Complexity_O_N/32k 14716 ns 14711 ns 47732 BM_Complexity_O_N/64k 29714 ns 29707 ns 23656 BM_Complexity_O_N_BigO 0.45 f ( N ) 0.45 f ( N ) BM_Complexity_O_N_RMS 1 % 1 % BM_Complexity_O_N/1024 442 ns 442 ns 1589204 BM_Complexity_O_N/2k 885 ns 884 ns 788984 BM_Complexity_O_N/4k 1781 ns 1781 ns 390535 BM_Complexity_O_N/8k 3691 ns
RMS calculation seems broken , takes time Unit into account ? __EoT__ Hi . Consider the [ ` BM_Complexity_O_N ` ] ( https : //github.com/google/benchmark/blob/36a251ab3ee4e6dc668129e0ef5fa62b1b8d5d49/test/complexity_test.cc # L80-L113 ) Vanilla version outputs : `` ` Benchmark Time CPU Iterations -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - BM_Complexity_O_N/1024 441 ns 441 ns 1583329 BM_Complexity_O_N/2k 881 ns 881 ns 797104 BM_Complexity_O_N/4k 1767 ns 1767 ns 395988 BM_Complexity_O_N/8k 3691 ns 3690 ns 189567 BM_Complexity_O_N/16k 7377 ns 7376 ns 95031 BM_Complexity_O_N/32k 14813 ns 14801 ns 47507 BM_Complexity_O_N/64k 29605 ns 29602 ns 23608 BM_Complexity_O_N_BigO 0.45 N 0.45 N BM_Complexity_O_N_RMS 0 % 0 % BM_Complexity_O_N/1024 446 ns 446 ns 1587672 BM_Complexity_O_N/2k 884 ns 884 ns 792614 BM_Complexity_O_N/4k 1787 ns 1786 ns 393225 BM_Complexity_O_N/8k 3684 ns 3683 ns 189791 BM_Complexity_O_N/16k 7373 ns 7369 ns 95181 BM_Complexity_O_N/32k 14716 ns 14711 ns 47732 BM_Complexity_O_N/64k 29714 ns 29707 ns 23656 BM_Complexity_O_N_BigO 0.45 f ( N ) 0.45 f ( N ) BM_Complexity_O_N_RMS 1 % 1 % BM_Complexity_O_N/1024 442 ns 442 ns 1589204 BM_Complexity_O_N/2k 885 ns 884 ns 788984 BM_Complexity_O_N/4k 1781 ns 1781 ns 390535 BM_Complexity_O_N/8k 3691 ns
RMS calculation seems broken , takes time Unit into account ? __EoT__ Hi . Consider the [ ` BM_Complexity_O_N ` ] ( https : //github.com/google/benchmark/blob/36a251ab3ee4e6dc668129e0ef5fa62b1b8d5d49/test/complexity_test.cc # L80-L113 ) Vanilla version outputs : `` ` Benchmark Time CPU Iterations -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - BM_Complexity_O_N/1024 441 ns 441 ns 1583329 BM_Complexity_O_N/2k 881 ns 881 ns 797104 BM_Complexity_O_N/4k 1767 ns 1767 ns 395988 BM_Complexity_O_N/8k 3691 ns 3690 ns 189567 BM_Complexity_O_N/16k 7377 ns 7376 ns 95031 BM_Complexity_O_N/32k 14813 ns 14801 ns 47507 BM_Complexity_O_N/64k 29605 ns 29602 ns 23608 BM_Complexity_O_N_BigO 0.45 N 0.45 N BM_Complexity_O_N_RMS 0 % 0 % BM_Complexity_O_N/1024 446 ns 446 ns 1587672 BM_Complexity_O_N/2k 884 ns 884 ns 792614 BM_Complexity_O_N/4k 1787 ns 1786 ns 393225 BM_Complexity_O_N/8k 3684 ns 3683 ns 189791 BM_Complexity_O_N/16k 7373 ns 7369 ns 95181 BM_Complexity_O_N/32k 14716 ns 14711 ns 47732 BM_Complexity_O_N/64k 29714 ns 29707 ns 23656 BM_Complexity_O_N_BigO 0.45 f ( N ) 0.45 f ( N ) BM_Complexity_O_N_RMS 1 % 1 % BM_Complexity_O_N/1024 442 ns 442 ns 1589204 BM_Complexity_O_N/2k 885 ns 884 ns 788984 BM_Complexity_O_N/4k 1781 ns 1781 ns 390535 BM_Complexity_O_N/8k 3691 ns
RMS calculation seems broken , takes time Unit into account ? __EoT__ Hi . Consider the [ ` BM_Complexity_O_N ` ] ( https : //github.com/google/benchmark/blob/36a251ab3ee4e6dc668129e0ef5fa62b1b8d5d49/test/complexity_test.cc # L80-L113 ) Vanilla version outputs : `` ` Benchmark Time CPU Iterations -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - BM_Complexity_O_N/1024 441 ns 441 ns 1583329 BM_Complexity_O_N/2k 881 ns 881 ns 797104 BM_Complexity_O_N/4k 1767 ns 1767 ns 395988 BM_Complexity_O_N/8k 3691 ns 3690 ns 189567 BM_Complexity_O_N/16k 7377 ns 7376 ns 95031 BM_Complexity_O_N/32k 14813 ns 14801 ns 47507 BM_Complexity_O_N/64k 29605 ns 29602 ns 23608 BM_Complexity_O_N_BigO 0.45 N 0.45 N BM_Complexity_O_N_RMS 0 % 0 % BM_Complexity_O_N/1024 446 ns 446 ns 1587672 BM_Complexity_O_N/2k 884 ns 884 ns 792614 BM_Complexity_O_N/4k 1787 ns 1786 ns 393225 BM_Complexity_O_N/8k 3684 ns 3683 ns 189791 BM_Complexity_O_N/16k 7373 ns 7369 ns 95181 BM_Complexity_O_N/32k 14716 ns 14711 ns 47732 BM_Complexity_O_N/64k 29714 ns 29707 ns 23656 BM_Complexity_O_N_BigO 0.45 f ( N ) 0.45 f ( N ) BM_Complexity_O_N_RMS 1 % 1 % BM_Complexity_O_N/1024 442 ns 442 ns 1589204 BM_Complexity_O_N/2k 885 ns 884 ns 788984 BM_Complexity_O_N/4k 1781 ns 1781 ns 390535 BM_Complexity_O_N/8k 3691 ns
RMS calculation seems broken , takes time Unit into account ? __EoT__ Hi . Consider the [ ` BM_Complexity_O_N ` ] ( https : //github.com/google/benchmark/blob/36a251ab3ee4e6dc668129e0ef5fa62b1b8d5d49/test/complexity_test.cc # L80-L113 ) Vanilla version outputs : `` ` Benchmark Time CPU Iterations -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - BM_Complexity_O_N/1024 441 ns 441 ns 1583329 BM_Complexity_O_N/2k 881 ns 881 ns 797104 BM_Complexity_O_N/4k 1767 ns 1767 ns 395988 BM_Complexity_O_N/8k 3691 ns 3690 ns 189567 BM_Complexity_O_N/16k 7377 ns 7376 ns 95031 BM_Complexity_O_N/32k 14813 ns 14801 ns 47507 BM_Complexity_O_N/64k 29605 ns 29602 ns 23608 BM_Complexity_O_N_BigO 0.45 N 0.45 N BM_Complexity_O_N_RMS 0 % 0 % BM_Complexity_O_N/1024 446 ns 446 ns 1587672 BM_Complexity_O_N/2k 884 ns 884 ns 792614 BM_Complexity_O_N/4k 1787 ns 1786 ns 393225 BM_Complexity_O_N/8k 3684 ns 3683 ns 189791 BM_Complexity_O_N/16k 7373 ns 7369 ns 95181 BM_Complexity_O_N/32k 14716 ns 14711 ns 47732 BM_Complexity_O_N/64k 29714 ns 29707 ns 23656 BM_Complexity_O_N_BigO 0.45 f ( N ) 0.45 f ( N ) BM_Complexity_O_N_RMS 1 % 1 % BM_Complexity_O_N/1024 442 ns 442 ns 1589204 BM_Complexity_O_N/2k 885 ns 884 ns 788984 BM_Complexity_O_N/4k 1781 ns 1781 ns 390535 BM_Complexity_O_N/8k 3691 ns
RMS calculation seems broken , takes time Unit into account ? __EoT__ Hi . Consider the [ ` BM_Complexity_O_N ` ] ( https : //github.com/google/benchmark/blob/36a251ab3ee4e6dc668129e0ef5fa62b1b8d5d49/test/complexity_test.cc # L80-L113 ) Vanilla version outputs : `` ` Benchmark Time CPU Iterations -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - BM_Complexity_O_N/1024 441 ns 441 ns 1583329 BM_Complexity_O_N/2k 881 ns 881 ns 797104 BM_Complexity_O_N/4k 1767 ns 1767 ns 395988 BM_Complexity_O_N/8k 3691 ns 3690 ns 189567 BM_Complexity_O_N/16k 7377 ns 7376 ns 95031 BM_Complexity_O_N/32k 14813 ns 14801 ns 47507 BM_Complexity_O_N/64k 29605 ns 29602 ns 23608 BM_Complexity_O_N_BigO 0.45 N 0.45 N BM_Complexity_O_N_RMS 0 % 0 % BM_Complexity_O_N/1024 446 ns 446 ns 1587672 BM_Complexity_O_N/2k 884 ns 884 ns 792614 BM_Complexity_O_N/4k 1787 ns 1786 ns 393225 BM_Complexity_O_N/8k 3684 ns 3683 ns 189791 BM_Complexity_O_N/16k 7373 ns 7369 ns 95181 BM_Complexity_O_N/32k 14716 ns 14711 ns 47732 BM_Complexity_O_N/64k 29714 ns 29707 ns 23656 BM_Complexity_O_N_BigO 0.45 f ( N ) 0.45 f ( N ) BM_Complexity_O_N_RMS 1 % 1 % BM_Complexity_O_N/1024 442 ns 442 ns 1589204 BM_Complexity_O_N/2k 885 ns 884 ns 788984 BM_Complexity_O_N/4k 1781 ns 1781 ns 390535 BM_Complexity_O_N/8k 3691 ns
RMS calculation seems broken , takes time Unit into account ? __EoT__ Hi . Consider the [ ` BM_Complexity_O_N ` ] ( https : //github.com/google/benchmark/blob/36a251ab3ee4e6dc668129e0ef5fa62b1b8d5d49/test/complexity_test.cc # L80-L113 ) Vanilla version outputs : `` ` Benchmark Time CPU Iterations -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - BM_Complexity_O_N/1024 441 ns 441 ns 1583329 BM_Complexity_O_N/2k 881 ns 881 ns 797104 BM_Complexity_O_N/4k 1767 ns 1767 ns 395988 BM_Complexity_O_N/8k 3691 ns 3690 ns 189567 BM_Complexity_O_N/16k 7377 ns 7376 ns 95031 BM_Complexity_O_N/32k 14813 ns 14801 ns 47507 BM_Complexity_O_N/64k 29605 ns 29602 ns 23608 BM_Complexity_O_N_BigO 0.45 N 0.45 N BM_Complexity_O_N_RMS 0 % 0 % BM_Complexity_O_N/1024 446 ns 446 ns 1587672 BM_Complexity_O_N/2k 884 ns 884 ns 792614 BM_Complexity_O_N/4k 1787 ns 1786 ns 393225 BM_Complexity_O_N/8k 3684 ns 3683 ns 189791 BM_Complexity_O_N/16k 7373 ns 7369 ns 95181 BM_Complexity_O_N/32k 14716 ns 14711 ns 47732 BM_Complexity_O_N/64k 29714 ns 29707 ns 23656 BM_Complexity_O_N_BigO 0.45 f ( N ) 0.45 f ( N ) BM_Complexity_O_N_RMS 1 % 1 % BM_Complexity_O_N/1024 442 ns 442 ns 1589204 BM_Complexity_O_N/2k 885 ns 884 ns 788984 BM_Complexity_O_N/4k 1781 ns 1781 ns 390535 BM_Complexity_O_N/8k 3691 ns
RMS calculation seems broken , takes time Unit into account ? __EoT__ Hi . Consider the [ ` BM_Complexity_O_N ` ] ( https : //github.com/google/benchmark/blob/36a251ab3ee4e6dc668129e0ef5fa62b1b8d5d49/test/complexity_test.cc # L80-L113 ) Vanilla version outputs : `` ` Benchmark Time CPU Iterations -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - BM_Complexity_O_N/1024 441 ns 441 ns 1583329 BM_Complexity_O_N/2k 881 ns 881 ns 797104 BM_Complexity_O_N/4k 1767 ns 1767 ns 395988 BM_Complexity_O_N/8k 3691 ns 3690 ns 189567 BM_Complexity_O_N/16k 7377 ns 7376 ns 95031 BM_Complexity_O_N/32k 14813 ns 14801 ns 47507 BM_Complexity_O_N/64k 29605 ns 29602 ns 23608 BM_Complexity_O_N_BigO 0.45 N 0.45 N BM_Complexity_O_N_RMS 0 % 0 % BM_Complexity_O_N/1024 446 ns 446 ns 1587672 BM_Complexity_O_N/2k 884 ns 884 ns 792614 BM_Complexity_O_N/4k 1787 ns 1786 ns 393225 BM_Complexity_O_N/8k 3684 ns 3683 ns 189791 BM_Complexity_O_N/16k 7373 ns 7369 ns 95181 BM_Complexity_O_N/32k 14716 ns 14711 ns 47732 BM_Complexity_O_N/64k 29714 ns 29707 ns 23656 BM_Complexity_O_N_BigO 0.45 f ( N ) 0.45 f ( N ) BM_Complexity_O_N_RMS 1 % 1 % BM_Complexity_O_N/1024 442 ns 442 ns 1589204 BM_Complexity_O_N/2k 885 ns 884 ns 788984 BM_Complexity_O_N/4k 1781 ns 1781 ns 390535 BM_Complexity_O_N/8k 3691 ns
RMS calculation seems broken , takes time Unit into account ? __EoT__ Hi . Consider the [ ` BM_Complexity_O_N ` ] ( https : //github.com/google/benchmark/blob/36a251ab3ee4e6dc668129e0ef5fa62b1b8d5d49/test/complexity_test.cc # L80-L113 ) Vanilla version outputs : `` ` Benchmark Time CPU Iterations -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - BM_Complexity_O_N/1024 441 ns 441 ns 1583329 BM_Complexity_O_N/2k 881 ns 881 ns 797104 BM_Complexity_O_N/4k 1767 ns 1767 ns 395988 BM_Complexity_O_N/8k 3691 ns 3690 ns 189567 BM_Complexity_O_N/16k 7377 ns 7376 ns 95031 BM_Complexity_O_N/32k 14813 ns 14801 ns 47507 BM_Complexity_O_N/64k 29605 ns 29602 ns 23608 BM_Complexity_O_N_BigO 0.45 N 0.45 N BM_Complexity_O_N_RMS 0 % 0 % BM_Complexity_O_N/1024 446 ns 446 ns 1587672 BM_Complexity_O_N/2k 884 ns 884 ns 792614 BM_Complexity_O_N/4k 1787 ns 1786 ns 393225 BM_Complexity_O_N/8k 3684 ns 3683 ns 189791 BM_Complexity_O_N/16k 7373 ns 7369 ns 95181 BM_Complexity_O_N/32k 14716 ns 14711 ns 47732 BM_Complexity_O_N/64k 29714 ns 29707 ns 23656 BM_Complexity_O_N_BigO 0.45 f ( N ) 0.45 f ( N ) BM_Complexity_O_N_RMS 1 % 1 % BM_Complexity_O_N/1024 442 ns 442 ns 1589204 BM_Complexity_O_N/2k 885 ns 884 ns 788984 BM_Complexity_O_N/4k 1781 ns 1781 ns 390535 BM_Complexity_O_N/8k 3691 ns
RMS calculation seems broken , takes time Unit into account ? __EoT__ Hi . Consider the [ ` BM_Complexity_O_N ` ] ( https : //github.com/google/benchmark/blob/36a251ab3ee4e6dc668129e0ef5fa62b1b8d5d49/test/complexity_test.cc # L80-L113 ) Vanilla version outputs : `` ` Benchmark Time CPU Iterations -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - BM_Complexity_O_N/1024 441 ns 441 ns 1583329 BM_Complexity_O_N/2k 881 ns 881 ns 797104 BM_Complexity_O_N/4k 1767 ns 1767 ns 395988 BM_Complexity_O_N/8k 3691 ns 3690 ns 189567 BM_Complexity_O_N/16k 7377 ns 7376 ns 95031 BM_Complexity_O_N/32k 14813 ns 14801 ns 47507 BM_Complexity_O_N/64k 29605 ns 29602 ns 23608 BM_Complexity_O_N_BigO 0.45 N 0.45 N BM_Complexity_O_N_RMS 0 % 0 % BM_Complexity_O_N/1024 446 ns 446 ns 1587672 BM_Complexity_O_N/2k 884 ns 884 ns 792614 BM_Complexity_O_N/4k 1787 ns 1786 ns 393225 BM_Complexity_O_N/8k 3684 ns 3683 ns 189791 BM_Complexity_O_N/16k 7373 ns 7369 ns 95181 BM_Complexity_O_N/32k 14716 ns 14711 ns 47732 BM_Complexity_O_N/64k 29714 ns 29707 ns 23656 BM_Complexity_O_N_BigO 0.45 f ( N ) 0.45 f ( N ) BM_Complexity_O_N_RMS 1 % 1 % BM_Complexity_O_N/1024 442 ns 442 ns 1589204 BM_Complexity_O_N/2k 885 ns 884 ns 788984 BM_Complexity_O_N/4k 1781 ns 1781 ns 390535 BM_Complexity_O_N/8k 3691 ns
RMS calculation seems broken , takes time Unit into account ? __EoT__ Hi . Consider the [ ` BM_Complexity_O_N ` ] ( https : //github.com/google/benchmark/blob/36a251ab3ee4e6dc668129e0ef5fa62b1b8d5d49/test/complexity_test.cc # L80-L113 ) Vanilla version outputs : `` ` Benchmark Time CPU Iterations -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - BM_Complexity_O_N/1024 441 ns 441 ns 1583329 BM_Complexity_O_N/2k 881 ns 881 ns 797104 BM_Complexity_O_N/4k 1767 ns 1767 ns 395988 BM_Complexity_O_N/8k 3691 ns 3690 ns 189567 BM_Complexity_O_N/16k 7377 ns 7376 ns 95031 BM_Complexity_O_N/32k 14813 ns 14801 ns 47507 BM_Complexity_O_N/64k 29605 ns 29602 ns 23608 BM_Complexity_O_N_BigO 0.45 N 0.45 N BM_Complexity_O_N_RMS 0 % 0 % BM_Complexity_O_N/1024 446 ns 446 ns 1587672 BM_Complexity_O_N/2k 884 ns 884 ns 792614 BM_Complexity_O_N/4k 1787 ns 1786 ns 393225 BM_Complexity_O_N/8k 3684 ns 3683 ns 189791 BM_Complexity_O_N/16k 7373 ns 7369 ns 95181 BM_Complexity_O_N/32k 14716 ns 14711 ns 47732 BM_Complexity_O_N/64k 29714 ns 29707 ns 23656 BM_Complexity_O_N_BigO 0.45 f ( N ) 0.45 f ( N ) BM_Complexity_O_N_RMS 1 % 1 % BM_Complexity_O_N/1024 442 ns 442 ns 1589204 BM_Complexity_O_N/2k 885 ns 884 ns 788984 BM_Complexity_O_N/4k 1781 ns 1781 ns 390535 BM_Complexity_O_N/8k 3691 ns
benchmark : :DoNotOptimize does not compile on gcc4.8 + ppc64 __EoT__ I 'm trying to build one of my projects on a ppc64le system , and compilation of benchmarks fails on assembly inside ` benchmark : :DoNotOptimize ` : `` ` /home/mdukhan3/NNPACK/deps/fxdiv/deps/googlebenchmark/include/benchmark/benchmark_api.h:236:45 : error : can not reload integer constant operand in ‘ asm ’ asm volatile ( `` '' : : `` g '' ( value ) : `` memory '' ) ; `` ` Results for ` g++ -v ` : `` ` mdukhan3 @ power8 : ~/NNPACK/deps/fxdiv $ g++ -v Using built-in specs . COLLECT_GCC=g++ COLLECT_LTO_WRAPPER=/usr/lib/gcc/powerpc64le-linux-gnu/4.8/lto-wrapper Target : powerpc64le-linux-gnu Configured with : ../src/configure -v -- with-pkgversion='Ubuntu 4.8.4-2ubuntu1~14.04.3 ' -- with-bugurl=file : ///usr/share/doc/gcc-4.8/README.Bugs -- enable-languages=c , c++ , java , go , d , fortran , objc , obj-c++ -- prefix=/usr -- program-suffix=-4.8 -- enable-shared -- enable-linker-build-id -- libexecdir=/usr/lib -- without-included-gettext -- enable-threads=posix -- with-gxx-include-dir=/usr/include/c++/4.8 -- libdir=/usr/lib -- enable-nls -- with-sysroot=/ -- enable-clocale=gnu -- enable-libstdcxx-debug -- enable-libstdcxx-time=yes -- enable-gnu-unique-object -- disable-libmudflap -- disable-libsanitizer -- disable-libsanitizer -- disable-libquadmath -- enable-plugin -- with-system-zlib -- disable-browser-plugin -- enable-java-awt=gtk -- enable-gtk-cairo -- with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el/jre -- enable-java-home -- with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el -- with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-ppc64el -- with-arch-directory=ppc64el -- with-ecj-jar=/usr/share/java/eclipse-ecj.jar -- enable-objc-gc -- enable-secureplt -- with-cpu=power7 -- with-tune=power8 --
benchmark : :DoNotOptimize does not compile on gcc4.8 + ppc64 __EoT__ I 'm trying to build one of my projects on a ppc64le system , and compilation of benchmarks fails on assembly inside ` benchmark : :DoNotOptimize ` : `` ` /home/mdukhan3/NNPACK/deps/fxdiv/deps/googlebenchmark/include/benchmark/benchmark_api.h:236:45 : error : can not reload integer constant operand in ‘ asm ’ asm volatile ( `` '' : : `` g '' ( value ) : `` memory '' ) ; `` ` Results for ` g++ -v ` : `` ` mdukhan3 @ power8 : ~/NNPACK/deps/fxdiv $ g++ -v Using built-in specs . COLLECT_GCC=g++ COLLECT_LTO_WRAPPER=/usr/lib/gcc/powerpc64le-linux-gnu/4.8/lto-wrapper Target : powerpc64le-linux-gnu Configured with : ../src/configure -v -- with-pkgversion='Ubuntu 4.8.4-2ubuntu1~14.04.3 ' -- with-bugurl=file : ///usr/share/doc/gcc-4.8/README.Bugs -- enable-languages=c , c++ , java , go , d , fortran , objc , obj-c++ -- prefix=/usr -- program-suffix=-4.8 -- enable-shared -- enable-linker-build-id -- libexecdir=/usr/lib -- without-included-gettext -- enable-threads=posix -- with-gxx-include-dir=/usr/include/c++/4.8 -- libdir=/usr/lib -- enable-nls -- with-sysroot=/ -- enable-clocale=gnu -- enable-libstdcxx-debug -- enable-libstdcxx-time=yes -- enable-gnu-unique-object -- disable-libmudflap -- disable-libsanitizer -- disable-libsanitizer -- disable-libquadmath -- enable-plugin -- with-system-zlib -- disable-browser-plugin -- enable-java-awt=gtk -- enable-gtk-cairo -- with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el/jre -- enable-java-home -- with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el -- with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-ppc64el -- with-arch-directory=ppc64el -- with-ecj-jar=/usr/share/java/eclipse-ecj.jar -- enable-objc-gc -- enable-secureplt -- with-cpu=power7 -- with-tune=power8 --
benchmark : :DoNotOptimize does not compile on gcc4.8 + ppc64 __EoT__ I 'm trying to build one of my projects on a ppc64le system , and compilation of benchmarks fails on assembly inside ` benchmark : :DoNotOptimize ` : `` ` /home/mdukhan3/NNPACK/deps/fxdiv/deps/googlebenchmark/include/benchmark/benchmark_api.h:236:45 : error : can not reload integer constant operand in ‘ asm ’ asm volatile ( `` '' : : `` g '' ( value ) : `` memory '' ) ; `` ` Results for ` g++ -v ` : `` ` mdukhan3 @ power8 : ~/NNPACK/deps/fxdiv $ g++ -v Using built-in specs . COLLECT_GCC=g++ COLLECT_LTO_WRAPPER=/usr/lib/gcc/powerpc64le-linux-gnu/4.8/lto-wrapper Target : powerpc64le-linux-gnu Configured with : ../src/configure -v -- with-pkgversion='Ubuntu 4.8.4-2ubuntu1~14.04.3 ' -- with-bugurl=file : ///usr/share/doc/gcc-4.8/README.Bugs -- enable-languages=c , c++ , java , go , d , fortran , objc , obj-c++ -- prefix=/usr -- program-suffix=-4.8 -- enable-shared -- enable-linker-build-id -- libexecdir=/usr/lib -- without-included-gettext -- enable-threads=posix -- with-gxx-include-dir=/usr/include/c++/4.8 -- libdir=/usr/lib -- enable-nls -- with-sysroot=/ -- enable-clocale=gnu -- enable-libstdcxx-debug -- enable-libstdcxx-time=yes -- enable-gnu-unique-object -- disable-libmudflap -- disable-libsanitizer -- disable-libsanitizer -- disable-libquadmath -- enable-plugin -- with-system-zlib -- disable-browser-plugin -- enable-java-awt=gtk -- enable-gtk-cairo -- with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el/jre -- enable-java-home -- with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el -- with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-ppc64el -- with-arch-directory=ppc64el -- with-ecj-jar=/usr/share/java/eclipse-ecj.jar -- enable-objc-gc -- enable-secureplt -- with-cpu=power7 -- with-tune=power8 --
benchmark : :DoNotOptimize does not compile on gcc4.8 + ppc64 __EoT__ I 'm trying to build one of my projects on a ppc64le system , and compilation of benchmarks fails on assembly inside ` benchmark : :DoNotOptimize ` : `` ` /home/mdukhan3/NNPACK/deps/fxdiv/deps/googlebenchmark/include/benchmark/benchmark_api.h:236:45 : error : can not reload integer constant operand in ‘ asm ’ asm volatile ( `` '' : : `` g '' ( value ) : `` memory '' ) ; `` ` Results for ` g++ -v ` : `` ` mdukhan3 @ power8 : ~/NNPACK/deps/fxdiv $ g++ -v Using built-in specs . COLLECT_GCC=g++ COLLECT_LTO_WRAPPER=/usr/lib/gcc/powerpc64le-linux-gnu/4.8/lto-wrapper Target : powerpc64le-linux-gnu Configured with : ../src/configure -v -- with-pkgversion='Ubuntu 4.8.4-2ubuntu1~14.04.3 ' -- with-bugurl=file : ///usr/share/doc/gcc-4.8/README.Bugs -- enable-languages=c , c++ , java , go , d , fortran , objc , obj-c++ -- prefix=/usr -- program-suffix=-4.8 -- enable-shared -- enable-linker-build-id -- libexecdir=/usr/lib -- without-included-gettext -- enable-threads=posix -- with-gxx-include-dir=/usr/include/c++/4.8 -- libdir=/usr/lib -- enable-nls -- with-sysroot=/ -- enable-clocale=gnu -- enable-libstdcxx-debug -- enable-libstdcxx-time=yes -- enable-gnu-unique-object -- disable-libmudflap -- disable-libsanitizer -- disable-libsanitizer -- disable-libquadmath -- enable-plugin -- with-system-zlib -- disable-browser-plugin -- enable-java-awt=gtk -- enable-gtk-cairo -- with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el/jre -- enable-java-home -- with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el -- with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-ppc64el -- with-arch-directory=ppc64el -- with-ecj-jar=/usr/share/java/eclipse-ecj.jar -- enable-objc-gc -- enable-secureplt -- with-cpu=power7 -- with-tune=power8 --
benchmark : :DoNotOptimize does not compile on gcc4.8 + ppc64 __EoT__ I 'm trying to build one of my projects on a ppc64le system , and compilation of benchmarks fails on assembly inside ` benchmark : :DoNotOptimize ` : `` ` /home/mdukhan3/NNPACK/deps/fxdiv/deps/googlebenchmark/include/benchmark/benchmark_api.h:236:45 : error : can not reload integer constant operand in ‘ asm ’ asm volatile ( `` '' : : `` g '' ( value ) : `` memory '' ) ; `` ` Results for ` g++ -v ` : `` ` mdukhan3 @ power8 : ~/NNPACK/deps/fxdiv $ g++ -v Using built-in specs . COLLECT_GCC=g++ COLLECT_LTO_WRAPPER=/usr/lib/gcc/powerpc64le-linux-gnu/4.8/lto-wrapper Target : powerpc64le-linux-gnu Configured with : ../src/configure -v -- with-pkgversion='Ubuntu 4.8.4-2ubuntu1~14.04.3 ' -- with-bugurl=file : ///usr/share/doc/gcc-4.8/README.Bugs -- enable-languages=c , c++ , java , go , d , fortran , objc , obj-c++ -- prefix=/usr -- program-suffix=-4.8 -- enable-shared -- enable-linker-build-id -- libexecdir=/usr/lib -- without-included-gettext -- enable-threads=posix -- with-gxx-include-dir=/usr/include/c++/4.8 -- libdir=/usr/lib -- enable-nls -- with-sysroot=/ -- enable-clocale=gnu -- enable-libstdcxx-debug -- enable-libstdcxx-time=yes -- enable-gnu-unique-object -- disable-libmudflap -- disable-libsanitizer -- disable-libsanitizer -- disable-libquadmath -- enable-plugin -- with-system-zlib -- disable-browser-plugin -- enable-java-awt=gtk -- enable-gtk-cairo -- with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el/jre -- enable-java-home -- with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el -- with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-ppc64el -- with-arch-directory=ppc64el -- with-ecj-jar=/usr/share/java/eclipse-ecj.jar -- enable-objc-gc -- enable-secureplt -- with-cpu=power7 -- with-tune=power8 --
benchmark : :DoNotOptimize does not compile on gcc4.8 + ppc64 __EoT__ I 'm trying to build one of my projects on a ppc64le system , and compilation of benchmarks fails on assembly inside ` benchmark : :DoNotOptimize ` : `` ` /home/mdukhan3/NNPACK/deps/fxdiv/deps/googlebenchmark/include/benchmark/benchmark_api.h:236:45 : error : can not reload integer constant operand in ‘ asm ’ asm volatile ( `` '' : : `` g '' ( value ) : `` memory '' ) ; `` ` Results for ` g++ -v ` : `` ` mdukhan3 @ power8 : ~/NNPACK/deps/fxdiv $ g++ -v Using built-in specs . COLLECT_GCC=g++ COLLECT_LTO_WRAPPER=/usr/lib/gcc/powerpc64le-linux-gnu/4.8/lto-wrapper Target : powerpc64le-linux-gnu Configured with : ../src/configure -v -- with-pkgversion='Ubuntu 4.8.4-2ubuntu1~14.04.3 ' -- with-bugurl=file : ///usr/share/doc/gcc-4.8/README.Bugs -- enable-languages=c , c++ , java , go , d , fortran , objc , obj-c++ -- prefix=/usr -- program-suffix=-4.8 -- enable-shared -- enable-linker-build-id -- libexecdir=/usr/lib -- without-included-gettext -- enable-threads=posix -- with-gxx-include-dir=/usr/include/c++/4.8 -- libdir=/usr/lib -- enable-nls -- with-sysroot=/ -- enable-clocale=gnu -- enable-libstdcxx-debug -- enable-libstdcxx-time=yes -- enable-gnu-unique-object -- disable-libmudflap -- disable-libsanitizer -- disable-libsanitizer -- disable-libquadmath -- enable-plugin -- with-system-zlib -- disable-browser-plugin -- enable-java-awt=gtk -- enable-gtk-cairo -- with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el/jre -- enable-java-home -- with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el -- with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-ppc64el -- with-arch-directory=ppc64el -- with-ecj-jar=/usr/share/java/eclipse-ecj.jar -- enable-objc-gc -- enable-secureplt -- with-cpu=power7 -- with-tune=power8 --
benchmark : :DoNotOptimize does not compile on gcc4.8 + ppc64 __EoT__ I 'm trying to build one of my projects on a ppc64le system , and compilation of benchmarks fails on assembly inside ` benchmark : :DoNotOptimize ` : `` ` /home/mdukhan3/NNPACK/deps/fxdiv/deps/googlebenchmark/include/benchmark/benchmark_api.h:236:45 : error : can not reload integer constant operand in ‘ asm ’ asm volatile ( `` '' : : `` g '' ( value ) : `` memory '' ) ; `` ` Results for ` g++ -v ` : `` ` mdukhan3 @ power8 : ~/NNPACK/deps/fxdiv $ g++ -v Using built-in specs . COLLECT_GCC=g++ COLLECT_LTO_WRAPPER=/usr/lib/gcc/powerpc64le-linux-gnu/4.8/lto-wrapper Target : powerpc64le-linux-gnu Configured with : ../src/configure -v -- with-pkgversion='Ubuntu 4.8.4-2ubuntu1~14.04.3 ' -- with-bugurl=file : ///usr/share/doc/gcc-4.8/README.Bugs -- enable-languages=c , c++ , java , go , d , fortran , objc , obj-c++ -- prefix=/usr -- program-suffix=-4.8 -- enable-shared -- enable-linker-build-id -- libexecdir=/usr/lib -- without-included-gettext -- enable-threads=posix -- with-gxx-include-dir=/usr/include/c++/4.8 -- libdir=/usr/lib -- enable-nls -- with-sysroot=/ -- enable-clocale=gnu -- enable-libstdcxx-debug -- enable-libstdcxx-time=yes -- enable-gnu-unique-object -- disable-libmudflap -- disable-libsanitizer -- disable-libsanitizer -- disable-libquadmath -- enable-plugin -- with-system-zlib -- disable-browser-plugin -- enable-java-awt=gtk -- enable-gtk-cairo -- with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el/jre -- enable-java-home -- with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el -- with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-ppc64el -- with-arch-directory=ppc64el -- with-ecj-jar=/usr/share/java/eclipse-ecj.jar -- enable-objc-gc -- enable-secureplt -- with-cpu=power7 -- with-tune=power8 --
benchmark : :DoNotOptimize does not compile on gcc4.8 + ppc64 __EoT__ I 'm trying to build one of my projects on a ppc64le system , and compilation of benchmarks fails on assembly inside ` benchmark : :DoNotOptimize ` : `` ` /home/mdukhan3/NNPACK/deps/fxdiv/deps/googlebenchmark/include/benchmark/benchmark_api.h:236:45 : error : can not reload integer constant operand in ‘ asm ’ asm volatile ( `` '' : : `` g '' ( value ) : `` memory '' ) ; `` ` Results for ` g++ -v ` : `` ` mdukhan3 @ power8 : ~/NNPACK/deps/fxdiv $ g++ -v Using built-in specs . COLLECT_GCC=g++ COLLECT_LTO_WRAPPER=/usr/lib/gcc/powerpc64le-linux-gnu/4.8/lto-wrapper Target : powerpc64le-linux-gnu Configured with : ../src/configure -v -- with-pkgversion='Ubuntu 4.8.4-2ubuntu1~14.04.3 ' -- with-bugurl=file : ///usr/share/doc/gcc-4.8/README.Bugs -- enable-languages=c , c++ , java , go , d , fortran , objc , obj-c++ -- prefix=/usr -- program-suffix=-4.8 -- enable-shared -- enable-linker-build-id -- libexecdir=/usr/lib -- without-included-gettext -- enable-threads=posix -- with-gxx-include-dir=/usr/include/c++/4.8 -- libdir=/usr/lib -- enable-nls -- with-sysroot=/ -- enable-clocale=gnu -- enable-libstdcxx-debug -- enable-libstdcxx-time=yes -- enable-gnu-unique-object -- disable-libmudflap -- disable-libsanitizer -- disable-libsanitizer -- disable-libquadmath -- enable-plugin -- with-system-zlib -- disable-browser-plugin -- enable-java-awt=gtk -- enable-gtk-cairo -- with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el/jre -- enable-java-home -- with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el -- with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-ppc64el -- with-arch-directory=ppc64el -- with-ecj-jar=/usr/share/java/eclipse-ecj.jar -- enable-objc-gc -- enable-secureplt -- with-cpu=power7 -- with-tune=power8 --
benchmark : :DoNotOptimize does not compile on gcc4.8 + ppc64 __EoT__ I 'm trying to build one of my projects on a ppc64le system , and compilation of benchmarks fails on assembly inside ` benchmark : :DoNotOptimize ` : `` ` /home/mdukhan3/NNPACK/deps/fxdiv/deps/googlebenchmark/include/benchmark/benchmark_api.h:236:45 : error : can not reload integer constant operand in ‘ asm ’ asm volatile ( `` '' : : `` g '' ( value ) : `` memory '' ) ; `` ` Results for ` g++ -v ` : `` ` mdukhan3 @ power8 : ~/NNPACK/deps/fxdiv $ g++ -v Using built-in specs . COLLECT_GCC=g++ COLLECT_LTO_WRAPPER=/usr/lib/gcc/powerpc64le-linux-gnu/4.8/lto-wrapper Target : powerpc64le-linux-gnu Configured with : ../src/configure -v -- with-pkgversion='Ubuntu 4.8.4-2ubuntu1~14.04.3 ' -- with-bugurl=file : ///usr/share/doc/gcc-4.8/README.Bugs -- enable-languages=c , c++ , java , go , d , fortran , objc , obj-c++ -- prefix=/usr -- program-suffix=-4.8 -- enable-shared -- enable-linker-build-id -- libexecdir=/usr/lib -- without-included-gettext -- enable-threads=posix -- with-gxx-include-dir=/usr/include/c++/4.8 -- libdir=/usr/lib -- enable-nls -- with-sysroot=/ -- enable-clocale=gnu -- enable-libstdcxx-debug -- enable-libstdcxx-time=yes -- enable-gnu-unique-object -- disable-libmudflap -- disable-libsanitizer -- disable-libsanitizer -- disable-libquadmath -- enable-plugin -- with-system-zlib -- disable-browser-plugin -- enable-java-awt=gtk -- enable-gtk-cairo -- with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el/jre -- enable-java-home -- with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el -- with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-ppc64el -- with-arch-directory=ppc64el -- with-ecj-jar=/usr/share/java/eclipse-ecj.jar -- enable-objc-gc -- enable-secureplt -- with-cpu=power7 -- with-tune=power8 --
benchmark : :DoNotOptimize does not compile on gcc4.8 + ppc64 __EoT__ I 'm trying to build one of my projects on a ppc64le system , and compilation of benchmarks fails on assembly inside ` benchmark : :DoNotOptimize ` : `` ` /home/mdukhan3/NNPACK/deps/fxdiv/deps/googlebenchmark/include/benchmark/benchmark_api.h:236:45 : error : can not reload integer constant operand in ‘ asm ’ asm volatile ( `` '' : : `` g '' ( value ) : `` memory '' ) ; `` ` Results for ` g++ -v ` : `` ` mdukhan3 @ power8 : ~/NNPACK/deps/fxdiv $ g++ -v Using built-in specs . COLLECT_GCC=g++ COLLECT_LTO_WRAPPER=/usr/lib/gcc/powerpc64le-linux-gnu/4.8/lto-wrapper Target : powerpc64le-linux-gnu Configured with : ../src/configure -v -- with-pkgversion='Ubuntu 4.8.4-2ubuntu1~14.04.3 ' -- with-bugurl=file : ///usr/share/doc/gcc-4.8/README.Bugs -- enable-languages=c , c++ , java , go , d , fortran , objc , obj-c++ -- prefix=/usr -- program-suffix=-4.8 -- enable-shared -- enable-linker-build-id -- libexecdir=/usr/lib -- without-included-gettext -- enable-threads=posix -- with-gxx-include-dir=/usr/include/c++/4.8 -- libdir=/usr/lib -- enable-nls -- with-sysroot=/ -- enable-clocale=gnu -- enable-libstdcxx-debug -- enable-libstdcxx-time=yes -- enable-gnu-unique-object -- disable-libmudflap -- disable-libsanitizer -- disable-libsanitizer -- disable-libquadmath -- enable-plugin -- with-system-zlib -- disable-browser-plugin -- enable-java-awt=gtk -- enable-gtk-cairo -- with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el/jre -- enable-java-home -- with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el -- with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-ppc64el -- with-arch-directory=ppc64el -- with-ecj-jar=/usr/share/java/eclipse-ecj.jar -- enable-objc-gc -- enable-secureplt -- with-cpu=power7 -- with-tune=power8 --
benchmark : :DoNotOptimize does not compile on gcc4.8 + ppc64 __EoT__ I 'm trying to build one of my projects on a ppc64le system , and compilation of benchmarks fails on assembly inside ` benchmark : :DoNotOptimize ` : `` ` /home/mdukhan3/NNPACK/deps/fxdiv/deps/googlebenchmark/include/benchmark/benchmark_api.h:236:45 : error : can not reload integer constant operand in ‘ asm ’ asm volatile ( `` '' : : `` g '' ( value ) : `` memory '' ) ; `` ` Results for ` g++ -v ` : `` ` mdukhan3 @ power8 : ~/NNPACK/deps/fxdiv $ g++ -v Using built-in specs . COLLECT_GCC=g++ COLLECT_LTO_WRAPPER=/usr/lib/gcc/powerpc64le-linux-gnu/4.8/lto-wrapper Target : powerpc64le-linux-gnu Configured with : ../src/configure -v -- with-pkgversion='Ubuntu 4.8.4-2ubuntu1~14.04.3 ' -- with-bugurl=file : ///usr/share/doc/gcc-4.8/README.Bugs -- enable-languages=c , c++ , java , go , d , fortran , objc , obj-c++ -- prefix=/usr -- program-suffix=-4.8 -- enable-shared -- enable-linker-build-id -- libexecdir=/usr/lib -- without-included-gettext -- enable-threads=posix -- with-gxx-include-dir=/usr/include/c++/4.8 -- libdir=/usr/lib -- enable-nls -- with-sysroot=/ -- enable-clocale=gnu -- enable-libstdcxx-debug -- enable-libstdcxx-time=yes -- enable-gnu-unique-object -- disable-libmudflap -- disable-libsanitizer -- disable-libsanitizer -- disable-libquadmath -- enable-plugin -- with-system-zlib -- disable-browser-plugin -- enable-java-awt=gtk -- enable-gtk-cairo -- with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el/jre -- enable-java-home -- with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el -- with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-ppc64el -- with-arch-directory=ppc64el -- with-ecj-jar=/usr/share/java/eclipse-ecj.jar -- enable-objc-gc -- enable-secureplt -- with-cpu=power7 -- with-tune=power8 --
benchmark : :DoNotOptimize does not compile on gcc4.8 + ppc64 __EoT__ I 'm trying to build one of my projects on a ppc64le system , and compilation of benchmarks fails on assembly inside ` benchmark : :DoNotOptimize ` : `` ` /home/mdukhan3/NNPACK/deps/fxdiv/deps/googlebenchmark/include/benchmark/benchmark_api.h:236:45 : error : can not reload integer constant operand in ‘ asm ’ asm volatile ( `` '' : : `` g '' ( value ) : `` memory '' ) ; `` ` Results for ` g++ -v ` : `` ` mdukhan3 @ power8 : ~/NNPACK/deps/fxdiv $ g++ -v Using built-in specs . COLLECT_GCC=g++ COLLECT_LTO_WRAPPER=/usr/lib/gcc/powerpc64le-linux-gnu/4.8/lto-wrapper Target : powerpc64le-linux-gnu Configured with : ../src/configure -v -- with-pkgversion='Ubuntu 4.8.4-2ubuntu1~14.04.3 ' -- with-bugurl=file : ///usr/share/doc/gcc-4.8/README.Bugs -- enable-languages=c , c++ , java , go , d , fortran , objc , obj-c++ -- prefix=/usr -- program-suffix=-4.8 -- enable-shared -- enable-linker-build-id -- libexecdir=/usr/lib -- without-included-gettext -- enable-threads=posix -- with-gxx-include-dir=/usr/include/c++/4.8 -- libdir=/usr/lib -- enable-nls -- with-sysroot=/ -- enable-clocale=gnu -- enable-libstdcxx-debug -- enable-libstdcxx-time=yes -- enable-gnu-unique-object -- disable-libmudflap -- disable-libsanitizer -- disable-libsanitizer -- disable-libquadmath -- enable-plugin -- with-system-zlib -- disable-browser-plugin -- enable-java-awt=gtk -- enable-gtk-cairo -- with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el/jre -- enable-java-home -- with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el -- with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-ppc64el -- with-arch-directory=ppc64el -- with-ecj-jar=/usr/share/java/eclipse-ecj.jar -- enable-objc-gc -- enable-secureplt -- with-cpu=power7 -- with-tune=power8 --
benchmark : :DoNotOptimize does not compile on gcc4.8 + ppc64 __EoT__ I 'm trying to build one of my projects on a ppc64le system , and compilation of benchmarks fails on assembly inside ` benchmark : :DoNotOptimize ` : `` ` /home/mdukhan3/NNPACK/deps/fxdiv/deps/googlebenchmark/include/benchmark/benchmark_api.h:236:45 : error : can not reload integer constant operand in ‘ asm ’ asm volatile ( `` '' : : `` g '' ( value ) : `` memory '' ) ; `` ` Results for ` g++ -v ` : `` ` mdukhan3 @ power8 : ~/NNPACK/deps/fxdiv $ g++ -v Using built-in specs . COLLECT_GCC=g++ COLLECT_LTO_WRAPPER=/usr/lib/gcc/powerpc64le-linux-gnu/4.8/lto-wrapper Target : powerpc64le-linux-gnu Configured with : ../src/configure -v -- with-pkgversion='Ubuntu 4.8.4-2ubuntu1~14.04.3 ' -- with-bugurl=file : ///usr/share/doc/gcc-4.8/README.Bugs -- enable-languages=c , c++ , java , go , d , fortran , objc , obj-c++ -- prefix=/usr -- program-suffix=-4.8 -- enable-shared -- enable-linker-build-id -- libexecdir=/usr/lib -- without-included-gettext -- enable-threads=posix -- with-gxx-include-dir=/usr/include/c++/4.8 -- libdir=/usr/lib -- enable-nls -- with-sysroot=/ -- enable-clocale=gnu -- enable-libstdcxx-debug -- enable-libstdcxx-time=yes -- enable-gnu-unique-object -- disable-libmudflap -- disable-libsanitizer -- disable-libsanitizer -- disable-libquadmath -- enable-plugin -- with-system-zlib -- disable-browser-plugin -- enable-java-awt=gtk -- enable-gtk-cairo -- with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el/jre -- enable-java-home -- with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el -- with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-ppc64el -- with-arch-directory=ppc64el -- with-ecj-jar=/usr/share/java/eclipse-ecj.jar -- enable-objc-gc -- enable-secureplt -- with-cpu=power7 -- with-tune=power8 --
benchmark : :DoNotOptimize does not compile on gcc4.8 + ppc64 __EoT__ I 'm trying to build one of my projects on a ppc64le system , and compilation of benchmarks fails on assembly inside ` benchmark : :DoNotOptimize ` : `` ` /home/mdukhan3/NNPACK/deps/fxdiv/deps/googlebenchmark/include/benchmark/benchmark_api.h:236:45 : error : can not reload integer constant operand in ‘ asm ’ asm volatile ( `` '' : : `` g '' ( value ) : `` memory '' ) ; `` ` Results for ` g++ -v ` : `` ` mdukhan3 @ power8 : ~/NNPACK/deps/fxdiv $ g++ -v Using built-in specs . COLLECT_GCC=g++ COLLECT_LTO_WRAPPER=/usr/lib/gcc/powerpc64le-linux-gnu/4.8/lto-wrapper Target : powerpc64le-linux-gnu Configured with : ../src/configure -v -- with-pkgversion='Ubuntu 4.8.4-2ubuntu1~14.04.3 ' -- with-bugurl=file : ///usr/share/doc/gcc-4.8/README.Bugs -- enable-languages=c , c++ , java , go , d , fortran , objc , obj-c++ -- prefix=/usr -- program-suffix=-4.8 -- enable-shared -- enable-linker-build-id -- libexecdir=/usr/lib -- without-included-gettext -- enable-threads=posix -- with-gxx-include-dir=/usr/include/c++/4.8 -- libdir=/usr/lib -- enable-nls -- with-sysroot=/ -- enable-clocale=gnu -- enable-libstdcxx-debug -- enable-libstdcxx-time=yes -- enable-gnu-unique-object -- disable-libmudflap -- disable-libsanitizer -- disable-libsanitizer -- disable-libquadmath -- enable-plugin -- with-system-zlib -- disable-browser-plugin -- enable-java-awt=gtk -- enable-gtk-cairo -- with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el/jre -- enable-java-home -- with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el -- with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-ppc64el -- with-arch-directory=ppc64el -- with-ecj-jar=/usr/share/java/eclipse-ecj.jar -- enable-objc-gc -- enable-secureplt -- with-cpu=power7 -- with-tune=power8 --
benchmark : :DoNotOptimize does not compile on gcc4.8 + ppc64 __EoT__ I 'm trying to build one of my projects on a ppc64le system , and compilation of benchmarks fails on assembly inside ` benchmark : :DoNotOptimize ` : `` ` /home/mdukhan3/NNPACK/deps/fxdiv/deps/googlebenchmark/include/benchmark/benchmark_api.h:236:45 : error : can not reload integer constant operand in ‘ asm ’ asm volatile ( `` '' : : `` g '' ( value ) : `` memory '' ) ; `` ` Results for ` g++ -v ` : `` ` mdukhan3 @ power8 : ~/NNPACK/deps/fxdiv $ g++ -v Using built-in specs . COLLECT_GCC=g++ COLLECT_LTO_WRAPPER=/usr/lib/gcc/powerpc64le-linux-gnu/4.8/lto-wrapper Target : powerpc64le-linux-gnu Configured with : ../src/configure -v -- with-pkgversion='Ubuntu 4.8.4-2ubuntu1~14.04.3 ' -- with-bugurl=file : ///usr/share/doc/gcc-4.8/README.Bugs -- enable-languages=c , c++ , java , go , d , fortran , objc , obj-c++ -- prefix=/usr -- program-suffix=-4.8 -- enable-shared -- enable-linker-build-id -- libexecdir=/usr/lib -- without-included-gettext -- enable-threads=posix -- with-gxx-include-dir=/usr/include/c++/4.8 -- libdir=/usr/lib -- enable-nls -- with-sysroot=/ -- enable-clocale=gnu -- enable-libstdcxx-debug -- enable-libstdcxx-time=yes -- enable-gnu-unique-object -- disable-libmudflap -- disable-libsanitizer -- disable-libsanitizer -- disable-libquadmath -- enable-plugin -- with-system-zlib -- disable-browser-plugin -- enable-java-awt=gtk -- enable-gtk-cairo -- with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el/jre -- enable-java-home -- with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el -- with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-ppc64el -- with-arch-directory=ppc64el -- with-ecj-jar=/usr/share/java/eclipse-ecj.jar -- enable-objc-gc -- enable-secureplt -- with-cpu=power7 -- with-tune=power8 --
Documentation of # 266 was n't added to Readme.md __EoT__ # 266 added a ./compare_bench.py command that is n't documented in the documentation . Maybe one could add a variation of [ this message ] ( https : //github.com/google/benchmark/pull/266 # issue-169026465 ) in the doc . /cc @ EricWF
` state.PauseTiming ( ) ` and state.ResumeTiming ( ) take a long time __EoT__ I 'm trying to benchmark a custom map type . I basically want to see how long it takes to look up random keys . To do so , I 'm generating random keys in the ` state.KeepRunning ( ) ` loop . I obviously do n't want the random key generation to be added to the overall time of the benchmark . To prevent this , I tried to use ` state.PauseTiming ( ) ` and ` state.ResumeTiming ( ) ` , but I found using those functions added 100s of nanoseconds to my benchmark -- which is much more than the map lookup itself . Is this a problem with these functions ? Or should I be generating my keys a different way ?
Google Benchmark should provide scripts to compare/process benchmark results . __EoT__ Currently the library makes it super easy to write and run benchmarks on code , but that 's only half the point of benchmarking code . Once you have the results you usually want to compare them to previous or different results . I propose that Google Benchmark should add scripts that allow the output of benchmarks to be easily compared and more . I imagine the usage to look something like : `` ` ./bench_before.out -- benchmark_format=json > /tmp/out1.json ./bench_after.out -- benchmark_format=json > /tmp/out2.json ./benchmark/benchmark_diff.py /tmp/out1.json /tmp/out2.json `` ` And the output would be something like `` ` BM_foo/1024 0.10 # bench_after is 10 % faster than bench_before BM_bar/5012 -5.0 # bench_before is 5x faster than bench_after `` ` This is just the basic idea of what I would like the library to provide . Additional utilities to process benchmark output would be great as well , including : - Graphing multiple runs over time . - Utilities for reporting performance regressions . I think it would make sense to write these utilities in python , but I 'm open to suggestions .
Report/error unrecognized command line flags __EoT__ Are there any use cases where additional flags that are not recognized by ` ParseCommandLineFlags ( ) ` are handled from another context ? I would prefer if , by default , benchmark would report unrecognized command line flags . Something like `` ` cpp // benchmark.cc : ParseCommandLineFlags ( ) if ( ... ) { // ... } else if ( IsFlag ( argv [ i ] , `` help '' ) ) { PrintUsageAndExit ( ) ; } else { fprintf ( stdout , `` unrecognized command line flag : % s\n '' , argv [ i ] ) ; exit ( 1 ) ; } `` `
Report/error unrecognized command line flags __EoT__ Are there any use cases where additional flags that are not recognized by ` ParseCommandLineFlags ( ) ` are handled from another context ? I would prefer if , by default , benchmark would report unrecognized command line flags . Something like `` ` cpp // benchmark.cc : ParseCommandLineFlags ( ) if ( ... ) { // ... } else if ( IsFlag ( argv [ i ] , `` help '' ) ) { PrintUsageAndExit ( ) ; } else { fprintf ( stdout , `` unrecognized command line flag : % s\n '' , argv [ i ] ) ; exit ( 1 ) ; } `` `
Report/error unrecognized command line flags __EoT__ Are there any use cases where additional flags that are not recognized by ` ParseCommandLineFlags ( ) ` are handled from another context ? I would prefer if , by default , benchmark would report unrecognized command line flags . Something like `` ` cpp // benchmark.cc : ParseCommandLineFlags ( ) if ( ... ) { // ... } else if ( IsFlag ( argv [ i ] , `` help '' ) ) { PrintUsageAndExit ( ) ; } else { fprintf ( stdout , `` unrecognized command line flag : % s\n '' , argv [ i ] ) ; exit ( 1 ) ; } `` `
Report/error unrecognized command line flags __EoT__ Are there any use cases where additional flags that are not recognized by ` ParseCommandLineFlags ( ) ` are handled from another context ? I would prefer if , by default , benchmark would report unrecognized command line flags . Something like `` ` cpp // benchmark.cc : ParseCommandLineFlags ( ) if ( ... ) { // ... } else if ( IsFlag ( argv [ i ] , `` help '' ) ) { PrintUsageAndExit ( ) ; } else { fprintf ( stdout , `` unrecognized command line flag : % s\n '' , argv [ i ] ) ; exit ( 1 ) ; } `` `
Report/error unrecognized command line flags __EoT__ Are there any use cases where additional flags that are not recognized by ` ParseCommandLineFlags ( ) ` are handled from another context ? I would prefer if , by default , benchmark would report unrecognized command line flags . Something like `` ` cpp // benchmark.cc : ParseCommandLineFlags ( ) if ( ... ) { // ... } else if ( IsFlag ( argv [ i ] , `` help '' ) ) { PrintUsageAndExit ( ) ; } else { fprintf ( stdout , `` unrecognized command line flag : % s\n '' , argv [ i ] ) ; exit ( 1 ) ; } `` `
Report/error unrecognized command line flags __EoT__ Are there any use cases where additional flags that are not recognized by ` ParseCommandLineFlags ( ) ` are handled from another context ? I would prefer if , by default , benchmark would report unrecognized command line flags . Something like `` ` cpp // benchmark.cc : ParseCommandLineFlags ( ) if ( ... ) { // ... } else if ( IsFlag ( argv [ i ] , `` help '' ) ) { PrintUsageAndExit ( ) ; } else { fprintf ( stdout , `` unrecognized command line flag : % s\n '' , argv [ i ] ) ; exit ( 1 ) ; } `` `
Report/error unrecognized command line flags __EoT__ Are there any use cases where additional flags that are not recognized by ` ParseCommandLineFlags ( ) ` are handled from another context ? I would prefer if , by default , benchmark would report unrecognized command line flags . Something like `` ` cpp // benchmark.cc : ParseCommandLineFlags ( ) if ( ... ) { // ... } else if ( IsFlag ( argv [ i ] , `` help '' ) ) { PrintUsageAndExit ( ) ; } else { fprintf ( stdout , `` unrecognized command line flag : % s\n '' , argv [ i ] ) ; exit ( 1 ) ; } `` `
Report/error unrecognized command line flags __EoT__ Are there any use cases where additional flags that are not recognized by ` ParseCommandLineFlags ( ) ` are handled from another context ? I would prefer if , by default , benchmark would report unrecognized command line flags . Something like `` ` cpp // benchmark.cc : ParseCommandLineFlags ( ) if ( ... ) { // ... } else if ( IsFlag ( argv [ i ] , `` help '' ) ) { PrintUsageAndExit ( ) ; } else { fprintf ( stdout , `` unrecognized command line flag : % s\n '' , argv [ i ] ) ; exit ( 1 ) ; } `` `
Report/error unrecognized command line flags __EoT__ Are there any use cases where additional flags that are not recognized by ` ParseCommandLineFlags ( ) ` are handled from another context ? I would prefer if , by default , benchmark would report unrecognized command line flags . Something like `` ` cpp // benchmark.cc : ParseCommandLineFlags ( ) if ( ... ) { // ... } else if ( IsFlag ( argv [ i ] , `` help '' ) ) { PrintUsageAndExit ( ) ; } else { fprintf ( stdout , `` unrecognized command line flag : % s\n '' , argv [ i ] ) ; exit ( 1 ) ; } `` `
Report/error unrecognized command line flags __EoT__ Are there any use cases where additional flags that are not recognized by ` ParseCommandLineFlags ( ) ` are handled from another context ? I would prefer if , by default , benchmark would report unrecognized command line flags . Something like `` ` cpp // benchmark.cc : ParseCommandLineFlags ( ) if ( ... ) { // ... } else if ( IsFlag ( argv [ i ] , `` help '' ) ) { PrintUsageAndExit ( ) ; } else { fprintf ( stdout , `` unrecognized command line flag : % s\n '' , argv [ i ] ) ; exit ( 1 ) ; } `` `
Report/error unrecognized command line flags __EoT__ Are there any use cases where additional flags that are not recognized by ` ParseCommandLineFlags ( ) ` are handled from another context ? I would prefer if , by default , benchmark would report unrecognized command line flags . Something like `` ` cpp // benchmark.cc : ParseCommandLineFlags ( ) if ( ... ) { // ... } else if ( IsFlag ( argv [ i ] , `` help '' ) ) { PrintUsageAndExit ( ) ; } else { fprintf ( stdout , `` unrecognized command line flag : % s\n '' , argv [ i ] ) ; exit ( 1 ) ; } `` `
Template arguments range __EoT__ Hello , What do you think about adding a mechanism to iterate over range of template arguments passed to ` TEMPLATE_BENCHMARK ` ? Such thing could be implemented using loop template or Boost.Preprocessor ( I guess we do n't want to have any dependency on that ) . I 've added some insight on the topic on my blog post : http : //dominikczarnota.blogspot.com/2015/12/how-i-saved-myself-lot-of-writing-with.html
Parsing CPU info on Linux reads only cpuinfo file __EoT__ There is a bug in a file ` sysinfo.cc ` in a function ` ReadIntFromFile ` which is used to read CPU frequency from files ` /sys/devices/system/cpu/cpu0/tsc_freq_khz ` and ` /sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_max_freq ` . Here is the code of the function : `` ` C // Helper function for reading an int from a file . Returns true if successful // and the memory location pointed to by value is set to the value read . bool ReadIntFromFile ( const char* file , long* value ) { bool ret = false ; int fd = open ( file , O_RDONLY ) ; if ( fd ! = -1 ) { char line [ 1024 ] ; char* err ; memset ( line , '\0 ' , sizeof ( line ) ) ; CHECK ( read ( fd , line , sizeof ( line ) - 1 ) ) ; const long temp_value = strtol ( line , & err , 10 ) ; if ( line [ 0 ] ! = '\0 ' & & ( *err == '\n ' || *err == '\0 ' ) ) { *value = temp_value
Parsing CPU info on Linux reads only cpuinfo file __EoT__ There is a bug in a file ` sysinfo.cc ` in a function ` ReadIntFromFile ` which is used to read CPU frequency from files ` /sys/devices/system/cpu/cpu0/tsc_freq_khz ` and ` /sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_max_freq ` . Here is the code of the function : `` ` C // Helper function for reading an int from a file . Returns true if successful // and the memory location pointed to by value is set to the value read . bool ReadIntFromFile ( const char* file , long* value ) { bool ret = false ; int fd = open ( file , O_RDONLY ) ; if ( fd ! = -1 ) { char line [ 1024 ] ; char* err ; memset ( line , '\0 ' , sizeof ( line ) ) ; CHECK ( read ( fd , line , sizeof ( line ) - 1 ) ) ; const long temp_value = strtol ( line , & err , 10 ) ; if ( line [ 0 ] ! = '\0 ' & & ( *err == '\n ' || *err == '\0 ' ) ) { *value = temp_value
Parsing CPU info on Linux reads only cpuinfo file __EoT__ There is a bug in a file ` sysinfo.cc ` in a function ` ReadIntFromFile ` which is used to read CPU frequency from files ` /sys/devices/system/cpu/cpu0/tsc_freq_khz ` and ` /sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_max_freq ` . Here is the code of the function : `` ` C // Helper function for reading an int from a file . Returns true if successful // and the memory location pointed to by value is set to the value read . bool ReadIntFromFile ( const char* file , long* value ) { bool ret = false ; int fd = open ( file , O_RDONLY ) ; if ( fd ! = -1 ) { char line [ 1024 ] ; char* err ; memset ( line , '\0 ' , sizeof ( line ) ) ; CHECK ( read ( fd , line , sizeof ( line ) - 1 ) ) ; const long temp_value = strtol ( line , & err , 10 ) ; if ( line [ 0 ] ! = '\0 ' & & ( *err == '\n ' || *err == '\0 ' ) ) { *value = temp_value
Parsing CPU info on Linux reads only cpuinfo file __EoT__ There is a bug in a file ` sysinfo.cc ` in a function ` ReadIntFromFile ` which is used to read CPU frequency from files ` /sys/devices/system/cpu/cpu0/tsc_freq_khz ` and ` /sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_max_freq ` . Here is the code of the function : `` ` C // Helper function for reading an int from a file . Returns true if successful // and the memory location pointed to by value is set to the value read . bool ReadIntFromFile ( const char* file , long* value ) { bool ret = false ; int fd = open ( file , O_RDONLY ) ; if ( fd ! = -1 ) { char line [ 1024 ] ; char* err ; memset ( line , '\0 ' , sizeof ( line ) ) ; CHECK ( read ( fd , line , sizeof ( line ) - 1 ) ) ; const long temp_value = strtol ( line , & err , 10 ) ; if ( line [ 0 ] ! = '\0 ' & & ( *err == '\n ' || *err == '\0 ' ) ) { *value = temp_value
Parsing CPU info on Linux reads only cpuinfo file __EoT__ There is a bug in a file ` sysinfo.cc ` in a function ` ReadIntFromFile ` which is used to read CPU frequency from files ` /sys/devices/system/cpu/cpu0/tsc_freq_khz ` and ` /sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_max_freq ` . Here is the code of the function : `` ` C // Helper function for reading an int from a file . Returns true if successful // and the memory location pointed to by value is set to the value read . bool ReadIntFromFile ( const char* file , long* value ) { bool ret = false ; int fd = open ( file , O_RDONLY ) ; if ( fd ! = -1 ) { char line [ 1024 ] ; char* err ; memset ( line , '\0 ' , sizeof ( line ) ) ; CHECK ( read ( fd , line , sizeof ( line ) - 1 ) ) ; const long temp_value = strtol ( line , & err , 10 ) ; if ( line [ 0 ] ! = '\0 ' & & ( *err == '\n ' || *err == '\0 ' ) ) { *value = temp_value
Parsing CPU info on Linux reads only cpuinfo file __EoT__ There is a bug in a file ` sysinfo.cc ` in a function ` ReadIntFromFile ` which is used to read CPU frequency from files ` /sys/devices/system/cpu/cpu0/tsc_freq_khz ` and ` /sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_max_freq ` . Here is the code of the function : `` ` C // Helper function for reading an int from a file . Returns true if successful // and the memory location pointed to by value is set to the value read . bool ReadIntFromFile ( const char* file , long* value ) { bool ret = false ; int fd = open ( file , O_RDONLY ) ; if ( fd ! = -1 ) { char line [ 1024 ] ; char* err ; memset ( line , '\0 ' , sizeof ( line ) ) ; CHECK ( read ( fd , line , sizeof ( line ) - 1 ) ) ; const long temp_value = strtol ( line , & err , 10 ) ; if ( line [ 0 ] ! = '\0 ' & & ( *err == '\n ' || *err == '\0 ' ) ) { *value = temp_value
Parsing CPU info on Linux reads only cpuinfo file __EoT__ There is a bug in a file ` sysinfo.cc ` in a function ` ReadIntFromFile ` which is used to read CPU frequency from files ` /sys/devices/system/cpu/cpu0/tsc_freq_khz ` and ` /sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_max_freq ` . Here is the code of the function : `` ` C // Helper function for reading an int from a file . Returns true if successful // and the memory location pointed to by value is set to the value read . bool ReadIntFromFile ( const char* file , long* value ) { bool ret = false ; int fd = open ( file , O_RDONLY ) ; if ( fd ! = -1 ) { char line [ 1024 ] ; char* err ; memset ( line , '\0 ' , sizeof ( line ) ) ; CHECK ( read ( fd , line , sizeof ( line ) - 1 ) ) ; const long temp_value = strtol ( line , & err , 10 ) ; if ( line [ 0 ] ! = '\0 ' & & ( *err == '\n ' || *err == '\0 ' ) ) { *value = temp_value
Parsing CPU info on Linux reads only cpuinfo file __EoT__ There is a bug in a file ` sysinfo.cc ` in a function ` ReadIntFromFile ` which is used to read CPU frequency from files ` /sys/devices/system/cpu/cpu0/tsc_freq_khz ` and ` /sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_max_freq ` . Here is the code of the function : `` ` C // Helper function for reading an int from a file . Returns true if successful // and the memory location pointed to by value is set to the value read . bool ReadIntFromFile ( const char* file , long* value ) { bool ret = false ; int fd = open ( file , O_RDONLY ) ; if ( fd ! = -1 ) { char line [ 1024 ] ; char* err ; memset ( line , '\0 ' , sizeof ( line ) ) ; CHECK ( read ( fd , line , sizeof ( line ) - 1 ) ) ; const long temp_value = strtol ( line , & err , 10 ) ; if ( line [ 0 ] ! = '\0 ' & & ( *err == '\n ' || *err == '\0 ' ) ) { *value = temp_value
Parsing CPU info on Linux reads only cpuinfo file __EoT__ There is a bug in a file ` sysinfo.cc ` in a function ` ReadIntFromFile ` which is used to read CPU frequency from files ` /sys/devices/system/cpu/cpu0/tsc_freq_khz ` and ` /sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_max_freq ` . Here is the code of the function : `` ` C // Helper function for reading an int from a file . Returns true if successful // and the memory location pointed to by value is set to the value read . bool ReadIntFromFile ( const char* file , long* value ) { bool ret = false ; int fd = open ( file , O_RDONLY ) ; if ( fd ! = -1 ) { char line [ 1024 ] ; char* err ; memset ( line , '\0 ' , sizeof ( line ) ) ; CHECK ( read ( fd , line , sizeof ( line ) - 1 ) ) ; const long temp_value = strtol ( line , & err , 10 ) ; if ( line [ 0 ] ! = '\0 ' & & ( *err == '\n ' || *err == '\0 ' ) ) { *value = temp_value
Parsing CPU info on Linux reads only cpuinfo file __EoT__ There is a bug in a file ` sysinfo.cc ` in a function ` ReadIntFromFile ` which is used to read CPU frequency from files ` /sys/devices/system/cpu/cpu0/tsc_freq_khz ` and ` /sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_max_freq ` . Here is the code of the function : `` ` C // Helper function for reading an int from a file . Returns true if successful // and the memory location pointed to by value is set to the value read . bool ReadIntFromFile ( const char* file , long* value ) { bool ret = false ; int fd = open ( file , O_RDONLY ) ; if ( fd ! = -1 ) { char line [ 1024 ] ; char* err ; memset ( line , '\0 ' , sizeof ( line ) ) ; CHECK ( read ( fd , line , sizeof ( line ) - 1 ) ) ; const long temp_value = strtol ( line , & err , 10 ) ; if ( line [ 0 ] ! = '\0 ' & & ( *err == '\n ' || *err == '\0 ' ) ) { *value = temp_value
Parsing CPU info on Linux reads only cpuinfo file __EoT__ There is a bug in a file ` sysinfo.cc ` in a function ` ReadIntFromFile ` which is used to read CPU frequency from files ` /sys/devices/system/cpu/cpu0/tsc_freq_khz ` and ` /sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_max_freq ` . Here is the code of the function : `` ` C // Helper function for reading an int from a file . Returns true if successful // and the memory location pointed to by value is set to the value read . bool ReadIntFromFile ( const char* file , long* value ) { bool ret = false ; int fd = open ( file , O_RDONLY ) ; if ( fd ! = -1 ) { char line [ 1024 ] ; char* err ; memset ( line , '\0 ' , sizeof ( line ) ) ; CHECK ( read ( fd , line , sizeof ( line ) - 1 ) ) ; const long temp_value = strtol ( line , & err , 10 ) ; if ( line [ 0 ] ! = '\0 ' & & ( *err == '\n ' || *err == '\0 ' ) ) { *value = temp_value
Parsing CPU info on Linux reads only cpuinfo file __EoT__ There is a bug in a file ` sysinfo.cc ` in a function ` ReadIntFromFile ` which is used to read CPU frequency from files ` /sys/devices/system/cpu/cpu0/tsc_freq_khz ` and ` /sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_max_freq ` . Here is the code of the function : `` ` C // Helper function for reading an int from a file . Returns true if successful // and the memory location pointed to by value is set to the value read . bool ReadIntFromFile ( const char* file , long* value ) { bool ret = false ; int fd = open ( file , O_RDONLY ) ; if ( fd ! = -1 ) { char line [ 1024 ] ; char* err ; memset ( line , '\0 ' , sizeof ( line ) ) ; CHECK ( read ( fd , line , sizeof ( line ) - 1 ) ) ; const long temp_value = strtol ( line , & err , 10 ) ; if ( line [ 0 ] ! = '\0 ' & & ( *err == '\n ' || *err == '\0 ' ) ) { *value = temp_value
Parsing CPU info on Linux reads only cpuinfo file __EoT__ There is a bug in a file ` sysinfo.cc ` in a function ` ReadIntFromFile ` which is used to read CPU frequency from files ` /sys/devices/system/cpu/cpu0/tsc_freq_khz ` and ` /sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_max_freq ` . Here is the code of the function : `` ` C // Helper function for reading an int from a file . Returns true if successful // and the memory location pointed to by value is set to the value read . bool ReadIntFromFile ( const char* file , long* value ) { bool ret = false ; int fd = open ( file , O_RDONLY ) ; if ( fd ! = -1 ) { char line [ 1024 ] ; char* err ; memset ( line , '\0 ' , sizeof ( line ) ) ; CHECK ( read ( fd , line , sizeof ( line ) - 1 ) ) ; const long temp_value = strtol ( line , & err , 10 ) ; if ( line [ 0 ] ! = '\0 ' & & ( *err == '\n ' || *err == '\0 ' ) ) { *value = temp_value
Parsing CPU info on Linux reads only cpuinfo file __EoT__ There is a bug in a file ` sysinfo.cc ` in a function ` ReadIntFromFile ` which is used to read CPU frequency from files ` /sys/devices/system/cpu/cpu0/tsc_freq_khz ` and ` /sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_max_freq ` . Here is the code of the function : `` ` C // Helper function for reading an int from a file . Returns true if successful // and the memory location pointed to by value is set to the value read . bool ReadIntFromFile ( const char* file , long* value ) { bool ret = false ; int fd = open ( file , O_RDONLY ) ; if ( fd ! = -1 ) { char line [ 1024 ] ; char* err ; memset ( line , '\0 ' , sizeof ( line ) ) ; CHECK ( read ( fd , line , sizeof ( line ) - 1 ) ) ; const long temp_value = strtol ( line , & err , 10 ) ; if ( line [ 0 ] ! = '\0 ' & & ( *err == '\n ' || *err == '\0 ' ) ) { *value = temp_value
Parsing CPU info on Linux reads only cpuinfo file __EoT__ There is a bug in a file ` sysinfo.cc ` in a function ` ReadIntFromFile ` which is used to read CPU frequency from files ` /sys/devices/system/cpu/cpu0/tsc_freq_khz ` and ` /sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_max_freq ` . Here is the code of the function : `` ` C // Helper function for reading an int from a file . Returns true if successful // and the memory location pointed to by value is set to the value read . bool ReadIntFromFile ( const char* file , long* value ) { bool ret = false ; int fd = open ( file , O_RDONLY ) ; if ( fd ! = -1 ) { char line [ 1024 ] ; char* err ; memset ( line , '\0 ' , sizeof ( line ) ) ; CHECK ( read ( fd , line , sizeof ( line ) - 1 ) ) ; const long temp_value = strtol ( line , & err , 10 ) ; if ( line [ 0 ] ! = '\0 ' & & ( *err == '\n ' || *err == '\0 ' ) ) { *value = temp_value
benchmark.h ( 645 ) : warning C4141 : 'inline ' : used more than once __EoT__ I am compiling release 1.2.0 with Visual Studio 2017 v15.4.0 . I have these compiler warnings : 1 > include\benchmark/benchmark.h ( 645 ) : warning C4141 : 'inline ' : used more than once 1 > include\benchmark/benchmark.h ( 648 ) : warning C4141 : 'inline ' : used more than once Corresponding code : BENCHMARK_ALWAYS_INLINE inline State : :StateIterator State : :begin ( ) { return StateIterator ( this ) ; } BENCHMARK_ALWAYS_INLINE inline State : :StateIterator State : :end ( ) { StartKeepRunning ( ) ; return StateIterator ( ) ; }
internal : remove vgo_docker image __EoT__ Now that Go 1.11 is out with Go Modules , the vgo Docker image I created for the [ GCP integration tests ] ( https : //github.com/google/go-cloud/blob/64759c9ac3d4068c1e322814b088ba08a8ee360f/tests/gcp/test.sh # L52-L57 ) is no longer needed , and we can just use the Go 1.11 Docker image .
all : run Windows CI __EoT__ It would be good to run the tests on Windows to shake out any development issues with using this library with Windows . [ Appveyor ] ( https : //www.appveyor.com/ ) seems to be the preferred way to do this .
contributebot : no longer auto-assign reviewers __EoT__ I just made a new PR ( https : //github.com/google/go-cloud/pull/877 ) , wanting to first look at the change from a reviewers perspective . By the time I opened it up , @ jba had already put review comments on it about things I already knew I needed to change . I do n't want to waste my reviewers ' time , but I also find it valuable to see how my change will look to them before sending it off .
contributebot : no longer auto-assign reviewers __EoT__ I just made a new PR ( https : //github.com/google/go-cloud/pull/877 ) , wanting to first look at the change from a reviewers perspective . By the time I opened it up , @ jba had already put review comments on it about things I already knew I needed to change . I do n't want to waste my reviewers ' time , but I also find it valuable to see how my change will look to them before sending it off .
contributebot : no longer auto-assign reviewers __EoT__ I just made a new PR ( https : //github.com/google/go-cloud/pull/877 ) , wanting to first look at the change from a reviewers perspective . By the time I opened it up , @ jba had already put review comments on it about things I already knew I needed to change . I do n't want to waste my reviewers ' time , but I also find it valuable to see how my change will look to them before sending it off .
contributebot : no longer auto-assign reviewers __EoT__ I just made a new PR ( https : //github.com/google/go-cloud/pull/877 ) , wanting to first look at the change from a reviewers perspective . By the time I opened it up , @ jba had already put review comments on it about things I already knew I needed to change . I do n't want to waste my reviewers ' time , but I also find it valuable to see how my change will look to them before sending it off .
blob/test : improve comments and tests for List __EoT__ These should wait until # 541 is done . - Add a comment about guarantees for blob.Bucket.Next consistency ( i.e. , everything in UTF-8 lexicographical order ) ( this might already be there ) . - Add a comment to driver.Bucket.ListPaged about consistency requirements ( i.e. , everything in UTF-8 lexicographical order , including across pages ) . - Add tests with a Unicode filename , a filename with `` / '' , a filename with `` \ '' . - Add a test that verifies no `` directory '' is returned after the last file `` in '' it is deleted . - Add a test that verifies consistency across ListPaged after insert & delete . E.g. , `` a b c d e f '' , fetch Page of size 3 = `` a b c '' , then [ insert after `` b '' | delete `` b '' ] , should get `` d e f '' in next Page .
blob/test : improve comments and tests for List __EoT__ These should wait until # 541 is done . - Add a comment about guarantees for blob.Bucket.Next consistency ( i.e. , everything in UTF-8 lexicographical order ) ( this might already be there ) . - Add a comment to driver.Bucket.ListPaged about consistency requirements ( i.e. , everything in UTF-8 lexicographical order , including across pages ) . - Add tests with a Unicode filename , a filename with `` / '' , a filename with `` \ '' . - Add a test that verifies no `` directory '' is returned after the last file `` in '' it is deleted . - Add a test that verifies consistency across ListPaged after insert & delete . E.g. , `` a b c d e f '' , fetch Page of size 3 = `` a b c '' , then [ insert after `` b '' | delete `` b '' ] , should get `` d e f '' in next Page .
blob/test : improve comments and tests for List __EoT__ These should wait until # 541 is done . - Add a comment about guarantees for blob.Bucket.Next consistency ( i.e. , everything in UTF-8 lexicographical order ) ( this might already be there ) . - Add a comment to driver.Bucket.ListPaged about consistency requirements ( i.e. , everything in UTF-8 lexicographical order , including across pages ) . - Add tests with a Unicode filename , a filename with `` / '' , a filename with `` \ '' . - Add a test that verifies no `` directory '' is returned after the last file `` in '' it is deleted . - Add a test that verifies consistency across ListPaged after insert & delete . E.g. , `` a b c d e f '' , fetch Page of size 3 = `` a b c '' , then [ insert after `` b '' | delete `` b '' ] , should get `` d e f '' in next Page .
blob/test : improve comments and tests for List __EoT__ These should wait until # 541 is done . - Add a comment about guarantees for blob.Bucket.Next consistency ( i.e. , everything in UTF-8 lexicographical order ) ( this might already be there ) . - Add a comment to driver.Bucket.ListPaged about consistency requirements ( i.e. , everything in UTF-8 lexicographical order , including across pages ) . - Add tests with a Unicode filename , a filename with `` / '' , a filename with `` \ '' . - Add a test that verifies no `` directory '' is returned after the last file `` in '' it is deleted . - Add a test that verifies consistency across ListPaged after insert & delete . E.g. , `` a b c d e f '' , fetch Page of size 3 = `` a b c '' , then [ insert after `` b '' | delete `` b '' ] , should get `` d e f '' in next Page .
blob/driver : provide a conformance test suite for implementations __EoT__ For each API , a cloud provider implements a Driver interface in order to support that API for their cloud . For each Driver type , we should create a testcase that takes a Driver as input , and runs it through its paces . If the Driver passes the tests , then we guarantee it will work with the go-cloud API implementation .
blob/driver : provide a conformance test suite for implementations __EoT__ For each API , a cloud provider implements a Driver interface in order to support that API for their cloud . For each Driver type , we should create a testcase that takes a Driver as input , and runs it through its paces . If the Driver passes the tests , then we guarantee it will work with the go-cloud API implementation .
wire : use new Wire repository __EoT__ Wire is now a separate repository and Go module : https : //github.com/google/wire We should remove ` wire/internal/ ` and ` wire/cmd/ ` , and leave ` README.md ` and ` wire.go ` as pointers over to the new repository . We 'll keep the placeholders for a while so that existing workflows wo n't break .
samples/guestbook : terraform apply fails on AWS __EoT__ Here is the error : `` ` aws/provision-db.sh : Connecting to database ... docker : Error response from daemon : Mounts denied : The path /var/folders/_h/ssstsdnj2998kj4bsmlxm17w00h7cm/T/tmp.JbxaL9nU is not shared from OS X and is not known to Docker . You can configure shared paths from Docker - > Preferences ... - > File Sharing . See https : //docs.docker.com/docker-for-mac/osxfs/ # namespaces for more info . . time= '' 2018-06-20T14:59:52-06:00 '' level=error msg= '' error waiting for container : context canceled '' aws/provision-db.sh : Removing ingress rule ... `` ` This with a vanilla install of Docker on MacOS 10.13.4 .
samples/guestbook : GCP section fails on terraform apply __EoT__ Terraform fails to provision GCP resources with the following errors : `` ` Error : Error applying plan : 4 error ( s ) occurred : * google_project_service.container : 1 error ( s ) occurred : * google_project_service.container : Error enabling service : Error waiting for apis [ `` container.googleapis.com '' ] to be enabled for enocom-dev : googleapi : Error 403 : The caller does not have permission , forbidden * google_project_service.runtimeconfig : 1 error ( s ) occurred : * google_project_service.runtimeconfig : Error enabling service : Error waiting for apis [ `` runtimeconfig.googleapis.com '' ] to be enabled for enocom-dev : googleapi : Error 403 : The caller does not have permission , forbidden * google_project_service.storage : 1 error ( s ) occurred : * google_project_service.storage : Error enabling service : failed to issue request : googleapi : Error 403 : The caller does not have permission , forbidden * google_project_service.sql : 1 error ( s ) occurred : * google_project_service.sql : Error enabling service : failed to issue request : googleapi : Error 403 : The caller does not have permission , forbidden `` ` This was apparently
internal/contributebot : allow titles like `` README : `` __EoT__ For both issue and pull request titles .
pubsub : add conformance tests __EoT__
pubsub : add conformance tests __EoT__
pubsub : add conformance tests __EoT__
pubsub : add conformance tests __EoT__
pubsub : add conformance tests __EoT__
blob : add As for List functions __EoT__ PR # 510 is adding a couple of ` List ` functions , we should support the ` As ` escape hatches for the ` List ` types , including ` ListOptions ` and ` ListObject ` .
blob : add As for List functions __EoT__ PR # 510 is adding a couple of ` List ` functions , we should support the ` As ` escape hatches for the ` List ` types , including ` ListOptions ` and ` ListObject ` .
blob/gcsblob : add HTTP replay tests __EoT__ There 's a few tests here , but it would be good to write some HTTP replay tests that exercise the GCS blob API . This can probably borrow structure from the fileblob tests .
samples/guestbook : add links to Wire/Terraform as they 're mentioned in README __EoT__ From documentation review .
all : travis should verify that ` go generate ` output matches checked-in ` wire_gen.go ` __EoT__ Travis should verify that running ` go generate ` for the Wire injectors in our repo actually produces the checked-in ` wire_gen.go ` .
samples/guestbook : Failing pod in deploy to GKE __EoT__ Following the deploy to GKE , the guestbook pod fails the readiness check : `` ` Warning Unhealthy [ omitted ] Readiness probe failed : HTTP probe failed with statuscode : 500 `` ` The logs for the pod are filled with the following messages : `` ` 2018/06/22 00:37:09 watch MOTD variable : Variable.Watch : rpc error : code = PermissionDenied desc = The caller does not have permission 2018/06/22 00:37:09 watch MOTD variable : Variable.Watch : rpc error : code = PermissionDenied desc = The caller does not have permission 2018/06/22 00:37:09 Error exporting to Stackdriver : rpc error : code = PermissionDenied desc = The caller does not have permission `` `
blob : bucket writer has no 'abandon ' or 'discard ' functionality __EoT__ When inserting a large blob into a bucket , there is often a desire to stream a value from some source ( e.g. , a large file on disk ) . This process may fail halfway , meaning there should be some way to discard a partially written file . The [ ` Writer ` object ] ( https : //godoc.org/github.com/google/go-cloud/blob # Writer ) only has a ` Write ( ) ` and ` Close ( ) ` function , but no method to cancel the write itself . Should such a function be added ? Noteworthy : The Minio and S3 libraries solve this by not having a ` Writer ` -like object , but instead have a function that takes a ` Reader ` and consumes the data .
pubsub : Topic.Shutdown ( ctx ) __EoT__ ` Subscription.Close ( ) ` is being converted to ` Shutdown ( ctx ) ` . It seems inconsistent not to do the same for ` Topic.Close ( ) ` .
samples/guestbook/gcp : provision-db.sh depends on jq __EoT__ When I ran ` terraform apply ` from the sample README , it failed like this on my Mac : `` ` provision-db.sh : line 63 : jq : command not found `` ` Of course I could just ` brew install jq ` but it 's a bit of a rough spot . Maybe it would be better to rewrite this script in Go so it ca n't fail this way .
samples/guestbook/gcp : provision-db.sh depends on jq __EoT__ When I ran ` terraform apply ` from the sample README , it failed like this on my Mac : `` ` provision-db.sh : line 63 : jq : command not found `` ` Of course I could just ` brew install jq ` but it 's a bit of a rough spot . Maybe it would be better to rewrite this script in Go so it ca n't fail this way .
samples/guestbook/gcp : provision-db.sh depends on jq __EoT__ When I ran ` terraform apply ` from the sample README , it failed like this on my Mac : `` ` provision-db.sh : line 63 : jq : command not found `` ` Of course I could just ` brew install jq ` but it 's a bit of a rough spot . Maybe it would be better to rewrite this script in Go so it ca n't fail this way .
samples/guestbook/gcp : provision-db.sh depends on jq __EoT__ When I ran ` terraform apply ` from the sample README , it failed like this on my Mac : `` ` provision-db.sh : line 63 : jq : command not found `` ` Of course I could just ` brew install jq ` but it 's a bit of a rough spot . Maybe it would be better to rewrite this script in Go so it ca n't fail this way .
samples/guestbook/gcp : provision-db.sh depends on jq __EoT__ When I ran ` terraform apply ` from the sample README , it failed like this on my Mac : `` ` provision-db.sh : line 63 : jq : command not found `` ` Of course I could just ` brew install jq ` but it 's a bit of a rough spot . Maybe it would be better to rewrite this script in Go so it ca n't fail this way .
samples/guestbook/gcp : provision-db.sh depends on jq __EoT__ When I ran ` terraform apply ` from the sample README , it failed like this on my Mac : `` ` provision-db.sh : line 63 : jq : command not found `` ` Of course I could just ` brew install jq ` but it 's a bit of a rough spot . Maybe it would be better to rewrite this script in Go so it ca n't fail this way .
travis : set up a Mac OS X environment __EoT__ Is it just as easy as https : //docs.travis-ci.com/user/reference/osx/ # Using-OS-X and running it as a shard ?
s3blob : Convert to hit AWS and use replays throughout __EoT__ TestMain is making it kinda gnarly to run the test . Let 's remove it and make it easier to modify .
s3blob : Convert to hit AWS and use replays throughout __EoT__ TestMain is making it kinda gnarly to run the test . Let 's remove it and make it easier to modify .
blob : support signed url __EoT__ `` ` go //SignedURLOptions allows you to restrict the access to the signed URL . type SignedURLOptions struct { // Expires is the expiration time on the signed URL . It must be // a datetime in the future . // Required . Expires time.Time // ContentType is the content type header the client must provide // to use the generated signed URL . // Optional . ContentType string } // Bucket provides read , write and delete operations on objects within it on the // blob service . type Bucket interface { // ... .other operations // SignedURL returns a URL for the specified object . Signed URLs allow // the users access to a restricted resource for a limited time SignedURL ( bucket , key string , opts *SignedURLOptions ) ( string , error ) } `` `
pubsub/gcppubsub : allow logging of pull errors __EoT__ Quoting @ zombiezen , in his review of https : //github.com/google/go-cloud/pull/705 , referring to the call to ` s.client.Pull ( ctx , req ) ` : > Unfortunately , this error will never get seen by the end user , but it could be helpful for debugging . We could add a hook to the constructor to the effect of : > > `` ` go > // A type that implements ErrorLogger reports errors that occur while receiving a batch . > type ErrorLogger interface { > LogError ( context.Context , error ) > } > `` ` > > ( Probably with some filtering for transient errors or such . )
pubsub/gcppubsub : allow logging of pull errors __EoT__ Quoting @ zombiezen , in his review of https : //github.com/google/go-cloud/pull/705 , referring to the call to ` s.client.Pull ( ctx , req ) ` : > Unfortunately , this error will never get seen by the end user , but it could be helpful for debugging . We could add a hook to the constructor to the effect of : > > `` ` go > // A type that implements ErrorLogger reports errors that occur while receiving a batch . > type ErrorLogger interface { > LogError ( context.Context , error ) > } > `` ` > > ( Probably with some filtering for transient errors or such . )
all : implement escape hatches __EoT__ Design proposal has circulated , and will be added to design.md in a PR soon .
all : replay tests have diffs even if interactions have n't changed __EoT__ The HTTP/gRPC replay tests currently do n't scrub transient attributes like date or request ID , which makes diffs noisy . This makes it difficult for a reviewer to see whether an interaction has actually changed or whether it 's just normal noise .
all : replay tests have diffs even if interactions have n't changed __EoT__ The HTTP/gRPC replay tests currently do n't scrub transient attributes like date or request ID , which makes diffs noisy . This makes it difficult for a reviewer to see whether an interaction has actually changed or whether it 's just normal noise .
all : concrete types and interfaces having really similar methods is confusing __EoT__ For Blob , we have a ` NewWriter ` in the concrete Bucket type , and a ` NewWriter ` in the driver.Bucket interface . Their method signatures are subtley different , which made me think that the concrete type was trying to incorrectly implement the driver.Bucket interface and there was a bug . This was n't the case ; it was working as designed and wrapping the interface implementation . To avoid confusion , let 's rename methods in the driver.Bucket interface if they have very subtle differences in method signature . In this example , ` driver.Bucket.NewWriter ` can go to ` driver.Bucket.TypedWriter ` . There may be more in other driver packages .
pubsub : GCP driver __EoT__
pubsub : GCP driver __EoT__
pubsub : GCP driver __EoT__
pubsub : GCP driver __EoT__
pubsub : GCP driver __EoT__
pubsub : GCP driver __EoT__
pubsub : GCP driver __EoT__
pubsub : GCP driver __EoT__
samples : guestbook still uses panic ( wire.Build ( ... ) ) __EoT__ It 's arguably clearer to use `` ` wire.Build ( ... ) return stuff `` `
mysql : OpenCensus instrumentation not used __EoT__ For example , in the ` cloudmysql ` package , the driver is wrapped here : https : //github.com/google/go-cloud/blob/11bb776c80e5cc8abaa6c9aead66d5c1129c9632/mysql/cloudmysql/cloudmysql.go # L95 but is not wrapped in the ` Connect ` method , which is what is called to create new connections : https : //github.com/google/go-cloud/blob/11bb776c80e5cc8abaa6c9aead66d5c1129c9632/mysql/cloudmysql/cloudmysql.go # L79
mysql : OpenCensus instrumentation not used __EoT__ For example , in the ` cloudmysql ` package , the driver is wrapped here : https : //github.com/google/go-cloud/blob/11bb776c80e5cc8abaa6c9aead66d5c1129c9632/mysql/cloudmysql/cloudmysql.go # L95 but is not wrapped in the ` Connect ` method , which is what is called to create new connections : https : //github.com/google/go-cloud/blob/11bb776c80e5cc8abaa6c9aead66d5c1129c9632/mysql/cloudmysql/cloudmysql.go # L79
travis : golint-ci fails build due to `` unused '' provider sets in package main __EoT__ While testing a PR , I noticed that Travis flagged a Wire provider set as an unused variable ( [ source ] ( https : //travis-ci.com/google/go-cloud/builds/75304110 # L661 ) ) : `` ` samples/guestbook/main.go:269:5 : ` appSet ` is unused ( varcheck ) var appSet = wire.NewSet ( ^ `` ` This is technically correct , but not an actual issue , and it did cause the build to fail , thus blocking submission . Based on recommendation from @ cflewis , I 'm disabling ` varcheck ` as a temporary workaround , then keeping this open for solving the underlying issue .
all : windows builds keep flaking __EoT__ See for example https : //github.com/google/go-cloud/pull/693/checks ? check_run_id=31559768 . This happened twice in a row and each time takes 15 minutes to run , so running it again is n't really a viable solution . Let 's disable the Windows builds until we get the flakes under control .
tests : AWS integration test harness __EoT__ Much like tests/gcp , add an integration test that runs on EC2 and checks that traces and logs are written .
tests : AWS integration test harness __EoT__ Much like tests/gcp , add an integration test that runs on EC2 and checks that traces and logs are written .
tests : AWS integration test harness __EoT__ Much like tests/gcp , add an integration test that runs on EC2 and checks that traces and logs are written .
tests : AWS integration test harness __EoT__ Much like tests/gcp , add an integration test that runs on EC2 and checks that traces and logs are written .
testing : Figure out how to scrub sensitive information from RPCs __EoT__ Can we scrub the project ID from the RPCs ? Maybe the RPCs are just replayed without matching ? Got to see how the RPC replay system works . Blocking # 61 .
all : rename repository to go-x-cloud __EoT__ The project name is `` Go X Cloud '' . Our repository name and import path should reflect that . Import path : ` github.com/google/go-x-cloud ` .
pubsub : split MakePair ( ) into OpenTopic ( ) and OpenSubscription ( topic ) __EoT__ @ vangent noted how MakePair does n't handle the case of making multiple subscriptions in a comment on https : //github.com/google/go-cloud/pull/712 .
pubsub : split MakePair ( ) into OpenTopic ( ) and OpenSubscription ( topic ) __EoT__ @ vangent noted how MakePair does n't handle the case of making multiple subscriptions in a comment on https : //github.com/google/go-cloud/pull/712 .
pubsub : split MakePair ( ) into OpenTopic ( ) and OpenSubscription ( topic ) __EoT__ @ vangent noted how MakePair does n't handle the case of making multiple subscriptions in a comment on https : //github.com/google/go-cloud/pull/712 .
pubsub : split MakePair ( ) into OpenTopic ( ) and OpenSubscription ( topic ) __EoT__ @ vangent noted how MakePair does n't handle the case of making multiple subscriptions in a comment on https : //github.com/google/go-cloud/pull/712 .
pubsub : split MakePair ( ) into OpenTopic ( ) and OpenSubscription ( topic ) __EoT__ @ vangent noted how MakePair does n't handle the case of making multiple subscriptions in a comment on https : //github.com/google/go-cloud/pull/712 .
pubsub : split MakePair ( ) into OpenTopic ( ) and OpenSubscription ( topic ) __EoT__ @ vangent noted how MakePair does n't handle the case of making multiple subscriptions in a comment on https : //github.com/google/go-cloud/pull/712 .
samples : tests broken by recent commit __EoT__ https : //github.com/google/go-cloud/pull/511 caused some breakage : https : //travis-ci.com/google/go-cloud/jobs/156110439 I 'm still not sure how this got through . I thought the checks prevented us from committing broken changes .
samples : tests broken by recent commit __EoT__ https : //github.com/google/go-cloud/pull/511 caused some breakage : https : //travis-ci.com/google/go-cloud/jobs/156110439 I 'm still not sure how this got through . I thought the checks prevented us from committing broken changes .
samples : tests broken by recent commit __EoT__ https : //github.com/google/go-cloud/pull/511 caused some breakage : https : //travis-ci.com/google/go-cloud/jobs/156110439 I 'm still not sure how this got through . I thought the checks prevented us from committing broken changes .
samples : tests broken by recent commit __EoT__ https : //github.com/google/go-cloud/pull/511 caused some breakage : https : //travis-ci.com/google/go-cloud/jobs/156110439 I 'm still not sure how this got through . I thought the checks prevented us from committing broken changes .
all : more instances of panic ( wire.Build ( ... ) ) found __EoT__ `` ` issactrotts-macbookpro : go-cloud issactrotts $ ack 'panic\ ( wire\.Build' wire/internal/wire/testdata/NamingWorstCase/foo/wire.go 26 : panic ( wire.Build ( provide ) ) wire/internal/wire/testdata/MultipleMissingInputs/foo/wire.go 24 : panic ( wire.Build ( provideBaz ) ) wire/internal/wire/testdata/NamingWorstCaseAllInOne/foo/foo.go 54 : panic ( wire.Build ( Provide ) ) wire/internal/wire/testdata/InjectWithPanic/foo/wire.go 24 : panic ( wire.Build ( provideMessage ) ) wire/README.md 354 : panic ( wire.Build ( /* ... */ ) ) wire/wire.go 36 : // panic ( wire.Build ( otherpkg.FooSet , myProviderFunc ) ) tests/gcp/app/inject.go 29 : panic ( wire.Build ( gcp/gcpcloud/example_test.go 60 : panic ( wire.Build ( aws/awscloud/example_test.go 60 : panic ( wire.Build ( `` `
internal/secrets : Consolidate interfaces __EoT__ Pursuant to the [ discussion ] ( https : //github.com/google/go-cloud/pull/899 # discussion_r239982555 ) on the ` localsecrets ` implementation , should the ` secrets ` package have : - 2 interface types : ` Encrypter ` and ` Decrypter ` - 3 interface types : ` Encrypter ` , ` Decrypter ` , and ` Crypter ` ( composed of the first two ) - 1 interface type : ` Crypter ` ( This is intended as an independent decision from whether the ` secrets ` package should have a driver or not . )
internal/secrets : Consolidate interfaces __EoT__ Pursuant to the [ discussion ] ( https : //github.com/google/go-cloud/pull/899 # discussion_r239982555 ) on the ` localsecrets ` implementation , should the ` secrets ` package have : - 2 interface types : ` Encrypter ` and ` Decrypter ` - 3 interface types : ` Encrypter ` , ` Decrypter ` , and ` Crypter ` ( composed of the first two ) - 1 interface type : ` Crypter ` ( This is intended as an independent decision from whether the ` secrets ` package should have a driver or not . )
internal/secrets : Consolidate interfaces __EoT__ Pursuant to the [ discussion ] ( https : //github.com/google/go-cloud/pull/899 # discussion_r239982555 ) on the ` localsecrets ` implementation , should the ` secrets ` package have : - 2 interface types : ` Encrypter ` and ` Decrypter ` - 3 interface types : ` Encrypter ` , ` Decrypter ` , and ` Crypter ` ( composed of the first two ) - 1 interface type : ` Crypter ` ( This is intended as an independent decision from whether the ` secrets ` package should have a driver or not . )
blob : check errors via an interface function , not an error subtype __EoT__ As per # 730 , blob drivers should provide a method to determine whether an error is retryable . The ` Error ` type would no longer do this , and maybe it should be removed .
runtimevar/runtimeconfigurator : add RPC replay tests __EoT__ There are existing tests in this package , they should be converted to use RPC replay . RPC replay already exists in the Go Cloud Client libraries .
internal/proxy : eliminate possibility of conflicts __EoT__ Currently , if two contributors to Go Cloud attempt to update the module proxy , each trying to add a new dependency , they may accidentally break each others ' pull request builds . It would be better if the script effectively only appended to the GCS bucket instead of deleting values to avoid this possibility , however infrequent .
blob/tests : Refactor tests to not doing any Bucket provisioning , and take all inputs as constants __EoT__ As a first step toward making -- record tests runnable by everyone , let 's make tests use constants for their inputs ( e.g. , bucket name ) , replacing any existing flags , and update tests to not use anything other than the constants . Tests should not create/destroy buckets either , do that manually for now . So , the process for running AWS blob tests in -- record would be : 1 . Manually provision a bucket . 2 . Update the constant ( s ) in s3blob_test.go to use your bucket name . 3 . Run the tests . There will be a separate follow-on bug for using Terraform to automate the provisioning .
pubsub : move gcppubsub endPoint into a helper __EoT__ @ vangent 's comment on https : //github.com/google/go-cloud/pull/774 , on line 55 of gcppubsub_test.go , `` The endpoint should probably go into a helper in gcppubsub , similar to what 's in runtimevar . OK to do in a separate PR . ''
pubsub : move gcppubsub endPoint into a helper __EoT__ @ vangent 's comment on https : //github.com/google/go-cloud/pull/774 , on line 55 of gcppubsub_test.go , `` The endpoint should probably go into a helper in gcppubsub , similar to what 's in runtimevar . OK to do in a separate PR . ''
pubsub : move gcppubsub endPoint into a helper __EoT__ @ vangent 's comment on https : //github.com/google/go-cloud/pull/774 , on line 55 of gcppubsub_test.go , `` The endpoint should probably go into a helper in gcppubsub , similar to what 's in runtimevar . OK to do in a separate PR . ''
pubsub : move gcppubsub endPoint into a helper __EoT__ @ vangent 's comment on https : //github.com/google/go-cloud/pull/774 , on line 55 of gcppubsub_test.go , `` The endpoint should probably go into a helper in gcppubsub , similar to what 's in runtimevar . OK to do in a separate PR . ''
pubsub : move gcppubsub endPoint into a helper __EoT__ @ vangent 's comment on https : //github.com/google/go-cloud/pull/774 , on line 55 of gcppubsub_test.go , `` The endpoint should probably go into a helper in gcppubsub , similar to what 's in runtimevar . OK to do in a separate PR . ''
pubsub : move gcppubsub endPoint into a helper __EoT__ @ vangent 's comment on https : //github.com/google/go-cloud/pull/774 , on line 55 of gcppubsub_test.go , `` The endpoint should probably go into a helper in gcppubsub , similar to what 's in runtimevar . OK to do in a separate PR . ''
Make tests catch invalid absolute URLs in methods . __EoT__ When adding support for new endpoints , it 's possible to make a mistake and use an absolute URL in the method implementation , rather than a relative one . We need to use relative URLs so that the ` Client.BaseURL ` field works properly in ` NewRequest ` method : https : //github.com/google/go-github/blob/0f6d3ce15ec23c92c74d014303a167a9a374dd7e/github/github.go # L112-L115 https : //github.com/google/go-github/blob/0f6d3ce15ec23c92c74d014303a167a9a374dd7e/github/github.go # L238-L247 E.g. , this is invalid : `` ` Go u : = fmt.Sprintf ( `` /admin/users/ % v/authorizations '' , username ) `` ` This is correct : `` ` Go u : = fmt.Sprintf ( `` admin/users/ % v/authorizations '' , username ) `` ` Including that leading slash has no negative effect on GitHub API clients , but it typically breaks GitHub Enterprise clients . That makes this type of mistake easy to miss during code review . Right now , our tests do not catch this , so we rely on human reviewers to catch it ( e.g. , this recently happened in # 749 ) . I 've thought about it and I think I found a way to make it so that our tests
Support for user 's permissions endpoint __EoT__ Maybe I just ca n't find it , but it looks like this endpoint is not supported : https : //developer.github.com/v3/repos/collaborators/ # review-a-users-permission-level Was added fairly recently : https : //developer.github.com/changes/2016-11-28-preview-org-membership/
[ Feature Request ] Support Branch protection for setting ` required_approving_review_count ` __EoT__ Per the [ rest api docs ] ( https : //developer.github.com/v3/repos/branches/ # update-branch-protection ) you can send ` required_approving_review_count ` as part of the ` required_pull_request_reviews ` object with an integer between 1-6 which represents the number of people required to approve the request in larger teams it is not uncommon to want more than a single approval before merging . ! [ image ] ( https : //user-images.githubusercontent.com/3145127/39460174-282769c8-4cb6-11e8-8d83-e01f5ef4c75c.png )
Update Projects API to support organizations projects __EoT__ Announcement : https : //developer.github.com/changes/2016-10-27-changes-to-projects-api/ /cc @ balintant since you did the initial implementation of this . **Edit by @ shurcooL : ** The initial implementation of this that we merged was # 438 that @ gmlewis made , not # 433 .
int64 IDs __EoT__ On 32-bit platforms , a Go ` int ` is an ` int32 ` . I worry about the use of ` int ` as the type for Github IDs in the go-github package . Is there documentation to suggest that Github IDs are limited to 2 billion items ? Seems unlikely . But I ca n't find anything . I propose we keep `` Number '' fields as int , but change all the ` ID ` fields to be ` int64 ` . Thoughts ? /cc @ gmlewis @ willnorris @ shurcooL
{ Create , Update } CheckRunOptions missing Actions __EoT__ As per the docs , check runs support optionally exposing some user-invoked actions . https : //developer.github.com/v3/checks/runs/ # create-a-check-run https : //developer.github.com/v3/checks/runs/ # update-a-check-run These fields are missing from ` CreateCheckRunOptions ` and ` UpdateCheckRunOptions ` structs .
Support new user blocking APIs and webhook __EoT__ Announcement : https : //developer.github.com/changes/2017-02-28-user-blocking-apis-and-webhook/ GitHub API docs : - [ x ] https : //developer.github.com/v3/users/blocking/ - [ x ] https : //developer.github.com/v3/orgs/blocking/ - [ x ] https : //developer.github.com/v3/activity/events/types/ # orgblockevent
Support new user blocking APIs and webhook __EoT__ Announcement : https : //developer.github.com/changes/2017-02-28-user-blocking-apis-and-webhook/ GitHub API docs : - [ x ] https : //developer.github.com/v3/users/blocking/ - [ x ] https : //developer.github.com/v3/orgs/blocking/ - [ x ] https : //developer.github.com/v3/activity/events/types/ # orgblockevent
Support new user blocking APIs and webhook __EoT__ Announcement : https : //developer.github.com/changes/2017-02-28-user-blocking-apis-and-webhook/ GitHub API docs : - [ x ] https : //developer.github.com/v3/users/blocking/ - [ x ] https : //developer.github.com/v3/orgs/blocking/ - [ x ] https : //developer.github.com/v3/activity/events/types/ # orgblockevent
Support preview Organization Invitation API __EoT__ GitHub Developer API announcement : https : //developer.github.com/changes/2018-01-25-organization-invitation-api-preview/ Supporting this change will require the following : - [ ] create two new endpoints - [ ] add two new fields to an existing endpoint - [ ] add the new custom media type - [ ] create tests for the new endpoints and update the test ( s ) for the existing endpoint - [ ] run the generator as described in [ CONTRIBUTING.md ] ( https : //github.com/google/go-github/blob/master/CONTRIBUTING.md ) This would be a great PR for any new contributor to this repo or a new Go developer . All contributions are greatly appreciated ! Feel free to volunteer for any issue , and we can send you an invite to contribute to the repo ( which you then accept ) and the issue can be assigned to you so that others do n't attempt to duplicate the work . Thank you !
Make empty responses due to 202 Accepted status code more visible . __EoT__ I 've built a tool to gather all stats of all repos in a given org and rank the users , and , as I just found , some times github returns an empty hash on the endpoint that returns the statistics ( ` client.Repositories.ListContributorsStats ` on go-github ) . I changed ` github.go ` to debug the issue : `` ` diff @ @ -415,9 +408,13 @ @ func ( c *Client ) Do ( req *http.Request , v interface { } ) ( *Response , error ) { if w , ok : = v. ( io.Writer ) ; ok { io.Copy ( w , resp.Body ) } else { - err = json.NewDecoder ( resp.Body ) .Decode ( v ) + content , _ : = ioutil.ReadAll ( response.Body ) + body2 : = ioutil.NopCloser ( bytes.NewReader ( content ) ) + err = json.NewDecoder ( body2 ) .Decode ( v ) if err == io.EOF { err = nil // ignore EOF errors caused by empty response body + } else if err ! = nil { + println ( `` Invalid JSON
Policy on using pointers/values in structs for receiving data from GitHub servers . __EoT__ I 'm making this issue to discuss a point that came up while reviewing # 534 . /cc @ alindeman # # # Background In # 534 , @ alindeman answered my questions : > > @ shurcooL : What 's the reason to use ` string ` value rather than ` *string ` pointer , as is commonly done in other structs ? > > @ shurcooL : What 's the reason to omit ` , omitempty ` option , contrary to most similar structs we have ? > > @ alindeman : Thinking back to the [ conversation we had over in # 512 ] ( https : //github.com/google/go-github/pull/512 # issuecomment-272350152 ) , I thought that since -- as far as I can tell -- these attributes will always be returned from the API , we could specify them as non-pointer types . One key difference is that the conversation in # 512 was primarily about the request structs - structs that are used to send values to github servers . The affect on structs for receiving and unmarshaling data from github servers was
Incorrect Accept header in DownloadReleaseAsset . __EoT__ https : //github.com/google/go-github/blob/d2b80df1e3eeccae96eb1cb489e65372cc9550c8/github/repos_releases.go # L256 I was able to confirm with curl , that removing ` mediaTypeGraphQLNodeIDPreview ` from the Accept header will result in the server returning the correct response . With the ` mediaTypeGraphQLNodeIDPreview ` Accept header the v3 json response is returned .
Add support for Commit Search preview API . __EoT__ Announcement : https : //developer.github.com/changes/2017-01-05-commit-search-api/ . Docs : https : //developer.github.com/v3/search/ # search-commits . This was announced today .
Unit tests should use local client instead of package-global client __EoT__ In https : //github.com/google/go-github/pull/732 it became clear that having a package-global ` client ` for unit testing will prevent the unit tests from being run in parallel in the future . Ideally , each test will have its own client ... something like this : `` ` go client , teardown : = setup ( ) defer teardown ( ) `` ` and the global ` client ` in ` github_test.go ` will be removed . This would make for a very large PR , but could possibly be done with the assistance of [ ` gofmt -r ` ] ( https : //blog.golang.org/go-fmt-your-code ) or your favorite text editor . : smile : This would also be a great project for a new contributor to the repo . Let it be known that all help is greatly appreciated ! Thank you in advance .
Support Marketplace ( preview ) API . __EoT__ GitHub API Docs : https : //developer.github.com/v3/apps/marketplace/ . Not seeing an announcement blog post mentioning this , but there are new API endpoints available , as documented at the URL above . This issue is to track their implementation . A sub-task here is supporting the ` MarketplacePurchaseEvent ` event , which is tracked in a separate issue # 704 .
Support Marketplace ( preview ) API . __EoT__ GitHub API Docs : https : //developer.github.com/v3/apps/marketplace/ . Not seeing an announcement blog post mentioning this , but there are new API endpoints available , as documented at the URL above . This issue is to track their implementation . A sub-task here is supporting the ` MarketplacePurchaseEvent ` event , which is tracked in a separate issue # 704 .
github-accessors.go : Missing accessors for non-Struct type fields __EoT__ While working on # 770 , I noticed that we are creating accessors only for fields which have a struct type . I looked further into it today and confirmed the same . Is this intentional ? If so , why ? /cc @ gmlewis @ shurcooL
github-accessors.go : Missing accessors for non-Struct type fields __EoT__ While working on # 770 , I noticed that we are creating accessors only for fields which have a struct type . I looked further into it today and confirmed the same . Is this intentional ? If so , why ? /cc @ gmlewis @ shurcooL
CommitsComparison should have method GetHTMLURL ( ) __EoT__ It would be useful to have this method for CommitsComparison . Of course , you can build it yourself if you have the parameters necessary to call RepositoriesService.CompareCommits ( ) , but it would be cleaner .
With removal of deprecated client.Rate in # 555 , rate limit info needs more visibility __EoT__ It used to be that clients might make API calls like : `` ` go repos , resp , err : = client.Repositories.ListByOrg ( org , opt ) if err ! = nil { rate : = client.Rate ( ) if rate.Remaining == 0 { // ... } // ... } `` ` but after # 555 , it turns out that ` checkRateLimitBeforeDo ` will return an error without rate information . I have a PR on the way that addresses this issue .
Support preview Protected Branches API __EoT__ GitHub Developer API announcement : https : //developer.github.com/changes/2018-03-16-protected-branches-required-approving-reviews/ This would be a great PR for any new contributor to this repo or a new Go developer . All contributions are greatly appreciated ! Feel free to volunteer for any issue , and we can send you an invite to contribute to the repo ( which you then accept after you enable two-factor authentication ) and the issue can be assigned to you so that others do n't attempt to duplicate the work . Please check out our [ CONTRIBUTING.md ] ( https : //github.com/google/go-github/blob/master/CONTRIBUTING.md ) guide to get started . Thank you !
Make use of testHeader and testMethod consistent __EoT__ As found out while reviewing https : //github.com/google/go-github/pull/817 , we have some inconsistencies in our tests where some of the tests do n't use the ` testHeader ` and ` testMethod ` functions to test request header and method . One of the defaulter is https : //github.com/google/go-github/blob/master/github/git_trees_test.go # L22 . This is completely harmless . Though it 's important to make sure it does n't cause further confusion for the new contributors as it did in that PR initially . Via this issue , I 'd like to point it out and mention that I 'd like to fix this inconsistency .
Support the new Checks API ( beta ) __EoT__ The new Checks API was recently announced in https : //developer.github.com/changes/2018-05-07-new-checks-api-public-beta/ While it 's currently in beta and supports only GithHubApps , it might be worthwhile adding support . I 'm happy to get an initial PR going for this .
Support the new Checks API ( beta ) __EoT__ The new Checks API was recently announced in https : //developer.github.com/changes/2018-05-07-new-checks-api-public-beta/ While it 's currently in beta and supports only GithHubApps , it might be worthwhile adding support . I 'm happy to get an initial PR going for this .
Support the new Checks API ( beta ) __EoT__ The new Checks API was recently announced in https : //developer.github.com/changes/2018-05-07-new-checks-api-public-beta/ While it 's currently in beta and supports only GithHubApps , it might be worthwhile adding support . I 'm happy to get an initial PR going for this .
Support the new Checks API ( beta ) __EoT__ The new Checks API was recently announced in https : //developer.github.com/changes/2018-05-07-new-checks-api-public-beta/ While it 's currently in beta and supports only GithHubApps , it might be worthwhile adding support . I 'm happy to get an initial PR going for this .
Support the new Checks API ( beta ) __EoT__ The new Checks API was recently announced in https : //developer.github.com/changes/2018-05-07-new-checks-api-public-beta/ While it 's currently in beta and supports only GithHubApps , it might be worthwhile adding support . I 'm happy to get an initial PR going for this .
( Another ) heads up : stricter validation coming soon in the create repo for org endpoint __EoT__ A few weeks back we discussed an issue where the GitHub API is going to return 422s when incorrect parameters are passed ( rather than just ignoring those parameters ) : https : //github.com/google/go-github/issues/992 We found another instance where we get incorrect parameters with the google/go-github header . Our current plan is to turn on stricter validation on November 1st ( we were going to do this earlier , but we 've been seeing a lot of mismatches across the board , and are doing some work to let everyone know first ! ) . The [ Create a repository for an organization ] ( https : //developer.github.com/v3/repos/ # create ) endpoint does n't take an ` archived ` parameter , but we 're seeing ` archived ` being passed to a number of endpoints . Would you have a chance to look into whether this is encoded into the library itself , or whether it might be user error ?
Apps.ListUserRepos may return message `` specify a custom media type in the 'Accept ' header '' . __EoT__ When I am trying to use ` Apps.ListUserRepos ` , I got message like this : ! [ image ] ( https : //user-images.githubusercontent.com/14567045/33372247-3e8de840-d538-11e7-944a-75e1d68be1ea.png ) I know this can be fix by adding following accept header : `` ` go // in github/github.go line 85~86 // https : //developer.github.com/changes/2016-09-14-Integrations-Early-Access/ mediaTypeIntegrationPreview = `` application/vnd.github.machine-man-preview+json '' `` ` And it can be fix by add following to ` ListUserRepos ` function in ` github/apps_installation.go ` `` ` go req.Header.Set ( `` Accept '' , mediaTypeIntegrationPreview ) `` ` I can provide a PR to fix this . I noticed that there is a TODO comment in other function says that ` TODO : remove custom Accept header when this API fully launches. ` , so I will keep this in my fix .
authorizations API support __EoT__ https : //developer.github.com/v3/oauth_authorizations/ One tricky part is that it only accepts basic auth , so I think it 'd require users of the library to implement a simple RoundTripper that added this header ( and then use that as the HTTP client transport for go-github ) .
Support preview Nested Teams API __EoT__ GitHub Announcement : https : //developer.github.com/changes/2017-08-30-preview-nested-teams/ This involves adding new endpoints and changing existing endpoints according to the blog post . Successful completion and closing of this issue involves addressing all the changes described in the GitHub announcement . Note that it does n't have to all be done in a single PR . It is always a good idea to carefully document what portions are being addressed in any given PR and if there is still work yet to be done . Thank you in advance for contributing to this open source project ! Your assistance is greatly appreciated .
Support preview Nested Teams API __EoT__ GitHub Announcement : https : //developer.github.com/changes/2017-08-30-preview-nested-teams/ This involves adding new endpoints and changing existing endpoints according to the blog post . Successful completion and closing of this issue involves addressing all the changes described in the GitHub announcement . Note that it does n't have to all be done in a single PR . It is always a good idea to carefully document what portions are being addressed in any given PR and if there is still work yet to be done . Thank you in advance for contributing to this open source project ! Your assistance is greatly appreciated .
Support preview Nested Teams API __EoT__ GitHub Announcement : https : //developer.github.com/changes/2017-08-30-preview-nested-teams/ This involves adding new endpoints and changing existing endpoints according to the blog post . Successful completion and closing of this issue involves addressing all the changes described in the GitHub announcement . Note that it does n't have to all be done in a single PR . It is always a good idea to carefully document what portions are being addressed in any given PR and if there is still work yet to be done . Thank you in advance for contributing to this open source project ! Your assistance is greatly appreciated .
Support preview Nested Teams API __EoT__ GitHub Announcement : https : //developer.github.com/changes/2017-08-30-preview-nested-teams/ This involves adding new endpoints and changing existing endpoints according to the blog post . Successful completion and closing of this issue involves addressing all the changes described in the GitHub announcement . Note that it does n't have to all be done in a single PR . It is always a good idea to carefully document what portions are being addressed in any given PR and if there is still work yet to be done . Thank you in advance for contributing to this open source project ! Your assistance is greatly appreciated .
Support preview Nested Teams API __EoT__ GitHub Announcement : https : //developer.github.com/changes/2017-08-30-preview-nested-teams/ This involves adding new endpoints and changing existing endpoints according to the blog post . Successful completion and closing of this issue involves addressing all the changes described in the GitHub announcement . Note that it does n't have to all be done in a single PR . It is always a good idea to carefully document what portions are being addressed in any given PR and if there is still work yet to be done . Thank you in advance for contributing to this open source project ! Your assistance is greatly appreciated .
reorganize code into separate files per API __EoT__ @ wlynch92 's [ comment ] ( https : //github.com/google/go-github/pull/23 # issuecomment-21897951 ) : > I was thinking about adopting some sort of naming scheme that would easily allow us to separate and distinguish source files and the APIs they provide . Instead of looking through repos.go ( which after adding all 12 APIs , might be very lengthy ) for the ListHook function , why not separate the hook API into something like repos_hooks.go . This lets us sort APIs into separate files while still keeping everything in a single package and making a clear relation to the service they are a part of . It also is a close mapping to the overall layout of the Github API ( i.e . http : //developer.github.com/v3/repos/ - > repos.go and http : //developer.github.com/v3/repos/hooks/ - > repos_hooks.go ) . > > I think this is something we should definitely consider for a large services such as RepositoryService , and apply to smaller services as a matter of consistency .
Add support for new repository traffic API __EoT__ Announcement : https : //developer.github.com/changes/2016-08-15-traffic-api-preview/ Docs : https : //developer.github.com/v3/repos/traffic/ This is a tad bit more advanced , but would still be a good beginner task . It does n't require adding a new service ( just put these methods on the existing RepositoriesService ) , but likely does involve adding a few new data types .
Add support for new repository traffic API __EoT__ Announcement : https : //developer.github.com/changes/2016-08-15-traffic-api-preview/ Docs : https : //developer.github.com/v3/repos/traffic/ This is a tad bit more advanced , but would still be a good beginner task . It does n't require adding a new service ( just put these methods on the existing RepositoriesService ) , but likely does involve adding a few new data types .
Transfer request has incorrect json name for specifying team ids __EoT__ As pointed out in the [ recent comment on the PR ] ( https : //github.com/google/go-github/pull/788 # discussion_r201838235 ) for adding support , the json name should be ` team_ids ` , not ` team_id ` .
Add support for MarketplacePurchaseEvent __EoT__ When looking through the [ webhook events ] ( https : //developer.github.com/webhooks/ ) I noticed that we currently do not support the [ MarketplacePurchaseEvent ] ( https : //developer.github.com/v3/activity/events/types/ # marketplacepurchaseevent ) in [ ` messages.go ` ] ( https : //github.com/google/go-github/blob/master/github/messages.go # L55 ) . It would be nice to add this support and it would be a great project for a new contributor to this repo .
goauth2 is deprecated __EoT__ golang.org/x/oauth2 should be supported in it 's place .
OrganizationsListOptions and UserListOptions should n't have ListOptions field . __EoT__ While reviewing # 899 , I spotted what I believe to be an issue with the ` OrganizationsService.ListAll ` method . It takes an ` opt *OrganizationsListOptions ` parameter , which is : https : //github.com/google/go-github/blob/69ab5d997213689855e5db797eb5016ad0cc957a/github/orgs.go # L74-L81 According to the GitHub API docs for that endpoint , available at https : //developer.github.com/v3/orgs/ # list-all-organizations : > Note : Pagination is powered exclusively by the ` since ` parameter . Use the Link header to get the URL for the next page of organizations . As a result , I believe it 's incorrect for ` OrganizationsListOptions ` to embed the ` ListOptions ` struct , which contains the normal pagination parameters page/per_page . If this analysis is confirmed to be correct , then ` ListOptions ` should be removed from ` OrganizationsListOptions ` , since it does n't belong there .
Please consider tagging versions __EoT__ API breaking changes like # 816 get merged into this package alarmingly often , which leaves the users scrambling to get their code compiling again . Because there are no published version tags , our options are to either be ready to update our code at a moments notice or pin/vendor a single snapshot in time and manually verify and update when we can . Essentially all dependency management tools in the ecosystem support the ability to tag certain changes as major versions so that we can get any non-breaking updates for `` free '' . It 'd make it a lot easier on the users of the package if you could maintain and publish version tags on GitHub , because otherwise we have no idea when the next ` go get ` is going to break everything .
search API __EoT__ starting work on the new [ search API ] ( http : //developer.github.com/v3/search/ )
Remove custom media type for License API __EoT__ GitHub API Announcement : https : //developer.github.com/changes/2017-11-28-ending-the-license-api-preview/ This issue is a place-holder for work that needs to be done after we have confirmation that Enterprise API customers no longer need the custom media type header , or when the semantic versioning issue ( # 376 ) is resolved .
versioning and tagging releases __EoT__ We generally try not to have egregious breaking changes in this library , but we 're also not afraid to do them when necessary ( the latest example being # 375 ) . We 've gotten a couple of people asking for us to use semver on google/go-querystring # 11 , which we could also consider doing here . That does n't help us for # 375 , but would allow us to do future breaking changes at a version boundary . I have no intention of changing the import path to use gopkg.in or anything like it , but it would still help set appropriate expectations for folks when they do upgrade to a new version . And keeping a ` CHANGELOG.md ` file might be nice ( although a bit of work to keep up to date ) to point out new APIs that we support , etc . @ gmlewis @ shurcooL what do you think ?
Support changes to the Checks REST API __EoT__ GitHub Developer API announcement : https : //developer.github.com/changes/2018-08-16-changes-to-the-checks-rest-api/ This would be a great PR for any new contributor to this repo or a new Go developer . All contributions are greatly appreciated ! Feel free to volunteer for any issue , and we can send you an invite to contribute to the repo ( which you then accept after you enable two-factor authentication ) and the issue can be assigned to you so that others do n't attempt to duplicate the work . Please check out our [ CONTRIBUTING.md ] ( https : //github.com/google/go-github/blob/master/CONTRIBUTING.md ) guide to get started . Thank you !
Support changes to the Checks REST API __EoT__ GitHub Developer API announcement : https : //developer.github.com/changes/2018-08-16-changes-to-the-checks-rest-api/ This would be a great PR for any new contributor to this repo or a new Go developer . All contributions are greatly appreciated ! Feel free to volunteer for any issue , and we can send you an invite to contribute to the repo ( which you then accept after you enable two-factor authentication ) and the issue can be assigned to you so that others do n't attempt to duplicate the work . Please check out our [ CONTRIBUTING.md ] ( https : //github.com/google/go-github/blob/master/CONTRIBUTING.md ) guide to get started . Thank you !
support the context package __EoT__ It would be nice if go-github supported the context package , so you could coordinate cancelation across many different goroutines . It 's now available in the standard library as https : //golang.org/pkg/context/ .
support the context package __EoT__ It would be nice if go-github supported the context package , so you could coordinate cancelation across many different goroutines . It 's now available in the standard library as https : //golang.org/pkg/context/ .
support the context package __EoT__ It would be nice if go-github supported the context package , so you could coordinate cancelation across many different goroutines . It 's now available in the standard library as https : //golang.org/pkg/context/ .
Support preview Installations API - updated routes for adding or removing a repository __EoT__ GitHub Developers announcment : https : //developer.github.com/changes/2017-06-30-installations-adding-removing-a-repository/ GitHub API docs : https : //developer.github.com/v3/apps/installations/ # add-repository-to-installation
What happens if you Git.CreateCommit ( ... , nil ) or PullRequests.Edit ( ... , nil ) ? __EoT__ ` GitService.CreateCommit ` is [ documented ] ( https : //godoc.org/github.com/google/go-github/github # GitService.CreateCommit ) as : > CreateCommit creates a new commit in a repository . > > The commit.Committer is optional and will be filled with the commit.Author data if omitted . If the commit.Author is omitted , it will be filled in with the authenticated user ’ s information and the current date . > > GitHub API docs : http : //developer.github.com/v3/git/commits/ # create-a-commit If it 's legal , what happens if a user calls ` CreateCommit ( `` owner '' , `` repo '' , nil ) ` ? Should we document it , because I do n't think it 's intuitive or clear . If it 's not legal , that should be documented . -- - Same question for ` PullRequestsService.Edit ` , [ documented ] ( https : //godoc.org/github.com/google/go-github/github # PullRequestsService.Edit ) as : > Edit a pull request . > > GitHub API docs : https : //developer.github.com/v3/pulls/ # update-a-pull-request -- - In case nil values are not legal , why are pointers
Heads up : stricter validation coming soon to the Update a release API endpoint __EoT__ Hi 👋 , I an engineer on the API team at GitHub . A colleague and I are currently going through GitHub 's REST API , tightening up validations . We noticed that google/go-github is passing undocumented parameters to the [ Edit a release ] ( https : //developer.github.com/v3/repos/releases/ # edit-a-release ) endpoint . In particular , we 're receiving the following parameters which the endpoint does n't know about : - ` assets ` - ` assets_url ` - ` author ` - ` created_at ` - ` html_url ` - ` id ` - ` node_id ` - ` published_at ` - ` tarball_url ` - ` url ` - ` zipball_url ` https : //github.com/google/go-github/blob/29a3681d054b5329b97878453f84a4ea20e54b49/github/repos_releases.go # L20-L40 Currently the backend code is ignoring unknown parameters , but we 're shortly going to change the validation to return a [ 422 Unprocessable Entity ] ( https : //httpstatuses.com/422 ) if an undocumented parameter is passed . We 're still discussing our timeline for this , but wanted to give you a heads up so you 're not caught by surprise . If you
Spreadsheet updates __EoT__ - Repos - > List Branches can be marked as `` Done '' - merged PR # 86 Also the following are deprecated API 's : - Downloads ( replaced with Releases , not on spreadsheet ) - > Replace w/ Releases ? - Legacy search ( replaced with completed `` Search '' spreadsheet group ) - > Remove ?
Support preview Hovercard API __EoT__ GitHub Developer API announcement : https : //developer.github.com/changes/2018-03-21-hovercard-api-preview/ This would be a great PR for any new contributor to this repo or a new Go developer . All contributions are greatly appreciated ! Feel free to volunteer for any issue , and we can send you an invite to contribute to the repo ( which you then accept after you enable two-factor authentication ) and the issue can be assigned to you so that others do n't attempt to duplicate the work . Please check out our [ CONTRIBUTING.md ] ( https : //github.com/google/go-github/blob/master/CONTRIBUTING.md ) guide to get started . Thank you !
GitHub Integrations Authentication Workflow __EoT__ Hi , Apart from the addition in # 436 , the new GitHub Integrations ( https : //developer.github.com/early-access/integrations/ ) requires a new authentication workflow , different from OAuth ( https : //developer.github.com/early-access/integrations/authentication/ as well as https : //developer.github.com/early-access/integrations/integrations-vs-oauth-applications/ ) . As many APIs require this installation token ( unique per installation of the integration , expires after short time period ) , I can easily ( and do ) provide a separate ` http.Client ` with a custom ` http.RoundTripper ` adding the necessary authentication headers , but , this requires a separate ` http.Client ` for every Integration installation , which means the underlying tcp connection is not being reused between installations ( although I believe I can workaround that ) . Further there is likely some room here to provide helpers for retrieving the required JWT tokens from a file as this is currently non-trivial and error prone ( although just an example would be sufficient for most people ) . I have n't developed a GitHub OAuth application before , so I 'm not certain if the Integrations flow is any different , but ideally , I would like to be able
Add support for Preview Review Requests API __EoT__ Announcement : https : //developer.github.com/changes/2016-12-16-review-requests-api/
Add support for Preview Review Requests API __EoT__ Announcement : https : //developer.github.com/changes/2016-12-16-review-requests-api/
OrganizationsService.ListPendingOrgInvitations does not work __EoT__ The argument of ` ListPendingOrgInvitations ` is to pass an integer as ` org ` , but this is a mistake and I think you should pass the string correctly .
Add Issue/PR template in CommunityHealthFiles __EoT__ Thanks to this endpoint : https : //developer.github.com/v3/repos/community/ We can get some interesting datas . Type CommunityHealthFiles contain CodeOfConduct , Contributing , License and Readme files , but not issue template and pull requests files . It might be a good idea to include them into CommunityHealthFiles
Add Issue/PR template in CommunityHealthFiles __EoT__ Thanks to this endpoint : https : //developer.github.com/v3/repos/community/ We can get some interesting datas . Type CommunityHealthFiles contain CodeOfConduct , Contributing , License and Readme files , but not issue template and pull requests files . It might be a good idea to include them into CommunityHealthFiles
Add Issue/PR template in CommunityHealthFiles __EoT__ Thanks to this endpoint : https : //developer.github.com/v3/repos/community/ We can get some interesting datas . Type CommunityHealthFiles contain CodeOfConduct , Contributing , License and Readme files , but not issue template and pull requests files . It might be a good idea to include them into CommunityHealthFiles
Proposal : Add GetByID methods to UsersService , RepositoriesService , with caveat . __EoT__ I 'd like to hear your thoughts on the following proposal . I know this project is Go client library for accessing the GitHub API . It tries to stay very current with GitHub API changes . It implements support for new APIs early , even when they 're still in preview period and may result in changes before preview period ends . As a result , this Go package tries to maintain a stable API , but when breaking GitHub API changes happen that necessitate otherwise , it follows suit . I propose considering adding two methods , but read below for details : `` ` Go // GetByID fetches a user . func ( s *UsersService ) GetByID ( id int ) ( *User , *Response , error ) { ... } // GetByID fetches a repository . func ( s *RepositoriesService ) GetByID ( id int ) ( *Repository , *Response , error ) { ... } `` ` # # # Motivation Right now , ` UsersService ` exposes two methods for getting users , one is by ` login ` ,
Proposal : Add GetByID methods to UsersService , RepositoriesService , with caveat . __EoT__ I 'd like to hear your thoughts on the following proposal . I know this project is Go client library for accessing the GitHub API . It tries to stay very current with GitHub API changes . It implements support for new APIs early , even when they 're still in preview period and may result in changes before preview period ends . As a result , this Go package tries to maintain a stable API , but when breaking GitHub API changes happen that necessitate otherwise , it follows suit . I propose considering adding two methods , but read below for details : `` ` Go // GetByID fetches a user . func ( s *UsersService ) GetByID ( id int ) ( *User , *Response , error ) { ... } // GetByID fetches a repository . func ( s *RepositoriesService ) GetByID ( id int ) ( *Repository , *Response , error ) { ... } `` ` # # # Motivation Right now , ` UsersService ` exposes two methods for getting users , one is by ` login ` ,
Adding a simple working program under the Usage section of the README __EoT__ Hello , I am writing a simple Go program which uses this package to make some API calls to GitHub as part of learning Go . It took me some redirections to get started with the package to make native Unauthenticated and Basic authenticated calls ; probably due to my naivety at reading/writing Go code . Hence , I thought of mentioning this so as to help beginners ( like me ) pick the package up quicker in the future . I would like to suggest having at least one complete working code snippet which can be used exclusively as a copy-paste program . This can be achieved in two ways : 1 . Adding a link to the ` examples ` directory in the `` Usage '' section . 2 . Modify the already present examples to be inclusive enough for direct use as recipes . Since I am totally new to Go development environment , I beg pardon if my suggestions seem rather ingenuous to the more experienced community members .
RepositoriesService.ReplaceAllTopics has *Topics parameter , it 'd be better made a value . __EoT__ I just noticed the signature of [ ` RepositoriesService.ReplaceAllTopics ` ] ( https : //godoc.org/github.com/google/go-github/github # RepositoriesService.ReplaceAllTopics ) has a ` *Topic ` parameter . The parameter is not optional , it 's mandatory . See https : //developer.github.com/v3/repos/ # replace-all-topics-for-a-repository : ! [ image ] ( https : //user-images.githubusercontent.com/1924134/32302727-9388673a-bf3a-11e7-8caa-771a70b31af0.png ) There 's no value in making it a pointer rather than value , only harm , because it makes it possible to pass ` nil ` value . I suggest we making a breaking API change to improve the API of ` go-github ` and change the signature of the method to : `` ` Go func ( s *RepositoriesService ) ReplaceAllTopics ( ctx context.Context , owner , repo string , topics Topics ) ( *Topics , *Response , error ) `` ` This is a new method , still under a preview API , so a breaking change is acceptable IMO , as long as it leads to an improve ` go-github ` API . /cc @ gmlewis
Support update to organization and team invitations APIs __EoT__ Announcement : https : //developer.github.com/changes/2017-01-12-organization-team-invitation-update/
Support update to organization and team invitations APIs __EoT__ Announcement : https : //developer.github.com/changes/2017-01-12-organization-team-invitation-update/
Support new Community Health preview API __EoT__ Announcement : https : //developer.github.com/changes/2017-02-09-community-health/ Docs : https : //developer.github.com/v3/repos/community/
Support new Community Health preview API __EoT__ Announcement : https : //developer.github.com/changes/2017-02-09-community-health/ Docs : https : //developer.github.com/v3/repos/community/
Support new Community Health preview API __EoT__ Announcement : https : //developer.github.com/changes/2017-02-09-community-health/ Docs : https : //developer.github.com/v3/repos/community/
Create `` Increment a Decimal-Coded Number '' Question __EoT__
Create sort-scrambled-itinerary question __EoT__
Create sort-scrambled-itinerary question __EoT__
Create sort-scrambled-itinerary question __EoT__
Create sort-scrambled-itinerary question __EoT__
Create sort-scrambled-itinerary question __EoT__
Add Auto-save feature to save codes to localStorage __EoT__ Add Auto-save feature to save codes to localStorage . A follow up feature for # 98
Feedback state lost when switching between windows or reload __EoT__ Expected behavior : When switching between questions , feedback state is maintained . Since code is saved , it would be odd to be able to get back to where you left off in terms of your written code but not be able to get back to where you were in terms of the feedback . Observed behavior : Switching between questions or reloading the page causes the app to lose state on the feedback window . Steps to reproduce : Modify and run code . Observe feedback . Reload page or go to another question and back again . Observe that feedback is reset .
Feedback state lost when switching between windows or reload __EoT__ Expected behavior : When switching between questions , feedback state is maintained . Since code is saved , it would be odd to be able to get back to where you left off in terms of your written code but not be able to get back to where you were in terms of the feedback . Observed behavior : Switching between questions or reloading the page causes the app to lose state on the feedback window . Steps to reproduce : Modify and run code . Observe feedback . Reload page or go to another question and back again . Observe that feedback is reset .
[ UI issue ] Console display starts from middle of the message __EoT__ Repro Steps : 1 . Open `` app.html '' . 2 . Navigate to question 1 . 3 . Change the returned string from `` '' to a random string ( e.g. , `` abcd '' ) which will produce wrong answer . 4 . Click `` Run '' . 5 . The console now shows the input ( `` moo cow bark dog '' ) and the output ( `` abcd '' ) , and the expected output ( `` oom woc krab god '' ) if scrolling down the window . ! [ screen shot 2017-04-03 at 11 49 40 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625396/c1d557a2-1863-11e7-9713-ef834d2e5e24.png ) 6 . Change the returned string from `` abcd '' to another random string ( suppose the user is fixing the bug in the code ) . 7 . Run the code again . 8 . The console shows the expected output , but the actual output is not seen until you scroll up the window . ! [ screen shot 2017-04-03 at 11 51 00 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625434/dbe4f4a4-1863-11e7-88b7-70e1e9ff1c67.png ) Expected behavior : The console
[ UI issue ] Console display starts from middle of the message __EoT__ Repro Steps : 1 . Open `` app.html '' . 2 . Navigate to question 1 . 3 . Change the returned string from `` '' to a random string ( e.g. , `` abcd '' ) which will produce wrong answer . 4 . Click `` Run '' . 5 . The console now shows the input ( `` moo cow bark dog '' ) and the output ( `` abcd '' ) , and the expected output ( `` oom woc krab god '' ) if scrolling down the window . ! [ screen shot 2017-04-03 at 11 49 40 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625396/c1d557a2-1863-11e7-9713-ef834d2e5e24.png ) 6 . Change the returned string from `` abcd '' to another random string ( suppose the user is fixing the bug in the code ) . 7 . Run the code again . 8 . The console shows the expected output , but the actual output is not seen until you scroll up the window . ! [ screen shot 2017-04-03 at 11 51 00 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625434/dbe4f4a4-1863-11e7-88b7-70e1e9ff1c67.png ) Expected behavior : The console
[ UI issue ] Console display starts from middle of the message __EoT__ Repro Steps : 1 . Open `` app.html '' . 2 . Navigate to question 1 . 3 . Change the returned string from `` '' to a random string ( e.g. , `` abcd '' ) which will produce wrong answer . 4 . Click `` Run '' . 5 . The console now shows the input ( `` moo cow bark dog '' ) and the output ( `` abcd '' ) , and the expected output ( `` oom woc krab god '' ) if scrolling down the window . ! [ screen shot 2017-04-03 at 11 49 40 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625396/c1d557a2-1863-11e7-9713-ef834d2e5e24.png ) 6 . Change the returned string from `` abcd '' to another random string ( suppose the user is fixing the bug in the code ) . 7 . Run the code again . 8 . The console shows the expected output , but the actual output is not seen until you scroll up the window . ! [ screen shot 2017-04-03 at 11 51 00 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625434/dbe4f4a4-1863-11e7-88b7-70e1e9ff1c67.png ) Expected behavior : The console
[ UI issue ] Console display starts from middle of the message __EoT__ Repro Steps : 1 . Open `` app.html '' . 2 . Navigate to question 1 . 3 . Change the returned string from `` '' to a random string ( e.g. , `` abcd '' ) which will produce wrong answer . 4 . Click `` Run '' . 5 . The console now shows the input ( `` moo cow bark dog '' ) and the output ( `` abcd '' ) , and the expected output ( `` oom woc krab god '' ) if scrolling down the window . ! [ screen shot 2017-04-03 at 11 49 40 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625396/c1d557a2-1863-11e7-9713-ef834d2e5e24.png ) 6 . Change the returned string from `` abcd '' to another random string ( suppose the user is fixing the bug in the code ) . 7 . Run the code again . 8 . The console shows the expected output , but the actual output is not seen until you scroll up the window . ! [ screen shot 2017-04-03 at 11 51 00 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625434/dbe4f4a4-1863-11e7-88b7-70e1e9ff1c67.png ) Expected behavior : The console
[ UI issue ] Console display starts from middle of the message __EoT__ Repro Steps : 1 . Open `` app.html '' . 2 . Navigate to question 1 . 3 . Change the returned string from `` '' to a random string ( e.g. , `` abcd '' ) which will produce wrong answer . 4 . Click `` Run '' . 5 . The console now shows the input ( `` moo cow bark dog '' ) and the output ( `` abcd '' ) , and the expected output ( `` oom woc krab god '' ) if scrolling down the window . ! [ screen shot 2017-04-03 at 11 49 40 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625396/c1d557a2-1863-11e7-9713-ef834d2e5e24.png ) 6 . Change the returned string from `` abcd '' to another random string ( suppose the user is fixing the bug in the code ) . 7 . Run the code again . 8 . The console shows the expected output , but the actual output is not seen until you scroll up the window . ! [ screen shot 2017-04-03 at 11 51 00 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625434/dbe4f4a4-1863-11e7-88b7-70e1e9ff1c67.png ) Expected behavior : The console
[ UI issue ] Console display starts from middle of the message __EoT__ Repro Steps : 1 . Open `` app.html '' . 2 . Navigate to question 1 . 3 . Change the returned string from `` '' to a random string ( e.g. , `` abcd '' ) which will produce wrong answer . 4 . Click `` Run '' . 5 . The console now shows the input ( `` moo cow bark dog '' ) and the output ( `` abcd '' ) , and the expected output ( `` oom woc krab god '' ) if scrolling down the window . ! [ screen shot 2017-04-03 at 11 49 40 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625396/c1d557a2-1863-11e7-9713-ef834d2e5e24.png ) 6 . Change the returned string from `` abcd '' to another random string ( suppose the user is fixing the bug in the code ) . 7 . Run the code again . 8 . The console shows the expected output , but the actual output is not seen until you scroll up the window . ! [ screen shot 2017-04-03 at 11 51 00 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625434/dbe4f4a4-1863-11e7-88b7-70e1e9ff1c67.png ) Expected behavior : The console
[ UI issue ] Console display starts from middle of the message __EoT__ Repro Steps : 1 . Open `` app.html '' . 2 . Navigate to question 1 . 3 . Change the returned string from `` '' to a random string ( e.g. , `` abcd '' ) which will produce wrong answer . 4 . Click `` Run '' . 5 . The console now shows the input ( `` moo cow bark dog '' ) and the output ( `` abcd '' ) , and the expected output ( `` oom woc krab god '' ) if scrolling down the window . ! [ screen shot 2017-04-03 at 11 49 40 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625396/c1d557a2-1863-11e7-9713-ef834d2e5e24.png ) 6 . Change the returned string from `` abcd '' to another random string ( suppose the user is fixing the bug in the code ) . 7 . Run the code again . 8 . The console shows the expected output , but the actual output is not seen until you scroll up the window . ! [ screen shot 2017-04-03 at 11 51 00 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625434/dbe4f4a4-1863-11e7-88b7-70e1e9ff1c67.png ) Expected behavior : The console
[ UI issue ] Console display starts from middle of the message __EoT__ Repro Steps : 1 . Open `` app.html '' . 2 . Navigate to question 1 . 3 . Change the returned string from `` '' to a random string ( e.g. , `` abcd '' ) which will produce wrong answer . 4 . Click `` Run '' . 5 . The console now shows the input ( `` moo cow bark dog '' ) and the output ( `` abcd '' ) , and the expected output ( `` oom woc krab god '' ) if scrolling down the window . ! [ screen shot 2017-04-03 at 11 49 40 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625396/c1d557a2-1863-11e7-9713-ef834d2e5e24.png ) 6 . Change the returned string from `` abcd '' to another random string ( suppose the user is fixing the bug in the code ) . 7 . Run the code again . 8 . The console shows the expected output , but the actual output is not seen until you scroll up the window . ! [ screen shot 2017-04-03 at 11 51 00 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625434/dbe4f4a4-1863-11e7-88b7-70e1e9ff1c67.png ) Expected behavior : The console
[ UI issue ] Console display starts from middle of the message __EoT__ Repro Steps : 1 . Open `` app.html '' . 2 . Navigate to question 1 . 3 . Change the returned string from `` '' to a random string ( e.g. , `` abcd '' ) which will produce wrong answer . 4 . Click `` Run '' . 5 . The console now shows the input ( `` moo cow bark dog '' ) and the output ( `` abcd '' ) , and the expected output ( `` oom woc krab god '' ) if scrolling down the window . ! [ screen shot 2017-04-03 at 11 49 40 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625396/c1d557a2-1863-11e7-9713-ef834d2e5e24.png ) 6 . Change the returned string from `` abcd '' to another random string ( suppose the user is fixing the bug in the code ) . 7 . Run the code again . 8 . The console shows the expected output , but the actual output is not seen until you scroll up the window . ! [ screen shot 2017-04-03 at 11 51 00 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625434/dbe4f4a4-1863-11e7-88b7-70e1e9ff1c67.png ) Expected behavior : The console
[ UI issue ] Console display starts from middle of the message __EoT__ Repro Steps : 1 . Open `` app.html '' . 2 . Navigate to question 1 . 3 . Change the returned string from `` '' to a random string ( e.g. , `` abcd '' ) which will produce wrong answer . 4 . Click `` Run '' . 5 . The console now shows the input ( `` moo cow bark dog '' ) and the output ( `` abcd '' ) , and the expected output ( `` oom woc krab god '' ) if scrolling down the window . ! [ screen shot 2017-04-03 at 11 49 40 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625396/c1d557a2-1863-11e7-9713-ef834d2e5e24.png ) 6 . Change the returned string from `` abcd '' to another random string ( suppose the user is fixing the bug in the code ) . 7 . Run the code again . 8 . The console shows the expected output , but the actual output is not seen until you scroll up the window . ! [ screen shot 2017-04-03 at 11 51 00 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625434/dbe4f4a4-1863-11e7-88b7-70e1e9ff1c67.png ) Expected behavior : The console
[ UI issue ] Console display starts from middle of the message __EoT__ Repro Steps : 1 . Open `` app.html '' . 2 . Navigate to question 1 . 3 . Change the returned string from `` '' to a random string ( e.g. , `` abcd '' ) which will produce wrong answer . 4 . Click `` Run '' . 5 . The console now shows the input ( `` moo cow bark dog '' ) and the output ( `` abcd '' ) , and the expected output ( `` oom woc krab god '' ) if scrolling down the window . ! [ screen shot 2017-04-03 at 11 49 40 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625396/c1d557a2-1863-11e7-9713-ef834d2e5e24.png ) 6 . Change the returned string from `` abcd '' to another random string ( suppose the user is fixing the bug in the code ) . 7 . Run the code again . 8 . The console shows the expected output , but the actual output is not seen until you scroll up the window . ! [ screen shot 2017-04-03 at 11 51 00 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625434/dbe4f4a4-1863-11e7-88b7-70e1e9ff1c67.png ) Expected behavior : The console
[ UI issue ] Console display starts from middle of the message __EoT__ Repro Steps : 1 . Open `` app.html '' . 2 . Navigate to question 1 . 3 . Change the returned string from `` '' to a random string ( e.g. , `` abcd '' ) which will produce wrong answer . 4 . Click `` Run '' . 5 . The console now shows the input ( `` moo cow bark dog '' ) and the output ( `` abcd '' ) , and the expected output ( `` oom woc krab god '' ) if scrolling down the window . ! [ screen shot 2017-04-03 at 11 49 40 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625396/c1d557a2-1863-11e7-9713-ef834d2e5e24.png ) 6 . Change the returned string from `` abcd '' to another random string ( suppose the user is fixing the bug in the code ) . 7 . Run the code again . 8 . The console shows the expected output , but the actual output is not seen until you scroll up the window . ! [ screen shot 2017-04-03 at 11 51 00 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625434/dbe4f4a4-1863-11e7-88b7-70e1e9ff1c67.png ) Expected behavior : The console
[ UI issue ] Console display starts from middle of the message __EoT__ Repro Steps : 1 . Open `` app.html '' . 2 . Navigate to question 1 . 3 . Change the returned string from `` '' to a random string ( e.g. , `` abcd '' ) which will produce wrong answer . 4 . Click `` Run '' . 5 . The console now shows the input ( `` moo cow bark dog '' ) and the output ( `` abcd '' ) , and the expected output ( `` oom woc krab god '' ) if scrolling down the window . ! [ screen shot 2017-04-03 at 11 49 40 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625396/c1d557a2-1863-11e7-9713-ef834d2e5e24.png ) 6 . Change the returned string from `` abcd '' to another random string ( suppose the user is fixing the bug in the code ) . 7 . Run the code again . 8 . The console shows the expected output , but the actual output is not seen until you scroll up the window . ! [ screen shot 2017-04-03 at 11 51 00 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625434/dbe4f4a4-1863-11e7-88b7-70e1e9ff1c67.png ) Expected behavior : The console
[ UI issue ] Console display starts from middle of the message __EoT__ Repro Steps : 1 . Open `` app.html '' . 2 . Navigate to question 1 . 3 . Change the returned string from `` '' to a random string ( e.g. , `` abcd '' ) which will produce wrong answer . 4 . Click `` Run '' . 5 . The console now shows the input ( `` moo cow bark dog '' ) and the output ( `` abcd '' ) , and the expected output ( `` oom woc krab god '' ) if scrolling down the window . ! [ screen shot 2017-04-03 at 11 49 40 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625396/c1d557a2-1863-11e7-9713-ef834d2e5e24.png ) 6 . Change the returned string from `` abcd '' to another random string ( suppose the user is fixing the bug in the code ) . 7 . Run the code again . 8 . The console shows the expected output , but the actual output is not seen until you scroll up the window . ! [ screen shot 2017-04-03 at 11 51 00 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625434/dbe4f4a4-1863-11e7-88b7-70e1e9ff1c67.png ) Expected behavior : The console
Description of question 4 : Run-Length Encoding __EoT__ In the description of question 4 : ! [ image ] ( https : //cloud.githubusercontent.com/assets/7587606/24636743/332a99e4-1891-11e7-957f-e5221352e0b9.png ) '' abcccccd '' should be encoded as `` ab5xcd '' but not `` ab5xc ''
Description of question 4 : Run-Length Encoding __EoT__ In the description of question 4 : ! [ image ] ( https : //cloud.githubusercontent.com/assets/7587606/24636743/332a99e4-1891-11e7-957f-e5221352e0b9.png ) '' abcccccd '' should be encoded as `` ab5xcd '' but not `` ab5xc ''
Description of question 4 : Run-Length Encoding __EoT__ In the description of question 4 : ! [ image ] ( https : //cloud.githubusercontent.com/assets/7587606/24636743/332a99e4-1891-11e7-957f-e5221352e0b9.png ) '' abcccccd '' should be encoded as `` ab5xcd '' but not `` ab5xc ''
Description of question 4 : Run-Length Encoding __EoT__ In the description of question 4 : ! [ image ] ( https : //cloud.githubusercontent.com/assets/7587606/24636743/332a99e4-1891-11e7-957f-e5221352e0b9.png ) '' abcccccd '' should be encoded as `` ab5xcd '' but not `` ab5xc ''
[ Question Schema ] No Checks on Previous Tasks __EoT__ The Judge does not test my code for tasks that I 've passed before . For example , on Question 2 , my code went through Task One smoothly but had bugs on Task Two . Then I made some changes so as to pass the tests for Task Two . Even through now my code failed on the tests of Task One , the Online Judge would not find out , but instead moved me forward to the third task . Such design seemed quite inappropriate to me , because the next round task , based on the previous one , encourages challengers to generalize their codes , rather than allowing them to pass the tasks individually .
When a question is completed , change its representation from a number to a check mark . __EoT__ In the stepper at the top of the page , it would be nice to show progress more clearly . One UI suggestion that we received is to change the number in the circle to a green check mark when a question is fully completed ; this seems like a nice thing to do . /cc @ rabidbit
When a question is completed , change its representation from a number to a check mark . __EoT__ In the stepper at the top of the page , it would be nice to show progress more clearly . One UI suggestion that we received is to change the number in the circle to a green check mark when a question is fully completed ; this seems like a nice thing to do . /cc @ rabidbit
When a question is completed , change its representation from a number to a check mark . __EoT__ In the stepper at the top of the page , it would be nice to show progress more clearly . One UI suggestion that we received is to change the number in the circle to a green check mark when a question is fully completed ; this seems like a nice thing to do . /cc @ rabidbit
When a question is completed , change its representation from a number to a check mark . __EoT__ In the stepper at the top of the page , it would be nice to show progress more clearly . One UI suggestion that we received is to change the number in the circle to a green check mark when a question is fully completed ; this seems like a nice thing to do . /cc @ rabidbit
When a question is completed , change its representation from a number to a check mark . __EoT__ In the stepper at the top of the page , it would be nice to show progress more clearly . One UI suggestion that we received is to change the number in the circle to a green check mark when a question is fully completed ; this seems like a nice thing to do . /cc @ rabidbit
When a question is completed , change its representation from a number to a check mark . __EoT__ In the stepper at the top of the page , it would be nice to show progress more clearly . One UI suggestion that we received is to change the number in the circle to a green check mark when a question is fully completed ; this seems like a nice thing to do . /cc @ rabidbit
When a question is completed , change its representation from a number to a check mark . __EoT__ In the stepper at the top of the page , it would be nice to show progress more clearly . One UI suggestion that we received is to change the number in the circle to a green check mark when a question is fully completed ; this seems like a nice thing to do . /cc @ rabidbit
When a question is completed , change its representation from a number to a check mark . __EoT__ In the stepper at the top of the page , it would be nice to show progress more clearly . One UI suggestion that we received is to change the number in the circle to a green check mark when a question is fully completed ; this seems like a nice thing to do . /cc @ rabidbit
When a question is completed , change its representation from a number to a check mark . __EoT__ In the stepper at the top of the page , it would be nice to show progress more clearly . One UI suggestion that we received is to change the number in the circle to a green check mark when a question is fully completed ; this seems like a nice thing to do . /cc @ rabidbit
Add tests in client/services __EoT__ - [ x ] FeedbackGeneratorService.js - [ x ] QuestionDataService.js - [ ] SolutionHandlerService.js ( Unable to work on this now )
Add tests in client/services __EoT__ - [ x ] FeedbackGeneratorService.js - [ x ] QuestionDataService.js - [ ] SolutionHandlerService.js ( Unable to work on this now )
Add tests in client/services __EoT__ - [ x ] FeedbackGeneratorService.js - [ x ] QuestionDataService.js - [ ] SolutionHandlerService.js ( Unable to work on this now )
Update/replace syntax error UI __EoT__ Remove syntax error history and replace it with something that occupies less space ( e.g. , consider a `` spell-checker '' UI with squiggly lines under the problematic code and an error message displayed when user clicks on the code snippet ) .
Change `` New to Python '' button to a link __EoT__ Some users were unsure if they should press the `` New to Python '' button because they were not sure if it would change something in the application ( versus simply opening a page ) . Changing this to a link should make it easier for the user to understand what they should expect ( a link to some page vs a button that could change something ) .
time.time used in performance tests . __EoT__ This is the most general method , but also probably the least accurate choice for the purposes of testing algorithm performance , which may account for some of the wonkyness of the performance tests . time.perf_counter would seem to be the most appropriate choice , being the highest available resolution and being meant for short duration timing rather than long running or user-facing clocks . However , this is only available in python 3 . ( I have n't quite figured out which python version we 're using yet . ) if we 're using 2.7 , time.clock may be a better option , as it seems to have better resolution . Apparently this is the function used to analyze python library algos . https : //docs.python.org/2.7/library/time.html https : //docs.python.org/3.6/library/time.html
Arrays as input are not currently supported . __EoT__ For are_all_unique , it takes in an array as an argument , but that 's not currently supported , so jsonVariableToPython crashes . Launch app.html Navigate to i18n question Complete tasks up to first instance of filling in are_all_unique App should crash .
Arrays as input are not currently supported . __EoT__ For are_all_unique , it takes in an array as an argument , but that 's not currently supported , so jsonVariableToPython crashes . Launch app.html Navigate to i18n question Complete tasks up to first instance of filling in are_all_unique App should crash .
Implement inputFunctionName feature for correctnessTest and buggyOutputTest __EoT__ We are creating the question of `` find closest value in binary search tree '' , and we need to use the inputFunctionName feature to transform string to BST , and it has n't been implemented . So we are working on it .
Code can not be saved in the webpage __EoT__ I finished the question 2 ( compiled successfully and got tests passed ) and moved forward to question 3 . When I went back to question 2 ( click `` 2 '' in the process bar ) to review the code , the code has been reset and my code was not saved .
Code can not be saved in the webpage __EoT__ I finished the question 2 ( compiled successfully and got tests passed ) and moved forward to question 3 . When I went back to question 2 ( click `` 2 '' in the process bar ) to review the code , the code has been reset and my code was not saved .
Code can not be saved in the webpage __EoT__ I finished the question 2 ( compiled successfully and got tests passed ) and moved forward to question 3 . When I went back to question 2 ( click `` 2 '' in the process bar ) to review the code , the code has been reset and my code was not saved .
Code can not be saved in the webpage __EoT__ I finished the question 2 ( compiled successfully and got tests passed ) and moved forward to question 3 . When I went back to question 2 ( click `` 2 '' in the process bar ) to review the code , the code has been reset and my code was not saved .
Code can not be saved in the webpage __EoT__ I finished the question 2 ( compiled successfully and got tests passed ) and moved forward to question 3 . When I went back to question 2 ( click `` 2 '' in the process bar ) to review the code , the code has been reset and my code was not saved .
Code can not be saved in the webpage __EoT__ I finished the question 2 ( compiled successfully and got tests passed ) and moved forward to question 3 . When I went back to question 2 ( click `` 2 '' in the process bar ) to review the code , the code has been reset and my code was not saved .
Code can not be saved in the webpage __EoT__ I finished the question 2 ( compiled successfully and got tests passed ) and moved forward to question 3 . When I went back to question 2 ( click `` 2 '' in the process bar ) to review the code , the code has been reset and my code was not saved .
Code can not be saved in the webpage __EoT__ I finished the question 2 ( compiled successfully and got tests passed ) and moved forward to question 3 . When I went back to question 2 ( click `` 2 '' in the process bar ) to review the code , the code has been reset and my code was not saved .
Code can not be saved in the webpage __EoT__ I finished the question 2 ( compiled successfully and got tests passed ) and moved forward to question 3 . When I went back to question 2 ( click `` 2 '' in the process bar ) to review the code , the code has been reset and my code was not saved .
Error messages are not interpolated . __EoT__ Repro instructions : Go to the reverseWords problem , and type the code seen in the screenshot below . In the screenshot below , the feedback message is not interpolated , so it is meaningless . We should either support interpolation or choose a different feedback message . ! [ screenshot from 2017-03-01 15 41 42 ] ( https : //cloud.githubusercontent.com/assets/10575562/23486620/0c5bdf2c-fe96-11e6-8409-ecfa63f240af.png )
Create Custom Modal Component __EoT__ **Steps for Reproduction : ** * Pull down ` add-privacy-notice ` branch ( if not already merged into ` master ` branch ) . * Open the ` app.html ` file in Chrome * Click on link labeled ` Privacy ` in the bottom right corner . * Notice that a pop-us is displayed with a `` This page says ... '' title at the top . The issue is that we do n't want this title to appear , but since it 's a browser-specific behavior , we need to make our own custom modals that can ideally be further extended to other use cases ( i.e . Clippy pop ups ) .
Create Custom Modal Component __EoT__ **Steps for Reproduction : ** * Pull down ` add-privacy-notice ` branch ( if not already merged into ` master ` branch ) . * Open the ` app.html ` file in Chrome * Click on link labeled ` Privacy ` in the bottom right corner . * Notice that a pop-us is displayed with a `` This page says ... '' title at the top . The issue is that we do n't want this title to appear , but since it 's a browser-specific behavior , we need to make our own custom modals that can ideally be further extended to other use cases ( i.e . Clippy pop ups ) .
Create Custom Modal Component __EoT__ **Steps for Reproduction : ** * Pull down ` add-privacy-notice ` branch ( if not already merged into ` master ` branch ) . * Open the ` app.html ` file in Chrome * Click on link labeled ` Privacy ` in the bottom right corner . * Notice that a pop-us is displayed with a `` This page says ... '' title at the top . The issue is that we do n't want this title to appear , but since it 's a browser-specific behavior , we need to make our own custom modals that can ideally be further extended to other use cases ( i.e . Clippy pop ups ) .
Give users positive reinforcement about anticipating edge cases and feedback on failing tests __EoT__ When a user 's code passes a broad category of tests ( e.g . short inputs ) we would like to give positive reinforcement for their work . Also , if tests that used to pass stop working , we would like to remind them that their code needs to be fixed . We would like to do this by adding tags to the test inputs that group them into relevant categories ( e.g . `` the general case '' , `` short inputs '' , `` large inputs '' , `` sorted inputs '' , etc. ) . Then , all inputs would need to be evaluated with each run in order to determine the tags that are passing and the tags that are still failing . Design doc is here : https : //goto.google.com/tie-positive Suggested implementation : 1 . Maintain a list of tags whose tests are all passing ( passing_tags_list starts off empty ) and a list of failing tests that are displayed in the output ( displayed_failing_tests_list starts off empty ) 2 . Run all the tests compiling a list of all
Give users positive reinforcement about anticipating edge cases and feedback on failing tests __EoT__ When a user 's code passes a broad category of tests ( e.g . short inputs ) we would like to give positive reinforcement for their work . Also , if tests that used to pass stop working , we would like to remind them that their code needs to be fixed . We would like to do this by adding tags to the test inputs that group them into relevant categories ( e.g . `` the general case '' , `` short inputs '' , `` large inputs '' , `` sorted inputs '' , etc. ) . Then , all inputs would need to be evaluated with each run in order to determine the tags that are passing and the tags that are still failing . Design doc is here : https : //goto.google.com/tie-positive Suggested implementation : 1 . Maintain a list of tags whose tests are all passing ( passing_tags_list starts off empty ) and a list of failing tests that are displayed in the output ( displayed_failing_tests_list starts off empty ) 2 . Run all the tests compiling a list of all
Give users positive reinforcement about anticipating edge cases and feedback on failing tests __EoT__ When a user 's code passes a broad category of tests ( e.g . short inputs ) we would like to give positive reinforcement for their work . Also , if tests that used to pass stop working , we would like to remind them that their code needs to be fixed . We would like to do this by adding tags to the test inputs that group them into relevant categories ( e.g . `` the general case '' , `` short inputs '' , `` large inputs '' , `` sorted inputs '' , etc. ) . Then , all inputs would need to be evaluated with each run in order to determine the tags that are passing and the tags that are still failing . Design doc is here : https : //goto.google.com/tie-positive Suggested implementation : 1 . Maintain a list of tags whose tests are all passing ( passing_tags_list starts off empty ) and a list of failing tests that are displayed in the output ( displayed_failing_tests_list starts off empty ) 2 . Run all the tests compiling a list of all
Give users positive reinforcement about anticipating edge cases and feedback on failing tests __EoT__ When a user 's code passes a broad category of tests ( e.g . short inputs ) we would like to give positive reinforcement for their work . Also , if tests that used to pass stop working , we would like to remind them that their code needs to be fixed . We would like to do this by adding tags to the test inputs that group them into relevant categories ( e.g . `` the general case '' , `` short inputs '' , `` large inputs '' , `` sorted inputs '' , etc. ) . Then , all inputs would need to be evaluated with each run in order to determine the tags that are passing and the tags that are still failing . Design doc is here : https : //goto.google.com/tie-positive Suggested implementation : 1 . Maintain a list of tags whose tests are all passing ( passing_tags_list starts off empty ) and a list of failing tests that are displayed in the output ( displayed_failing_tests_list starts off empty ) 2 . Run all the tests compiling a list of all
Give users positive reinforcement about anticipating edge cases and feedback on failing tests __EoT__ When a user 's code passes a broad category of tests ( e.g . short inputs ) we would like to give positive reinforcement for their work . Also , if tests that used to pass stop working , we would like to remind them that their code needs to be fixed . We would like to do this by adding tags to the test inputs that group them into relevant categories ( e.g . `` the general case '' , `` short inputs '' , `` large inputs '' , `` sorted inputs '' , etc. ) . Then , all inputs would need to be evaluated with each run in order to determine the tags that are passing and the tags that are still failing . Design doc is here : https : //goto.google.com/tie-positive Suggested implementation : 1 . Maintain a list of tags whose tests are all passing ( passing_tags_list starts off empty ) and a list of failing tests that are displayed in the output ( displayed_failing_tests_list starts off empty ) 2 . Run all the tests compiling a list of all
User study - Include line number in language detection __EoT__ When issue # 413 occurred , it was very hard to know what part of the code was being referenced . Adding a line number to the feedback would help the user better understand where the problem is and also helps to better troubleshoot a potential bug .
User study - Include line number in language detection __EoT__ When issue # 413 occurred , it was very hard to know what part of the code was being referenced . Adding a line number to the feedback would help the user better understand where the problem is and also helps to better troubleshoot a potential bug .
User study - Include line number in language detection __EoT__ When issue # 413 occurred , it was very hard to know what part of the code was being referenced . Adding a line number to the feedback would help the user better understand where the problem is and also helps to better troubleshoot a potential bug .
Create Bomberman Question __EoT__ Working on creating this question in branch : bomberman_question
Create Bomberman Question __EoT__ Working on creating this question in branch : bomberman_question
Write remaining Karma unit tests for the frontend . __EoT__ Karma tests still need to be added for : - [ ] client/services/SolutionHandlerService.js - [ ] client/services/code_evaluators/PythonCodeRunnerService.js Both of these require figuring out how to use the Sk library in tests .
Write remaining Karma unit tests for the frontend . __EoT__ Karma tests still need to be added for : - [ ] client/services/SolutionHandlerService.js - [ ] client/services/code_evaluators/PythonCodeRunnerService.js Both of these require figuring out how to use the Sk library in tests .
Write remaining Karma unit tests for the frontend . __EoT__ Karma tests still need to be added for : - [ ] client/services/SolutionHandlerService.js - [ ] client/services/code_evaluators/PythonCodeRunnerService.js Both of these require figuring out how to use the Sk library in tests .
No auto indentation for Python __EoT__ Hi , The current version of TIE does not do auto-indentation when there is a new line after 'def ' , 'if ' or 'where ' clauses . It just aligns the new line with the previous one . It may be more user-friendly and save some time if this feature is included , since it is not convenient for users to count spaces themselves . I captured two screenshots to compare between TIE and Leetcode . ! [ screen shot 2017-04-03 at 6 31 24 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638265/eda11898-189b-11e7-8721-0c0097d9f1f6.png ) ! [ screen shot 2017-04-03 at 6 32 25 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638264/ed9fa0b2-189b-11e7-9035-1d2ff08feafc.png )
No auto indentation for Python __EoT__ Hi , The current version of TIE does not do auto-indentation when there is a new line after 'def ' , 'if ' or 'where ' clauses . It just aligns the new line with the previous one . It may be more user-friendly and save some time if this feature is included , since it is not convenient for users to count spaces themselves . I captured two screenshots to compare between TIE and Leetcode . ! [ screen shot 2017-04-03 at 6 31 24 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638265/eda11898-189b-11e7-8721-0c0097d9f1f6.png ) ! [ screen shot 2017-04-03 at 6 32 25 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638264/ed9fa0b2-189b-11e7-9035-1d2ff08feafc.png )
No auto indentation for Python __EoT__ Hi , The current version of TIE does not do auto-indentation when there is a new line after 'def ' , 'if ' or 'where ' clauses . It just aligns the new line with the previous one . It may be more user-friendly and save some time if this feature is included , since it is not convenient for users to count spaces themselves . I captured two screenshots to compare between TIE and Leetcode . ! [ screen shot 2017-04-03 at 6 31 24 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638265/eda11898-189b-11e7-8721-0c0097d9f1f6.png ) ! [ screen shot 2017-04-03 at 6 32 25 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638264/ed9fa0b2-189b-11e7-9035-1d2ff08feafc.png )
No auto indentation for Python __EoT__ Hi , The current version of TIE does not do auto-indentation when there is a new line after 'def ' , 'if ' or 'where ' clauses . It just aligns the new line with the previous one . It may be more user-friendly and save some time if this feature is included , since it is not convenient for users to count spaces themselves . I captured two screenshots to compare between TIE and Leetcode . ! [ screen shot 2017-04-03 at 6 31 24 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638265/eda11898-189b-11e7-8721-0c0097d9f1f6.png ) ! [ screen shot 2017-04-03 at 6 32 25 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638264/ed9fa0b2-189b-11e7-9035-1d2ff08feafc.png )
No auto indentation for Python __EoT__ Hi , The current version of TIE does not do auto-indentation when there is a new line after 'def ' , 'if ' or 'where ' clauses . It just aligns the new line with the previous one . It may be more user-friendly and save some time if this feature is included , since it is not convenient for users to count spaces themselves . I captured two screenshots to compare between TIE and Leetcode . ! [ screen shot 2017-04-03 at 6 31 24 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638265/eda11898-189b-11e7-8721-0c0097d9f1f6.png ) ! [ screen shot 2017-04-03 at 6 32 25 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638264/ed9fa0b2-189b-11e7-9035-1d2ff08feafc.png )
No auto indentation for Python __EoT__ Hi , The current version of TIE does not do auto-indentation when there is a new line after 'def ' , 'if ' or 'where ' clauses . It just aligns the new line with the previous one . It may be more user-friendly and save some time if this feature is included , since it is not convenient for users to count spaces themselves . I captured two screenshots to compare between TIE and Leetcode . ! [ screen shot 2017-04-03 at 6 31 24 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638265/eda11898-189b-11e7-8721-0c0097d9f1f6.png ) ! [ screen shot 2017-04-03 at 6 32 25 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638264/ed9fa0b2-189b-11e7-9035-1d2ff08feafc.png )
No auto indentation for Python __EoT__ Hi , The current version of TIE does not do auto-indentation when there is a new line after 'def ' , 'if ' or 'where ' clauses . It just aligns the new line with the previous one . It may be more user-friendly and save some time if this feature is included , since it is not convenient for users to count spaces themselves . I captured two screenshots to compare between TIE and Leetcode . ! [ screen shot 2017-04-03 at 6 31 24 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638265/eda11898-189b-11e7-8721-0c0097d9f1f6.png ) ! [ screen shot 2017-04-03 at 6 32 25 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638264/ed9fa0b2-189b-11e7-9035-1d2ff08feafc.png )
No auto indentation for Python __EoT__ Hi , The current version of TIE does not do auto-indentation when there is a new line after 'def ' , 'if ' or 'where ' clauses . It just aligns the new line with the previous one . It may be more user-friendly and save some time if this feature is included , since it is not convenient for users to count spaces themselves . I captured two screenshots to compare between TIE and Leetcode . ! [ screen shot 2017-04-03 at 6 31 24 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638265/eda11898-189b-11e7-8721-0c0097d9f1f6.png ) ! [ screen shot 2017-04-03 at 6 32 25 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638264/ed9fa0b2-189b-11e7-9035-1d2ff08feafc.png )
No auto indentation for Python __EoT__ Hi , The current version of TIE does not do auto-indentation when there is a new line after 'def ' , 'if ' or 'where ' clauses . It just aligns the new line with the previous one . It may be more user-friendly and save some time if this feature is included , since it is not convenient for users to count spaces themselves . I captured two screenshots to compare between TIE and Leetcode . ! [ screen shot 2017-04-03 at 6 31 24 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638265/eda11898-189b-11e7-8721-0c0097d9f1f6.png ) ! [ screen shot 2017-04-03 at 6 32 25 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638264/ed9fa0b2-189b-11e7-9035-1d2ff08feafc.png )
No auto indentation for Python __EoT__ Hi , The current version of TIE does not do auto-indentation when there is a new line after 'def ' , 'if ' or 'where ' clauses . It just aligns the new line with the previous one . It may be more user-friendly and save some time if this feature is included , since it is not convenient for users to count spaces themselves . I captured two screenshots to compare between TIE and Leetcode . ! [ screen shot 2017-04-03 at 6 31 24 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638265/eda11898-189b-11e7-8721-0c0097d9f1f6.png ) ! [ screen shot 2017-04-03 at 6 32 25 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638264/ed9fa0b2-189b-11e7-9035-1d2ff08feafc.png )
No auto indentation for Python __EoT__ Hi , The current version of TIE does not do auto-indentation when there is a new line after 'def ' , 'if ' or 'where ' clauses . It just aligns the new line with the previous one . It may be more user-friendly and save some time if this feature is included , since it is not convenient for users to count spaces themselves . I captured two screenshots to compare between TIE and Leetcode . ! [ screen shot 2017-04-03 at 6 31 24 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638265/eda11898-189b-11e7-8721-0c0097d9f1f6.png ) ! [ screen shot 2017-04-03 at 6 32 25 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638264/ed9fa0b2-189b-11e7-9035-1d2ff08feafc.png )
No auto indentation for Python __EoT__ Hi , The current version of TIE does not do auto-indentation when there is a new line after 'def ' , 'if ' or 'where ' clauses . It just aligns the new line with the previous one . It may be more user-friendly and save some time if this feature is included , since it is not convenient for users to count spaces themselves . I captured two screenshots to compare between TIE and Leetcode . ! [ screen shot 2017-04-03 at 6 31 24 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638265/eda11898-189b-11e7-8721-0c0097d9f1f6.png ) ! [ screen shot 2017-04-03 at 6 32 25 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638264/ed9fa0b2-189b-11e7-9035-1d2ff08feafc.png )
( a ) Make it more clear what language the coding window is in and ( b ) Not clear that 's ' should be treated as a string . __EoT__
Maintain feedback history when moving on to the next task / question . __EoT__ Maintain feedback history when moving on to the next task / question . Currently , feedback history is cleared when moving to the next question . User studies show users prefer to either maintain the history or give the option to clear the history if the user would like .
Maintain feedback history when moving on to the next task / question . __EoT__ Maintain feedback history when moving on to the next task / question . Currently , feedback history is cleared when moving to the next question . User studies show users prefer to either maintain the history or give the option to clear the history if the user would like .
add tests in client/services/code_evaluators/ __EoT__ - [ x ] CodeRunnerDispatcherService.js ( one line uncovered due to failure to use Sk library ) - [ ] PythonCodeRunnerService.js ( blocked by failure to use Sk library )
add tests in client/services/code_evaluators/ __EoT__ - [ x ] CodeRunnerDispatcherService.js ( one line uncovered due to failure to use Sk library ) - [ ] PythonCodeRunnerService.js ( blocked by failure to use Sk library )
add tests in client/services/code_evaluators/ __EoT__ - [ x ] CodeRunnerDispatcherService.js ( one line uncovered due to failure to use Sk library ) - [ ] PythonCodeRunnerService.js ( blocked by failure to use Sk library )
Allow user to change size of code editing area __EoT__ From feedback received in # 112 , users would like to be able to change the size of the code editing area in a way that is similar to the other boxes on the page .
Question Title UI Modifications for better space utilization and prominence __EoT__ **Description : ** Every question title has a prefix `` Question # : '' in it . This is redundant as the question index is already being displayed in the navigation bar on the top . The highlighted index number displays the current question number . So , we can remove the `` Question # : '' prefix from the question title . Also , the CSS of the question title can be modified to make it more prominent . **Current state : ** < img width= '' 1343 '' alt= '' screen shot 2017-04-10 at 10 37 54 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24874548/e5e3dfee-1dd9-11e7-8fbd-f5ea96e4646d.png '' > **Suggested Modification : ** The following screenshot is just a mock-up of the suggested modifications.We can have a discussion about the best way to approach the same . < img width= '' 1354 '' alt= '' screen shot 2017-04-07 at 10 25 59 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24873848/4918f228-1dd7-11e7-89d5-531589e98759.png '' >
Question Title UI Modifications for better space utilization and prominence __EoT__ **Description : ** Every question title has a prefix `` Question # : '' in it . This is redundant as the question index is already being displayed in the navigation bar on the top . The highlighted index number displays the current question number . So , we can remove the `` Question # : '' prefix from the question title . Also , the CSS of the question title can be modified to make it more prominent . **Current state : ** < img width= '' 1343 '' alt= '' screen shot 2017-04-10 at 10 37 54 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24874548/e5e3dfee-1dd9-11e7-8fbd-f5ea96e4646d.png '' > **Suggested Modification : ** The following screenshot is just a mock-up of the suggested modifications.We can have a discussion about the best way to approach the same . < img width= '' 1354 '' alt= '' screen shot 2017-04-07 at 10 25 59 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24873848/4918f228-1dd7-11e7-89d5-531589e98759.png '' >
Question Title UI Modifications for better space utilization and prominence __EoT__ **Description : ** Every question title has a prefix `` Question # : '' in it . This is redundant as the question index is already being displayed in the navigation bar on the top . The highlighted index number displays the current question number . So , we can remove the `` Question # : '' prefix from the question title . Also , the CSS of the question title can be modified to make it more prominent . **Current state : ** < img width= '' 1343 '' alt= '' screen shot 2017-04-10 at 10 37 54 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24874548/e5e3dfee-1dd9-11e7-8fbd-f5ea96e4646d.png '' > **Suggested Modification : ** The following screenshot is just a mock-up of the suggested modifications.We can have a discussion about the best way to approach the same . < img width= '' 1354 '' alt= '' screen shot 2017-04-07 at 10 25 59 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24873848/4918f228-1dd7-11e7-89d5-531589e98759.png '' >
Question Title UI Modifications for better space utilization and prominence __EoT__ **Description : ** Every question title has a prefix `` Question # : '' in it . This is redundant as the question index is already being displayed in the navigation bar on the top . The highlighted index number displays the current question number . So , we can remove the `` Question # : '' prefix from the question title . Also , the CSS of the question title can be modified to make it more prominent . **Current state : ** < img width= '' 1343 '' alt= '' screen shot 2017-04-10 at 10 37 54 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24874548/e5e3dfee-1dd9-11e7-8fbd-f5ea96e4646d.png '' > **Suggested Modification : ** The following screenshot is just a mock-up of the suggested modifications.We can have a discussion about the best way to approach the same . < img width= '' 1354 '' alt= '' screen shot 2017-04-07 at 10 25 59 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24873848/4918f228-1dd7-11e7-89d5-531589e98759.png '' >
Question Title UI Modifications for better space utilization and prominence __EoT__ **Description : ** Every question title has a prefix `` Question # : '' in it . This is redundant as the question index is already being displayed in the navigation bar on the top . The highlighted index number displays the current question number . So , we can remove the `` Question # : '' prefix from the question title . Also , the CSS of the question title can be modified to make it more prominent . **Current state : ** < img width= '' 1343 '' alt= '' screen shot 2017-04-10 at 10 37 54 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24874548/e5e3dfee-1dd9-11e7-8fbd-f5ea96e4646d.png '' > **Suggested Modification : ** The following screenshot is just a mock-up of the suggested modifications.We can have a discussion about the best way to approach the same . < img width= '' 1354 '' alt= '' screen shot 2017-04-07 at 10 25 59 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24873848/4918f228-1dd7-11e7-89d5-531589e98759.png '' >
Question Title UI Modifications for better space utilization and prominence __EoT__ **Description : ** Every question title has a prefix `` Question # : '' in it . This is redundant as the question index is already being displayed in the navigation bar on the top . The highlighted index number displays the current question number . So , we can remove the `` Question # : '' prefix from the question title . Also , the CSS of the question title can be modified to make it more prominent . **Current state : ** < img width= '' 1343 '' alt= '' screen shot 2017-04-10 at 10 37 54 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24874548/e5e3dfee-1dd9-11e7-8fbd-f5ea96e4646d.png '' > **Suggested Modification : ** The following screenshot is just a mock-up of the suggested modifications.We can have a discussion about the best way to approach the same . < img width= '' 1354 '' alt= '' screen shot 2017-04-07 at 10 25 59 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24873848/4918f228-1dd7-11e7-89d5-531589e98759.png '' >
Question Title UI Modifications for better space utilization and prominence __EoT__ **Description : ** Every question title has a prefix `` Question # : '' in it . This is redundant as the question index is already being displayed in the navigation bar on the top . The highlighted index number displays the current question number . So , we can remove the `` Question # : '' prefix from the question title . Also , the CSS of the question title can be modified to make it more prominent . **Current state : ** < img width= '' 1343 '' alt= '' screen shot 2017-04-10 at 10 37 54 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24874548/e5e3dfee-1dd9-11e7-8fbd-f5ea96e4646d.png '' > **Suggested Modification : ** The following screenshot is just a mock-up of the suggested modifications.We can have a discussion about the best way to approach the same . < img width= '' 1354 '' alt= '' screen shot 2017-04-07 at 10 25 59 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24873848/4918f228-1dd7-11e7-89d5-531589e98759.png '' >
Question Title UI Modifications for better space utilization and prominence __EoT__ **Description : ** Every question title has a prefix `` Question # : '' in it . This is redundant as the question index is already being displayed in the navigation bar on the top . The highlighted index number displays the current question number . So , we can remove the `` Question # : '' prefix from the question title . Also , the CSS of the question title can be modified to make it more prominent . **Current state : ** < img width= '' 1343 '' alt= '' screen shot 2017-04-10 at 10 37 54 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24874548/e5e3dfee-1dd9-11e7-8fbd-f5ea96e4646d.png '' > **Suggested Modification : ** The following screenshot is just a mock-up of the suggested modifications.We can have a discussion about the best way to approach the same . < img width= '' 1354 '' alt= '' screen shot 2017-04-07 at 10 25 59 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24873848/4918f228-1dd7-11e7-89d5-531589e98759.png '' >
Question Title UI Modifications for better space utilization and prominence __EoT__ **Description : ** Every question title has a prefix `` Question # : '' in it . This is redundant as the question index is already being displayed in the navigation bar on the top . The highlighted index number displays the current question number . So , we can remove the `` Question # : '' prefix from the question title . Also , the CSS of the question title can be modified to make it more prominent . **Current state : ** < img width= '' 1343 '' alt= '' screen shot 2017-04-10 at 10 37 54 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24874548/e5e3dfee-1dd9-11e7-8fbd-f5ea96e4646d.png '' > **Suggested Modification : ** The following screenshot is just a mock-up of the suggested modifications.We can have a discussion about the best way to approach the same . < img width= '' 1354 '' alt= '' screen shot 2017-04-07 at 10 25 59 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24873848/4918f228-1dd7-11e7-89d5-531589e98759.png '' >
Question Title UI Modifications for better space utilization and prominence __EoT__ **Description : ** Every question title has a prefix `` Question # : '' in it . This is redundant as the question index is already being displayed in the navigation bar on the top . The highlighted index number displays the current question number . So , we can remove the `` Question # : '' prefix from the question title . Also , the CSS of the question title can be modified to make it more prominent . **Current state : ** < img width= '' 1343 '' alt= '' screen shot 2017-04-10 at 10 37 54 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24874548/e5e3dfee-1dd9-11e7-8fbd-f5ea96e4646d.png '' > **Suggested Modification : ** The following screenshot is just a mock-up of the suggested modifications.We can have a discussion about the best way to approach the same . < img width= '' 1354 '' alt= '' screen shot 2017-04-07 at 10 25 59 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24873848/4918f228-1dd7-11e7-89d5-531589e98759.png '' >
Question Title UI Modifications for better space utilization and prominence __EoT__ **Description : ** Every question title has a prefix `` Question # : '' in it . This is redundant as the question index is already being displayed in the navigation bar on the top . The highlighted index number displays the current question number . So , we can remove the `` Question # : '' prefix from the question title . Also , the CSS of the question title can be modified to make it more prominent . **Current state : ** < img width= '' 1343 '' alt= '' screen shot 2017-04-10 at 10 37 54 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24874548/e5e3dfee-1dd9-11e7-8fbd-f5ea96e4646d.png '' > **Suggested Modification : ** The following screenshot is just a mock-up of the suggested modifications.We can have a discussion about the best way to approach the same . < img width= '' 1354 '' alt= '' screen shot 2017-04-07 at 10 25 59 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24873848/4918f228-1dd7-11e7-89d5-531589e98759.png '' >
Question Title UI Modifications for better space utilization and prominence __EoT__ **Description : ** Every question title has a prefix `` Question # : '' in it . This is redundant as the question index is already being displayed in the navigation bar on the top . The highlighted index number displays the current question number . So , we can remove the `` Question # : '' prefix from the question title . Also , the CSS of the question title can be modified to make it more prominent . **Current state : ** < img width= '' 1343 '' alt= '' screen shot 2017-04-10 at 10 37 54 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24874548/e5e3dfee-1dd9-11e7-8fbd-f5ea96e4646d.png '' > **Suggested Modification : ** The following screenshot is just a mock-up of the suggested modifications.We can have a discussion about the best way to approach the same . < img width= '' 1354 '' alt= '' screen shot 2017-04-07 at 10 25 59 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24873848/4918f228-1dd7-11e7-89d5-531589e98759.png '' >
Question Title UI Modifications for better space utilization and prominence __EoT__ **Description : ** Every question title has a prefix `` Question # : '' in it . This is redundant as the question index is already being displayed in the navigation bar on the top . The highlighted index number displays the current question number . So , we can remove the `` Question # : '' prefix from the question title . Also , the CSS of the question title can be modified to make it more prominent . **Current state : ** < img width= '' 1343 '' alt= '' screen shot 2017-04-10 at 10 37 54 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24874548/e5e3dfee-1dd9-11e7-8fbd-f5ea96e4646d.png '' > **Suggested Modification : ** The following screenshot is just a mock-up of the suggested modifications.We can have a discussion about the best way to approach the same . < img width= '' 1354 '' alt= '' screen shot 2017-04-07 at 10 25 59 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24873848/4918f228-1dd7-11e7-89d5-531589e98759.png '' >
Git hook does n't run when pushing on Mac __EoT__ I 'm not sure if this is an issue on all OS , but when I push from my mac it does n't seem to run the tests from the hook script . # # # Repro steps : 1 . Make edits and commit ( I 'm using a Macbook Air ) 2 . Push # # # Expected behaviour Linter and karma tests are run before push
Give graduated feedback for syntax errors . __EoT__ In an interview setting , students are not going to get feedback that a syntax error has happened `` on line X , column Y '' . To be fair , such syntax errors probably do n't matter too much anyway . However , in TIE , the code has to be syntactically correct so that it can run and so that feedback can be given . A possible middle ground might therefore be to do the following : when a syntax error is found , highlight the line that contains it in red , and say something along the lines of `` there 's a syntax error here ; please find it and fix it '' , while offering an optional popup hint that shows the exact error and line number . That way , if a student wants to practice finding syntax errors on inspection , they have the opportunity to do so ; if they do n't want to practice this , they can just open the popup .
Harden the question verification tests . __EoT__ In [ QuestionSchemaValidationService ] ( https : //github.com/google/tie/blob/0580cd73546a9ea77113839b0b84bff5fd1e473f/assets/tests/QuestionSchemaValidationService.js # L21 ) , there are a number of TODOs that involve additional verification ( though I think the first of this has been involved in # 142 -- @ eyurko , is this right ? ) The last of these TODOs ( verify that the starter code matches a language pattern for the language specified ) is probably not worth doing , but I think we should : - [ x ] Verify that AuxiliaryCode contains the AuxiliaryCode class definition . - [ x ] Verify that starter code contains the necessary Python function definitions . since these should be reasonably straightforward tests to write .
Add functionality for e2e tests to ensure that the application does not break . __EoT__ We should probably add an end-to-end test that loads the page and clicks some of the buttons / submits some code , in order to prevent breakages like the one that was fixed in # 80 . ( Note that even an end-to-end test which just loads the app.html page in a browser and checks that there are no console errors would have helped here . ) Protractor is likely to be a good library for this , and is worth investigating .
Add functionality for e2e tests to ensure that the application does not break . __EoT__ We should probably add an end-to-end test that loads the page and clicks some of the buttons / submits some code , in order to prevent breakages like the one that was fixed in # 80 . ( Note that even an end-to-end test which just loads the app.html page in a browser and checks that there are no console errors would have helped here . ) Protractor is likely to be a good library for this , and is worth investigating .
Add functionality for e2e tests to ensure that the application does not break . __EoT__ We should probably add an end-to-end test that loads the page and clicks some of the buttons / submits some code , in order to prevent breakages like the one that was fixed in # 80 . ( Note that even an end-to-end test which just loads the app.html page in a browser and checks that there are no console errors would have helped here . ) Protractor is likely to be a good library for this , and is worth investigating .
Add functionality for e2e tests to ensure that the application does not break . __EoT__ We should probably add an end-to-end test that loads the page and clicks some of the buttons / submits some code , in order to prevent breakages like the one that was fixed in # 80 . ( Note that even an end-to-end test which just loads the app.html page in a browser and checks that there are no console errors would have helped here . ) Protractor is likely to be a good library for this , and is worth investigating .
Add functionality for e2e tests to ensure that the application does not break . __EoT__ We should probably add an end-to-end test that loads the page and clicks some of the buttons / submits some code , in order to prevent breakages like the one that was fixed in # 80 . ( Note that even an end-to-end test which just loads the app.html page in a browser and checks that there are no console errors would have helped here . ) Protractor is likely to be a good library for this , and is worth investigating .
Add functionality for e2e tests to ensure that the application does not break . __EoT__ We should probably add an end-to-end test that loads the page and clicks some of the buttons / submits some code , in order to prevent breakages like the one that was fixed in # 80 . ( Note that even an end-to-end test which just loads the app.html page in a browser and checks that there are no console errors would have helped here . ) Protractor is likely to be a good library for this , and is worth investigating .
Add functionality for e2e tests to ensure that the application does not break . __EoT__ We should probably add an end-to-end test that loads the page and clicks some of the buttons / submits some code , in order to prevent breakages like the one that was fixed in # 80 . ( Note that even an end-to-end test which just loads the app.html page in a browser and checks that there are no console errors would have helped here . ) Protractor is likely to be a good library for this , and is worth investigating .
Add functionality for e2e tests to ensure that the application does not break . __EoT__ We should probably add an end-to-end test that loads the page and clicks some of the buttons / submits some code , in order to prevent breakages like the one that was fixed in # 80 . ( Note that even an end-to-end test which just loads the app.html page in a browser and checks that there are no console errors would have helped here . ) Protractor is likely to be a good library for this , and is worth investigating .
Add functionality for e2e tests to ensure that the application does not break . __EoT__ We should probably add an end-to-end test that loads the page and clicks some of the buttons / submits some code , in order to prevent breakages like the one that was fixed in # 80 . ( Note that even an end-to-end test which just loads the app.html page in a browser and checks that there are no console errors would have helped here . ) Protractor is likely to be a good library for this , and is worth investigating .
Add functionality for e2e tests to ensure that the application does not break . __EoT__ We should probably add an end-to-end test that loads the page and clicks some of the buttons / submits some code , in order to prevent breakages like the one that was fixed in # 80 . ( Note that even an end-to-end test which just loads the app.html page in a browser and checks that there are no console errors would have helped here . ) Protractor is likely to be a good library for this , and is worth investigating .
No reset button to reset the current code __EoT__ @ sniffsky and I are currently working on # 98 , however , we realize that if we are able to save codes , then it 's probably good to have a reset button once # 98 is resolved .
No reset button to reset the current code __EoT__ @ sniffsky and I are currently working on # 98 , however , we realize that if we are able to save codes , then it 's probably good to have a reset button once # 98 is resolved .
No reset button to reset the current code __EoT__ @ sniffsky and I are currently working on # 98 , however , we realize that if we are able to save codes , then it 's probably good to have a reset button once # 98 is resolved .
No reset button to reset the current code __EoT__ @ sniffsky and I are currently working on # 98 , however , we realize that if we are able to save codes , then it 's probably good to have a reset button once # 98 is resolved .
No reset button to reset the current code __EoT__ @ sniffsky and I are currently working on # 98 , however , we realize that if we are able to save codes , then it 's probably good to have a reset button once # 98 is resolved .
No reset button to reset the current code __EoT__ @ sniffsky and I are currently working on # 98 , however , we realize that if we are able to save codes , then it 's probably good to have a reset button once # 98 is resolved .
No reset button to reset the current code __EoT__ @ sniffsky and I are currently working on # 98 , however , we realize that if we are able to save codes , then it 's probably good to have a reset button once # 98 is resolved .
No reset button to reset the current code __EoT__ @ sniffsky and I are currently working on # 98 , however , we realize that if we are able to save codes , then it 's probably good to have a reset button once # 98 is resolved .
No reset button to reset the current code __EoT__ @ sniffsky and I are currently working on # 98 , however , we realize that if we are able to save codes , then it 's probably good to have a reset button once # 98 is resolved .
No reset button to reset the current code __EoT__ @ sniffsky and I are currently working on # 98 , however , we realize that if we are able to save codes , then it 's probably good to have a reset button once # 98 is resolved .
No reset button to reset the current code __EoT__ @ sniffsky and I are currently working on # 98 , however , we realize that if we are able to save codes , then it 's probably good to have a reset button once # 98 is resolved .
Add a pre-push git hook to catch silly mistakes prior to submit . __EoT__ It would be nice to have a script that runs the lint checks and karma tests just prior to a push -- see e.g . the `` pre-push '' hook in [ this link ] ( https : //git-scm.com/book/gr/v2/Customizing-Git-Git-Hooks ) . This script would then be run automatically after `` git push '' is done on the command line . We probably do not need to include e2e tests in this script , since they generally take a while to run and the developer is expected to do some manual testing anyway .
Add a pre-push git hook to catch silly mistakes prior to submit . __EoT__ It would be nice to have a script that runs the lint checks and karma tests just prior to a push -- see e.g . the `` pre-push '' hook in [ this link ] ( https : //git-scm.com/book/gr/v2/Customizing-Git-Git-Hooks ) . This script would then be run automatically after `` git push '' is done on the command line . We probably do not need to include e2e tests in this script , since they generally take a while to run and the developer is expected to do some manual testing anyway .
Add a pre-push git hook to catch silly mistakes prior to submit . __EoT__ It would be nice to have a script that runs the lint checks and karma tests just prior to a push -- see e.g . the `` pre-push '' hook in [ this link ] ( https : //git-scm.com/book/gr/v2/Customizing-Git-Git-Hooks ) . This script would then be run automatically after `` git push '' is done on the command line . We probably do not need to include e2e tests in this script , since they generally take a while to run and the developer is expected to do some manual testing anyway .
Add a pre-push git hook to catch silly mistakes prior to submit . __EoT__ It would be nice to have a script that runs the lint checks and karma tests just prior to a push -- see e.g . the `` pre-push '' hook in [ this link ] ( https : //git-scm.com/book/gr/v2/Customizing-Git-Git-Hooks ) . This script would then be run automatically after `` git push '' is done on the command line . We probably do not need to include e2e tests in this script , since they generally take a while to run and the developer is expected to do some manual testing anyway .
Add a pre-push git hook to catch silly mistakes prior to submit . __EoT__ It would be nice to have a script that runs the lint checks and karma tests just prior to a push -- see e.g . the `` pre-push '' hook in [ this link ] ( https : //git-scm.com/book/gr/v2/Customizing-Git-Git-Hooks ) . This script would then be run automatically after `` git push '' is done on the command line . We probably do not need to include e2e tests in this script , since they generally take a while to run and the developer is expected to do some manual testing anyway .
Add a pre-push git hook to catch silly mistakes prior to submit . __EoT__ It would be nice to have a script that runs the lint checks and karma tests just prior to a push -- see e.g . the `` pre-push '' hook in [ this link ] ( https : //git-scm.com/book/gr/v2/Customizing-Git-Git-Hooks ) . This script would then be run automatically after `` git push '' is done on the command line . We probably do not need to include e2e tests in this script , since they generally take a while to run and the developer is expected to do some manual testing anyway .
Add a pre-push git hook to catch silly mistakes prior to submit . __EoT__ It would be nice to have a script that runs the lint checks and karma tests just prior to a push -- see e.g . the `` pre-push '' hook in [ this link ] ( https : //git-scm.com/book/gr/v2/Customizing-Git-Git-Hooks ) . This script would then be run automatically after `` git push '' is done on the command line . We probably do not need to include e2e tests in this script , since they generally take a while to run and the developer is expected to do some manual testing anyway .
Add a pre-push git hook to catch silly mistakes prior to submit . __EoT__ It would be nice to have a script that runs the lint checks and karma tests just prior to a push -- see e.g . the `` pre-push '' hook in [ this link ] ( https : //git-scm.com/book/gr/v2/Customizing-Git-Git-Hooks ) . This script would then be run automatically after `` git push '' is done on the command line . We probably do not need to include e2e tests in this script , since they generally take a while to run and the developer is expected to do some manual testing anyway .
Add a pre-push git hook to catch silly mistakes prior to submit . __EoT__ It would be nice to have a script that runs the lint checks and karma tests just prior to a push -- see e.g . the `` pre-push '' hook in [ this link ] ( https : //git-scm.com/book/gr/v2/Customizing-Git-Git-Hooks ) . This script would then be run automatically after `` git push '' is done on the command line . We probably do not need to include e2e tests in this script , since they generally take a while to run and the developer is expected to do some manual testing anyway .
Add a pre-push git hook to catch silly mistakes prior to submit . __EoT__ It would be nice to have a script that runs the lint checks and karma tests just prior to a push -- see e.g . the `` pre-push '' hook in [ this link ] ( https : //git-scm.com/book/gr/v2/Customizing-Git-Git-Hooks ) . This script would then be run automatically after `` git push '' is done on the command line . We probably do not need to include e2e tests in this script , since they generally take a while to run and the developer is expected to do some manual testing anyway .
Add a pre-push git hook to catch silly mistakes prior to submit . __EoT__ It would be nice to have a script that runs the lint checks and karma tests just prior to a push -- see e.g . the `` pre-push '' hook in [ this link ] ( https : //git-scm.com/book/gr/v2/Customizing-Git-Git-Hooks ) . This script would then be run automatically after `` git push '' is done on the command line . We probably do not need to include e2e tests in this script , since they generally take a while to run and the developer is expected to do some manual testing anyway .
Add a pre-push git hook to catch silly mistakes prior to submit . __EoT__ It would be nice to have a script that runs the lint checks and karma tests just prior to a push -- see e.g . the `` pre-push '' hook in [ this link ] ( https : //git-scm.com/book/gr/v2/Customizing-Git-Git-Hooks ) . This script would then be run automatically after `` git push '' is done on the command line . We probably do not need to include e2e tests in this script , since they generally take a while to run and the developer is expected to do some manual testing anyway .
Prevent UI components rearranging when window is resized . __EoT__ When the page is zoomed or when the browser window is resized , the instructions pane drops to the bottom of the page . This is confusing and also probably not that useful -- instead , it should just horizontally overflow to the right of the page . The aim of this issue is therefore to remove this behavior . In other words , always show two columns on the page , and overflow off the right if needed . /cf # 81 /cc @ rabidbit @ shaman-rajan
Prevent UI components rearranging when window is resized . __EoT__ When the page is zoomed or when the browser window is resized , the instructions pane drops to the bottom of the page . This is confusing and also probably not that useful -- instead , it should just horizontally overflow to the right of the page . The aim of this issue is therefore to remove this behavior . In other words , always show two columns on the page , and overflow off the right if needed . /cf # 81 /cc @ rabidbit @ shaman-rajan
Prevent UI components rearranging when window is resized . __EoT__ When the page is zoomed or when the browser window is resized , the instructions pane drops to the bottom of the page . This is confusing and also probably not that useful -- instead , it should just horizontally overflow to the right of the page . The aim of this issue is therefore to remove this behavior . In other words , always show two columns on the page , and overflow off the right if needed . /cf # 81 /cc @ rabidbit @ shaman-rajan
Prevent UI components rearranging when window is resized . __EoT__ When the page is zoomed or when the browser window is resized , the instructions pane drops to the bottom of the page . This is confusing and also probably not that useful -- instead , it should just horizontally overflow to the right of the page . The aim of this issue is therefore to remove this behavior . In other words , always show two columns on the page , and overflow off the right if needed . /cf # 81 /cc @ rabidbit @ shaman-rajan
Prevent UI components rearranging when window is resized . __EoT__ When the page is zoomed or when the browser window is resized , the instructions pane drops to the bottom of the page . This is confusing and also probably not that useful -- instead , it should just horizontally overflow to the right of the page . The aim of this issue is therefore to remove this behavior . In other words , always show two columns on the page , and overflow off the right if needed . /cf # 81 /cc @ rabidbit @ shaman-rajan
Prevent UI components rearranging when window is resized . __EoT__ When the page is zoomed or when the browser window is resized , the instructions pane drops to the bottom of the page . This is confusing and also probably not that useful -- instead , it should just horizontally overflow to the right of the page . The aim of this issue is therefore to remove this behavior . In other words , always show two columns on the page , and overflow off the right if needed . /cf # 81 /cc @ rabidbit @ shaman-rajan
Prevent UI components rearranging when window is resized . __EoT__ When the page is zoomed or when the browser window is resized , the instructions pane drops to the bottom of the page . This is confusing and also probably not that useful -- instead , it should just horizontally overflow to the right of the page . The aim of this issue is therefore to remove this behavior . In other words , always show two columns on the page , and overflow off the right if needed . /cf # 81 /cc @ rabidbit @ shaman-rajan
Prevent UI components rearranging when window is resized . __EoT__ When the page is zoomed or when the browser window is resized , the instructions pane drops to the bottom of the page . This is confusing and also probably not that useful -- instead , it should just horizontally overflow to the right of the page . The aim of this issue is therefore to remove this behavior . In other words , always show two columns on the page , and overflow off the right if needed . /cf # 81 /cc @ rabidbit @ shaman-rajan
Prevent UI components rearranging when window is resized . __EoT__ When the page is zoomed or when the browser window is resized , the instructions pane drops to the bottom of the page . This is confusing and also probably not that useful -- instead , it should just horizontally overflow to the right of the page . The aim of this issue is therefore to remove this behavior . In other words , always show two columns on the page , and overflow off the right if needed . /cf # 81 /cc @ rabidbit @ shaman-rajan
Prevent UI components rearranging when window is resized . __EoT__ When the page is zoomed or when the browser window is resized , the instructions pane drops to the bottom of the page . This is confusing and also probably not that useful -- instead , it should just horizontally overflow to the right of the page . The aim of this issue is therefore to remove this behavior . In other words , always show two columns on the page , and overflow off the right if needed . /cf # 81 /cc @ rabidbit @ shaman-rajan
Prevent UI components rearranging when window is resized . __EoT__ When the page is zoomed or when the browser window is resized , the instructions pane drops to the bottom of the page . This is confusing and also probably not that useful -- instead , it should just horizontally overflow to the right of the page . The aim of this issue is therefore to remove this behavior . In other words , always show two columns on the page , and overflow off the right if needed . /cf # 81 /cc @ rabidbit @ shaman-rajan
Prevent UI components rearranging when window is resized . __EoT__ When the page is zoomed or when the browser window is resized , the instructions pane drops to the bottom of the page . This is confusing and also probably not that useful -- instead , it should just horizontally overflow to the right of the page . The aim of this issue is therefore to remove this behavior . In other words , always show two columns on the page , and overflow off the right if needed . /cf # 81 /cc @ rabidbit @ shaman-rajan
Create `` Find closest value in BST '' question __EoT__
Create `` Find closest value in BST '' question __EoT__
Create `` Find closest value in BST '' question __EoT__
Bottom nav bar font could be lighter in dark mode __EoT__
Create question to find if a given string is a palindrome __EoT__
Create `` Find best meeting point in grid-like city '' problem __EoT__
[ UI Issues ] Output text can not wrap within the text area __EoT__ The output screen at the left top corner has a black box to indicate the Run results , which only scales based on the screen size instead of the text length . Thus , if the text within this black box is longer than screen size , the rest text will simply extend out of the black box , which is not very elegant . Repro instructions : 1 . Run app.html 2 . Navigate to Question-1 3 . Change code to return a very long string , such as < img width= '' 674 '' alt= '' screen shot 2017-04-03 at 1 40 57 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630894/374ca652-1873-11e7-8a7d-220de99b2957.png '' > 4 . Click on Run button 5 . Actual result will come out in this black box < img width= '' 664 '' alt= '' screen shot 2017-04-03 at 1 36 29 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630929/5b647b46-1873-11e7-8c06-f0c7081b3591.png '' > We can find out the output text exceeds the actual size of the black box .
[ UI Issues ] Output text can not wrap within the text area __EoT__ The output screen at the left top corner has a black box to indicate the Run results , which only scales based on the screen size instead of the text length . Thus , if the text within this black box is longer than screen size , the rest text will simply extend out of the black box , which is not very elegant . Repro instructions : 1 . Run app.html 2 . Navigate to Question-1 3 . Change code to return a very long string , such as < img width= '' 674 '' alt= '' screen shot 2017-04-03 at 1 40 57 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630894/374ca652-1873-11e7-8a7d-220de99b2957.png '' > 4 . Click on Run button 5 . Actual result will come out in this black box < img width= '' 664 '' alt= '' screen shot 2017-04-03 at 1 36 29 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630929/5b647b46-1873-11e7-8c06-f0c7081b3591.png '' > We can find out the output text exceeds the actual size of the black box .
[ UI Issues ] Output text can not wrap within the text area __EoT__ The output screen at the left top corner has a black box to indicate the Run results , which only scales based on the screen size instead of the text length . Thus , if the text within this black box is longer than screen size , the rest text will simply extend out of the black box , which is not very elegant . Repro instructions : 1 . Run app.html 2 . Navigate to Question-1 3 . Change code to return a very long string , such as < img width= '' 674 '' alt= '' screen shot 2017-04-03 at 1 40 57 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630894/374ca652-1873-11e7-8a7d-220de99b2957.png '' > 4 . Click on Run button 5 . Actual result will come out in this black box < img width= '' 664 '' alt= '' screen shot 2017-04-03 at 1 36 29 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630929/5b647b46-1873-11e7-8c06-f0c7081b3591.png '' > We can find out the output text exceeds the actual size of the black box .
[ UI Issues ] Output text can not wrap within the text area __EoT__ The output screen at the left top corner has a black box to indicate the Run results , which only scales based on the screen size instead of the text length . Thus , if the text within this black box is longer than screen size , the rest text will simply extend out of the black box , which is not very elegant . Repro instructions : 1 . Run app.html 2 . Navigate to Question-1 3 . Change code to return a very long string , such as < img width= '' 674 '' alt= '' screen shot 2017-04-03 at 1 40 57 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630894/374ca652-1873-11e7-8a7d-220de99b2957.png '' > 4 . Click on Run button 5 . Actual result will come out in this black box < img width= '' 664 '' alt= '' screen shot 2017-04-03 at 1 36 29 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630929/5b647b46-1873-11e7-8c06-f0c7081b3591.png '' > We can find out the output text exceeds the actual size of the black box .
[ UI Issues ] Output text can not wrap within the text area __EoT__ The output screen at the left top corner has a black box to indicate the Run results , which only scales based on the screen size instead of the text length . Thus , if the text within this black box is longer than screen size , the rest text will simply extend out of the black box , which is not very elegant . Repro instructions : 1 . Run app.html 2 . Navigate to Question-1 3 . Change code to return a very long string , such as < img width= '' 674 '' alt= '' screen shot 2017-04-03 at 1 40 57 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630894/374ca652-1873-11e7-8a7d-220de99b2957.png '' > 4 . Click on Run button 5 . Actual result will come out in this black box < img width= '' 664 '' alt= '' screen shot 2017-04-03 at 1 36 29 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630929/5b647b46-1873-11e7-8c06-f0c7081b3591.png '' > We can find out the output text exceeds the actual size of the black box .
[ UI Issues ] Output text can not wrap within the text area __EoT__ The output screen at the left top corner has a black box to indicate the Run results , which only scales based on the screen size instead of the text length . Thus , if the text within this black box is longer than screen size , the rest text will simply extend out of the black box , which is not very elegant . Repro instructions : 1 . Run app.html 2 . Navigate to Question-1 3 . Change code to return a very long string , such as < img width= '' 674 '' alt= '' screen shot 2017-04-03 at 1 40 57 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630894/374ca652-1873-11e7-8a7d-220de99b2957.png '' > 4 . Click on Run button 5 . Actual result will come out in this black box < img width= '' 664 '' alt= '' screen shot 2017-04-03 at 1 36 29 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630929/5b647b46-1873-11e7-8c06-f0c7081b3591.png '' > We can find out the output text exceeds the actual size of the black box .
[ UI Issues ] Output text can not wrap within the text area __EoT__ The output screen at the left top corner has a black box to indicate the Run results , which only scales based on the screen size instead of the text length . Thus , if the text within this black box is longer than screen size , the rest text will simply extend out of the black box , which is not very elegant . Repro instructions : 1 . Run app.html 2 . Navigate to Question-1 3 . Change code to return a very long string , such as < img width= '' 674 '' alt= '' screen shot 2017-04-03 at 1 40 57 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630894/374ca652-1873-11e7-8a7d-220de99b2957.png '' > 4 . Click on Run button 5 . Actual result will come out in this black box < img width= '' 664 '' alt= '' screen shot 2017-04-03 at 1 36 29 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630929/5b647b46-1873-11e7-8c06-f0c7081b3591.png '' > We can find out the output text exceeds the actual size of the black box .
[ UI Issues ] Output text can not wrap within the text area __EoT__ The output screen at the left top corner has a black box to indicate the Run results , which only scales based on the screen size instead of the text length . Thus , if the text within this black box is longer than screen size , the rest text will simply extend out of the black box , which is not very elegant . Repro instructions : 1 . Run app.html 2 . Navigate to Question-1 3 . Change code to return a very long string , such as < img width= '' 674 '' alt= '' screen shot 2017-04-03 at 1 40 57 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630894/374ca652-1873-11e7-8a7d-220de99b2957.png '' > 4 . Click on Run button 5 . Actual result will come out in this black box < img width= '' 664 '' alt= '' screen shot 2017-04-03 at 1 36 29 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630929/5b647b46-1873-11e7-8c06-f0c7081b3591.png '' > We can find out the output text exceeds the actual size of the black box .
Create Most Common Character Question __EoT__ Create _Most Common Character_ Question with @ spwahaha
Create Most Common Character Question __EoT__ Create _Most Common Character_ Question with @ spwahaha
Syntax errors only display the last error __EoT__ Expected behavior : when running into multiple syntax errors , all past errors can be viewed . This is important for the alternative UI ( # 268 ) , which maintains a history of feedback . Observed behavior : In alternative UI , past syntax error feedback is displayed , but only the most recent error message can be viewed . Steps to reproduce : In alternative UI ( # 268 ) , generate multiple syntax errors and try to view all the errors . Notice only details of the last error is displayed .
Syntax errors only display the last error __EoT__ Expected behavior : when running into multiple syntax errors , all past errors can be viewed . This is important for the alternative UI ( # 268 ) , which maintains a history of feedback . Observed behavior : In alternative UI , past syntax error feedback is displayed , but only the most recent error message can be viewed . Steps to reproduce : In alternative UI ( # 268 ) , generate multiple syntax errors and try to view all the errors . Notice only details of the last error is displayed .
Show better error message if user deletes or modifies starter code . __EoT__ Repro steps : 1 . Open app.html 2 . Navigate to the i18n problem . 3 . Change `` abbreviate '' to `` isBalanced '' 4 . Run code . 5 . You should see this error : [ `` Your code threw an error : AttributeError : 'StudentCode ' object has no attribute 'abbreviate ' on line 42 '' ] It might be worth catching that error and saying something closer to `` You seem to be missing an 'abbreviate ' method . Check the starter code and make sure that it has n't been deleted or modified accidentally . ''
Show better error message if user deletes or modifies starter code . __EoT__ Repro steps : 1 . Open app.html 2 . Navigate to the i18n problem . 3 . Change `` abbreviate '' to `` isBalanced '' 4 . Run code . 5 . You should see this error : [ `` Your code threw an error : AttributeError : 'StudentCode ' object has no attribute 'abbreviate ' on line 42 '' ] It might be worth catching that error and saying something closer to `` You seem to be missing an 'abbreviate ' method . Check the starter code and make sure that it has n't been deleted or modified accidentally . ''
Show a visual reward on completion of a question . __EoT__ This is a follow-up from the discussion in # 469 . On completing a question , we should show a visual reward to the learner . E.g . create a cake in the form of a big emoji or sticker in the feedback speech balloon , or an overlay over the coding window . Note that doing this in the speech balloon would mimic real chat apps , but we could user test this .
ReverseWords Starter Code Hides str ( ) Builtin __EoT__ The name str is a built in python function , but can not be used in the reverse word challenge because it is hidden by the function argument .
Create `` Increment a decimal-coded number '' problem __EoT__
Resetting feedback window does not remove syntax error __EoT__ Expected behavior : When resetting the feedback window , displayed syntax errors are also cleared . Observed behavior : Resetting the feedback window does not clear displayed syntax errors . Steps to reproduce : Generate a syntax error and display details . Reset feedback window and note that syntax error is not cleared .
User study - Alternative to output / expected output feedback __EoT__ Users are relying on the default output / expected output feedback , and it 's causing them to use this feedback as a form of printing and compare it more closely to existing tools . @ seanlip suggested only showing the input and asking the user to consider that input . This would provide us with a default feedback that should always be useful yet prevents the user ( and us ) from depending on output , which is a closer experience to a real interview .
User study - Alternative to output / expected output feedback __EoT__ Users are relying on the default output / expected output feedback , and it 's causing them to use this feedback as a form of printing and compare it more closely to existing tools . @ seanlip suggested only showing the input and asking the user to consider that input . This would provide us with a default feedback that should always be useful yet prevents the user ( and us ) from depending on output , which is a closer experience to a real interview .
User study - Alternative to output / expected output feedback __EoT__ Users are relying on the default output / expected output feedback , and it 's causing them to use this feedback as a form of printing and compare it more closely to existing tools . @ seanlip suggested only showing the input and asking the user to consider that input . This would provide us with a default feedback that should always be useful yet prevents the user ( and us ) from depending on output , which is a closer experience to a real interview .
User study - Alternative to output / expected output feedback __EoT__ Users are relying on the default output / expected output feedback , and it 's causing them to use this feedback as a form of printing and compare it more closely to existing tools . @ seanlip suggested only showing the input and asking the user to consider that input . This would provide us with a default feedback that should always be useful yet prevents the user ( and us ) from depending on output , which is a closer experience to a real interview .
