TestScoped does n't work in non-test thread . __EoT__ When start a thread from the test , it ca n't inject ` @ TestScoped ` instance to the new thread . It throws ` com.google.inject.OutOfScopeException : Attempt to inject @ TestScoped binding outside test : `
Ca n't install publicsuffic=x per instruction __EoT__ When I get public suffix : `` ` % go get code.google.com/p/go.net/publicsuffix warning : code.google.com is shutting down ; import path code.google.com/p/go.net/publicsuffix will stop working `` ` build seems to succeed , but then on run `` ` Â± % ./namebench 2015/12/22 18:36:53 URL : http : //127.0.0.1:65086/ 2015/12/22 18:36:53 error running /Applications/node-webkit.app/Contents/MacOS/node-webkit ./ui/app.nw : fork/exec /Applications/node-webkit.app/Contents/MacOS/node-webkit : no such file or directory `` `
Replace campfire with bazel . __EoT__ Kythe has moved over so we should be able to do so as well . Some notes from schroederc : Context : http : //kythe.io/phabricator/T29 Kythe has custom rules , unsupported by the Bazel team , to build proto and Go . They should be relatively easy to get working in the Shipshape repo since they are derived from the campfire rules . See https : //github.com/google/kythe/tree/master/tools/build_rules/ for the Skylark code .
I got `` in matMul : inputs must be rank 2 '' error when I used retained model . __EoT__ Hi I love Emoji Scavenger Hunt . I want to play the game with my own trained model . But I got the following error . Can you please help me ? **Summary** I created a model and deployed it . However I got `` ` Error : Error in matMul : inputs must be rank 2 , got ranks 1 and 2. `` ` error during starting the game . I want to solve it . **Steps to Reproduce** `` ` # Git clone from my forked repo $ git clone -- depth 1 -- branch matmul-error https : //github.com/y-zono/emoji-scavenger-hunt.git # Training $ cd emoji-scavenger-hunt/training/ $ docker build -t model-builder . $ docker run -v /xxx/xxx/emoji-scavenger-hunt/training/data : /data -it model-builder # Copy the trained files into dist $ cp data/saved_model_web/group1-shard1of1 ../dist/model/group1-shard1of1 $ cp data/saved_model_web/tensorflowjs_model.pb ../dist/model/web_model.pb $ cp data/saved_model_web/weights_manifest.json ../dist/model/weights_manifest.json # Run the app $ cd .. ; yarn prep ; yarn dev # Open browser and click `` LET 'S PLAY '' http : //localhost:3000 `` ` **Expected Results** The game is started normally . **Actual Results** The following
Incorrect mecurial URL __EoT__ https : //github.com/google/google-api-dotnet-client-samples/tree/master/Plus.ServiceAccount Issue originally from https : //github.com/google/google-api-dotnet-client/issues/834
Failed to recognize trailing comma on import statement with parenthese __EoT__ Hi , Trailing comma is not allowed in import , but when inside parentheses it 's valid syntax I know that this one can sound too specific , but still a valid syntax = ) file : `` ` from foo.a import ( do , ) do ( ) `` ` Script : `` ` import pasta from pasta.augment import rename path = r'test.py' with open ( path ) as file : tree = pasta.parse ( file.read ( ) ) `` ` Full stack trace `` ` Traceback ( most recent call last ) : File `` script.py '' , line 8 , in < module > tree = pasta.parse ( file.read ( ) ) File `` /opt/pasta/pasta/__init__.py '' , line 25 , in parse annotator.visit ( t ) File `` /opt/pasta/pasta/base/annotate.py '' , line 1055 , in visit super ( AstAnnotator , self ) .visit ( node ) File `` /opt/pasta/pasta/base/annotate.py '' , line 115 , in visit super ( BaseVisitor , self ) .visit ( node ) File `` /home/william/miniconda3/envs/google_pasta_py27/lib/python2.7/ast.py '' , line 241 , in visit return visitor ( node ) File `` /opt/pasta/pasta/base/annotate.py '' ,
streams can cause a file description exhaustion __EoT__ Something I have seen several times is a condition were the process hits the limit of the available file descriptors , and it results in a server that accepts the TCP sockets , but then it waits in allocating a file descriptor for them ( until it 's free ) . This is typically due to : `` ` js stream.pipe ( res ) // where res is an http response `` ` If ` stream ` errors , ` res ` is not closed automatically , leaving a file descriptor behind . To mitigate this : `` ` js stream.pipe ( res ) stream.on ( 'error ' , ( err ) = > { // sample implementation res.writeHead ( 500 ) res.end ( ) } ) `` ` or `` ` js const pump = require ( 'pump ' ) pump ( stream , res ) `` ` While this is not a security vulnerability by itself , it could be exploited .
Could not analyze bug report generated on Android N Preview __EoT__ When I posted a bug report generated on Android N Preview , Battery Historian just stopped working with the following log and the UI stayed at `` Analyzing ... '' status . Any report format changes introduced in Android N ? `` ` 2016/04/26 17:30:15 Listening on port : 9999 2016/04/26 17:30:18 Trace starting analysisServer processing for : GET 2016/04/26 17:30:18 Trace finished analysisServer processing for : GET 2016/04/26 17:30:19 Trace starting analysisServer processing for : GET 2016/04/26 17:30:19 Trace finished analysisServer processing for : GET 2016/04/26 17:30:24 Trace starting analysisServer processing for : POST 2016/04/26 17:30:24 Trace starting reading uploaded file . 22327437 bytes 2016/04/26 17:30:24 Trace started analyzing file . 2016/04/26 17:30:25 Trace finished generating Historian plot . `` `
Give ` LocalProcessManager ` a ` const ` constructor ... __EoT__ ... so it can be used as a default parameter .
Incorrect inferred module name for __init__.py files __EoT__ See google/pytype # 154 for more detail . ` resolve.infer_module_name ` calculates the wrong name for ` __init__.py ` files . For example , for ` foo/bar/__init__.py ` , it will return ` foo.bar.__init__ ` . The module name should be ` foo.bar ` .
Publish ct-java to OSSRH __EoT__ Guide : http : //central.sonatype.org/pages/ossrh-guide.html Requirements : http : //central.sonatype.org/pages/requirements.html PR # 6 addresses the requirements , so what 's left is signing up for account and publishing it . @ AlCutter , I 'd like to know your thoughts on this before proceeding .
API -- dhcp-no-bind flag seems invalid __EoT__ Perhaps a silly question here but when attempting to run : pixiecore api http : //127.0.0.1:6000/ -- dhcp-no-bind The following error is produced : listen ipv4:17 : lookup < nil > : invalid domain name Note : I am attempting to run this inside a Docker container with an OS base of Ubuntu 16.04 I have also tried placing the -- dhcp-no-bind flag infront of the api URL and the result is the same . Am I missing something obvious with the -- dhcp-no-bind flag ? I am needing this as I am running a dhcp server inside the same container .
Skip very fresh points from Datadog __EoT__ Some folks from Datadog that I talked to mentioned that the most recent point returned by the API might contain incomplete data if a query aggregates values reported from multiple machines and data coming from some of the machines is slightly delayed . Since it 's impossible to edit values that have already been submitted to Stackdriver , I suggest ts-bridge ignore points from Datadog that are less than 1 minute old to allow data to be fully processed and aggregated .
Restart hung process __EoT__ Hi , my computer hibernated overnight and the process has hung after loading 15000 mail messages . Is there any way to re-start the process from the point it has hung ?
Prune low-scoring scenarios __EoT__ The genetic orbits scenario currently allows the database to grow without bounds . The database should occasionally be pruned to the top N scenarios , either as an occasionally-running background task , or as a shutdown task , or both .
Deadlink to SQL-03 specs __EoT__ The link to the SQL-03 specs ( http : //savage.net.au/SQL/sql-2003-1.bnf.html ) found in https : //github.com/google/lovefield/blob/master/docs/spec/00_intro.md is 404 .
Unreadable text with dark theme __EoT__ With the dark theme enabled , this tool is unreadable .
messageview attempts to decode partial data __EoT__ ` messageview.MessageView.BodyReader ` tries to uncompress the body by looking only at the Content-Encoding . But it neglects to check for a Range header ( or for a 206 Partial Content status ) . This results in an error when attempting to unzip a partial gzip file .
Usage question __EoT__ I 'm evaluating Copybara but the lack of documentation makes it a bit hard to understand some basic concepts . Let 's suppose there a private repo A and a public repo B , both hosted on Github . B is a filtered version of A , in the sense that some files and folders are removed . Most work ( internal ) will happen on A , but some public contributions might happen on B as pull requests . My understanding so far is that I would have to setup webhooks on both repos . On A to mirror branches to B on commits , and on B to mirror PRs to A . Is my understanding correct ? thank you for the product and your attention !
No interrobangsâ€½ __EoT__ We should have interrobangs because they 're trendy ( that 's questionable , I know ) and useful .
Podspec not pushed to trunk __EoT__ Hey there , after getting no response on my [ PR ] ( https : //github.com/google/GTMAppAuth/pull/23 ) the repo apparently got updated by Phillip Zsika . He bumped the podspec to [ 0.6.0 ] ( https : //github.com/google/GTMAppAuth/commit/780f6a9491b1168ca961d5d625aefa78887edb12 ) but seemed to have forgotten to push the spec to the cocoapods repo so it can be actually discovered and used . Can you please update the pod repo in a timely manner so people can start using this library again ? ! Jonas
AppAuth upgrade problem __EoT__ The latest AppAuth version had change to 0.90.0 , but the version in podspec is still limit to 0.9.x . Need change podspec define and macOS support .
No detection of mutation during iteration . __EoT__ reported by @ adonovan
Raise something more explicit when wrong Docker folder is passed __EoT__ Something along the lines : `` ` diff -- git a/docker_explorer/lib/container.py b/docker_explorer/lib/container.py index 5f926e1..5a7df25 100644 -- - a/docker_explorer/lib/container.py +++ b/docker_explorer/lib/container.py @ @ -81,8 +81,14 @ @ class Container ( object ) : container_info_json_path = os.path.join ( self.docker_directory , 'containers ' , container_id , self.container_config_filename ) - with open ( container_info_json_path ) as container_info_json_file : - container_info_dict = json.load ( container_info_json_file ) + try : + with open ( container_info_json_path ) as container_info_json_file : + container_info_dict = json.load ( container_info_json_file ) + except IOError as exception : + raise errors.BadContainerException ( + 'Unable to open configuration file { 0 } make sure you use the proper docker dir'.format ( + container_info_json_path ) + ) if container_info_dict is None : raise errors.BadContainerException ( `` `
Fix the flaky ExampleThread_Load_parallelCycle test __EoT__ e.g . https : //travis-ci.org/google/starlark-go/jobs/450227265
Hot-swap active processing pipeline at runtime __EoT__ We want to be able to update the currently running pipeline at runtime , in response to a new configuration being pushed to an active NEL collector process . To do that we need a new implementation of ` http.Handler ` that : - wraps another handler , delegating to it for all requests - allows you to atomically update the current wrapped handler in a thread-safe way
Use file to configure processing pipeline __EoT__ We have a nice pipeline abstraction for defining at runtime how to process all of the NEL reports that are uploaded to the collector . Right now , you have to manually instantiate the particular processors you want to use via Go code . We want this to be configurable via a config file , so that we can have a single binary that works regardless of how you want to process and publish the reports .
Step by step setup documentation . __EoT__ In order to use library with vulkano I need step by step instructions for setting up the build environment in the vulkano readme . It would make sense to also include these instructions in the shaderc-rs readme . For ubuntu this is easy : ` sudo apt-get install git python cmake ` People using other distros can fairly easily adapt this to their own package manager . I have no access to macOS so I have no idea about this one . I hear brew is popular over there ? For windows I need both msvc and gnu toolchains to work , I 'm not sure if they require different tools , however I can only get msvc to work . I setup the dependencies via msys2 . I ran : ` pacman -Syu mingw-w64-x86_64-cmake mingw-w64-x86_64-make mingw-w64-x86_64-gcc mingw64/mingw-w64-x86_64-pkg-config ` and have ` C : \msys64\mingw64\bin ; C : \msys64\usr\bin ` added to my path At this point I can set the toolchain to msvc and ` cargo build ` + ` cargo test ` successfully However I tried to compile with the gnu toolchain using the terminal window packaged with git and I
Make code Python3.6 compatible __EoT__
Make code Python3.6 compatible __EoT__
Make code Python3.6 compatible __EoT__
gtm_stringByUnescapingFromHTML does not unescape & # 128077 ; __EoT__ I was trying to unescape ` & # 128077 ; ` ( which is & # 128077 ; ) , but found that gtm_stringByUnescapingFromHTML could only unescape number sequence below 65535 ( USHRT_MAX ) . Is is possible to support unescaping HTML entities above 65535 and make ` XCTAssertEqualObjects ( [ @ '' & # 128077 ; '' gtm_stringByUnescapingFromHTML ] , @ '' ðŸ‘ '' , @ '' HTML unescaping failed '' ) ; ` pass ? Edit : I am thinking of solving this issue with https : //github.com/salagadoola/google-toolbox-for-mac/commit/c28a73bb35b704b54f66dfa89bc5286e26738dac which is derived from stackoverflow https : //stackoverflow.com/questions/6458708/converting-an-nsstring-to-and-from-utf32 # answer-6458988
remove fastify as a dependency . __EoT__ as titled
FreeBSD support __EoT__ I 'd like to support FreeBSD somehow . Issues with FreeBSD : - As we use -std=c11 , we ca n't use gettimeofday ( ) in until_nonidle . There seems to be no workaround other than not enabling C11 . - pam_unix can not check one 's own password ( https : //bugs.freebsd.org/bugzilla/show_bug.cgi ? id=194604 ) . We probably need our own setuid root helper binary . Making auth_pam_x11 setuid root would probably work but is absolutely not advised . As a current workaround , users could install pam_pwdfile and use a htpasswd-like file . The former is easy to solve by build script hackery , although I do n't quite like it ; the latter however is a major obstacle for any screen locker .
FreeBSD support __EoT__ I 'd like to support FreeBSD somehow . Issues with FreeBSD : - As we use -std=c11 , we ca n't use gettimeofday ( ) in until_nonidle . There seems to be no workaround other than not enabling C11 . - pam_unix can not check one 's own password ( https : //bugs.freebsd.org/bugzilla/show_bug.cgi ? id=194604 ) . We probably need our own setuid root helper binary . Making auth_pam_x11 setuid root would probably work but is absolutely not advised . As a current workaround , users could install pam_pwdfile and use a htpasswd-like file . The former is easy to solve by build script hackery , although I do n't quite like it ; the latter however is a major obstacle for any screen locker .
Error message formatting __EoT__
Python 3 compatibility ( NameError : name 'textfsm ' is not defined during installation ) __EoT__ I can install ` gtextfsm ` using Python 2.7 but not using Python 3.5 . Python 2 : `` ` sh user @ debian : /tmp $ virtualenv py27 New python executable in /tmp/py27/bin/python Installing setuptools , pip , wheel ... done . user @ debian : /tmp $ . ./py27/bin/activate ( py27 ) user @ debian : /tmp $ python Python 2.7.3 ( default , Jun 21 2016 , 18:38:19 ) [ GCC 4.7.2 ] on linux2 Type `` help '' , `` copyright '' , `` credits '' or `` license '' for more information . > > > ( py27 ) user @ debian : /tmp $ pip install gtextfsm Collecting gtextfsm Installing collected packages : gtextfsm Successfully installed gtextfsm-0.2.1 `` ` Python 3 : `` ` sh user @ debian : /tmp $ virtualenv-3.5 -p python3.5 py35 Already using interpreter /home/user/bin/python3.5 Using base prefix '/home/user' New python executable in /tmp/py35/bin/python3.5 Also creating executable in /tmp/py35/bin/python Installing setuptools , pip , wheel ... done . user @ debian : /tmp $ . py35/bin/activate ( py35 ) user @ debian : /tmp
Bundle scan should detect the most-general bundle , not the least-general __EoT__ Using Spotify.app as an example below , running `` Spotify Helper '' inside the `` Spotify Helper.app '' bundle should map to `` Spotify.app '' because it is the most general ( and , thus , the most user-recognizable ) part of the application . However the current logic only walks up the file tree until it finds any bundle which , as in this case , may not always be the best choice . **/Applications/Spotify.app/** â””â”€â”€ Contents â”œâ”€â”€ Frameworks â”‚ â”œâ”€â”€ Chromium\ Embedded\ Framework.framework â”‚ â”‚ â””â”€â”€ ... â”‚ â””â”€â”€ **Spotify\ Helper.app** â”‚ â””â”€â”€ Contents â”‚ â”œâ”€â”€ Info.plist â”‚ â”œâ”€â”€ MacOS â”‚ â”‚ â””â”€â”€ **Spotify\ Helper** < -- -- -- -- -- â”‚ â””â”€â”€ _CodeSignature â”‚ â””â”€â”€ CodeResources â”œâ”€â”€ Info.plist â”œâ”€â”€ MacOS â”‚ â”œâ”€â”€ Spotify â”‚ â”œâ”€â”€ SpotifyWebHelper â”‚ â””â”€â”€ sp_relauncher â”œâ”€â”€ Resources â”‚ â””â”€â”€ ... â””â”€â”€ _CodeSignature â””â”€â”€ CodeResources
setup continuous integration __EoT__
Create new module proxy for Wire __EoT__ Wire is currently using Go Cloud 's module proxy , which could cause issues later on . This project should have its own module proxy used for Travis builds .
gowire reports `` can not find package '' for various pkgs after running vgo mod -vendor __EoT__ Here 's my session . vgo installs for example fsnotify , but then gowire does n't find it . `` ` issactrotts-macbookpro : ~ issactrotts $ git clone git @ github.com : google/go-cloud.git issactrotts-macbookpro : ~ issactrotts $ cd go-cloud issactrotts-macbookpro : go-cloud issactrotts $ vgo install ./wire/cmd/gowire issactrotts-macbookpro : go-cloud issactrotts $ cd samples/guestbook/ issactrotts-macbookpro : guestbook issactrotts $ vgo mod -vendor go : downloading github.com/dnaeon/go-vcr v0.0.0-20180504081357-f8a7e8b9c630 go : downloading github.com/stretchr/testify v1.2.1 go : downloading github.com/go-sql-driver/mysql v0.0.0-20180308100310-1a676ac6e4dc go : downloading github.com/census-ecosystem/opencensus-go-exporter-aws v0.0.0-20180411051634-41633bc1ff6b go : downloading github.com/GoogleCloudPlatform/cloudsql-proxy v0.0.0-20180321230639-1e456b1c68cb go : downloading github.com/pmezard/go-difflib v1.0.0 go : downloading github.com/fsnotify/fsnotify v1.4.7 go : downloading github.com/gorilla/mux v1.6.1 go : downloading github.com/davecgh/go-spew v1.1.0 go : downloading github.com/gorilla/context v1.1.1 go : downloading google.golang.org/appengine v1.1.0 go : downloading gopkg.in/ini.v1 v1.37.0 go : downloading github.com/aws/aws-xray-sdk-go v1.0.0-rc.5 go : downloading golang.org/x/sys v0.0.0-20180329131831-378d26f46672 go : downloading gopkg.in/yaml.v2 v2.2.1 go : downloading github.com/google/go-cmp v0.2.0 go : downloading github.com/smartystreets/goconvey v0.0.0-20180222194500-ef6db91d284a go : downloading contrib.go.opencensus.io/exporter/stackdriver v0.0.0-20180421005815-665cf5131b71 go : downloading gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405 go : downloading github.com/jtolds/gls v0.0.0-20170503224851-77f18212c9c7 go : downloading github.com/golang/glog v0.0.0-20160126235308-23def4e6c14b go : downloading golang.org/x/sync v0.0.0-20180314180146-1d60e4601c6f go : downloading github.com/gopherjs/gopherjs v0.0.0-20180424202546-8dffc02ea1cb go : downloading github.com/smartystreets/assertions v0.0.0-20180301161246-7678a5452ebe
Cause the Homebrew build to use newer source __EoT__ I 'm using the Guetzli build with Homebrew . When compressing smaller images I 'm getting a segmentation fault . The file in question is 419 bytes , and can be found at [ https : //www.coastercms.org/themes/coaster/img/line.jpg ] ( url ) .
Homebrew gflags has no ` libgflags.pc ` __EoT__ Building from master ( ` c78689c5e020c0cd3580783213798b6323eaa454 ` ) on macOS gives me the following error : `` ` ==== Building guetzli ( release ) ==== Creating obj/Release butteraugli_comparator.cc Package libgflags was not found in the pkg-config search path . Perhaps you should add the directory containing ` libgflags.pc' to the PKG_CONFIG_PATH environment variable No package 'libgflags ' found ... snip ... butteraugli.cc Package libgflags was not found in the pkg-config search path . Perhaps you should add the directory containing ` libgflags.pc' to the PKG_CONFIG_PATH environment variable No package 'libgflags ' found Linking guetzli Package libgflags was not found in the pkg-config search path . Perhaps you should add the directory containing ` libgflags.pc' to the PKG_CONFIG_PATH environment variable No package 'libgflags ' found Undefined symbols for architecture x86_64 : `` google : :FlagRegisterer : :FlagRegisterer < bool > ( char const* , char const* , char const* , bool* , bool* ) '' , referenced from : __GLOBAL__sub_I_guetzli.cc in guetzli.o `` google : :FlagRegisterer : :FlagRegisterer < double > ( char const* , char const* , char const* , double* , double* ) '' , referenced from : __GLOBAL__sub_I_guetzli.cc in guetzli.o
Ubuntu 16.04 compilation error due to gflags was not found in the pkg-config search path . __EoT__ After installing the needed packages as described in the README.MD using apt-get in Ubuntu 16.04 there 's this error after running make : ` ==== Building guetzli ( release ) ==== Linking guetzli Package gflags was not found in the pkg-config search path . Perhaps you should add the directory containing 'gflags.pc' to the PKG_CONFIG_PATH environment variable No package 'gflags ' found ` The ` gflags.pc ` file simply does n't exist on the whole system .
container_pull authentication failure __EoT__ I 'm having an issue with ` container_pull ` from private Docker Registry based on Sonatype OSS Nexus Repository Manager : `` ` containerregistry.client.v2_2.docker_http_.BadStateException : Unexpected `` www-authenticate '' challenge type : BASIC ( /usr/bin/python /private/var/tmp/_bazel_eric/35e4f788de46e2946a456376a62aa566/external/puller/file/puller.par -- directory /private/var/tmp/_bazel_eric/35e4f788de46e2946a456376a62aa566/external/base/image -- name suxen.vipdmp.com/com.veon/jvm-builder @ sha256 : e3aefc80adf5c8d403a1887d55f53c54ded0cdaf3b1c3e95bb861526b9cb515f ) . `` ` Looking at Python source code , I see , that it only accepts ` Basic ` challenge type and not ` BASIC ` I suppose the check should be case-insensitive .
gradle sync failing __EoT__ `` Error : Conflict with dependency 'com.google.errorprone : error_prone_annotations ' in project 'mobly-bundled-snippets ' . Resolved versions for app ( 2.0.18 ) and test app ( 2.0.19 ) differ . See http : //g.co/androidstudio/app-test-app-conflict for details . '' Adding this seems to work around it : ` compile 'com.google.errorprone : error_prone_annotations:2.0.19 ' ` Not sure what 's going on
` : lint ` is failing and does n't say why __EoT__ `` ` : lint FAILED FAILURE : Build failed with an exception . * What went wrong : Execution failed for task ' : lint ' . > Lint found errors in the project ; aborting build . Fix the issues identified by lint , or add the following to your build script to proceed with errors : ... android { lintOptions { abortOnError false } } ... * Try : Run with -- info or -- debug option to get more log output . * Exception is : org.gradle.api.tasks.TaskExecutionException : Execution failed for task ' : lint ' . at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions ( ExecuteActionsTaskExecuter.java:84 ) at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute ( ExecuteActionsTaskExecuter.java:55 ) at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute ( SkipUpToDateTaskExecuter.java:62 ) at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute ( ValidatingTaskExecuter.java:58 ) at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute ( SkipEmptySourceFilesTaskExecuter.java:88 ) at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute ( ResolveTaskArtifactStateTaskExecuter.java:46 ) at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute ( SkipTaskWithNoActionsExecuter.java:51 ) at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute ( SkipOnlyIfTaskExecuter.java:54 ) at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute ( ExecuteAtMostOnceTaskExecuter.java:43 ) at org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute ( CatchExceptionTaskExecuter.java:34 ) at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter $ EventFiringTaskWorker $ 1.execute ( DefaultTaskGraphExecuter.java:236 ) at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter $ EventFiringTaskWorker $ 1.execute ( DefaultTaskGraphExecuter.java:228 ) at org.gradle.internal.Transformers $ 4.transform ( Transformers.java:169 ) at org.gradle.internal.progress.DefaultBuildOperationExecutor.run ( DefaultBuildOperationExecutor.java:106 ) at org.gradle.internal.progress.DefaultBuildOperationExecutor.run ( DefaultBuildOperationExecutor.java:61 ) at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter $
Use context for timeouts __EoT__ In # 59 , a config option was added to set a default timeout for the HTTP client . However , there may be a problem : ` ListUpdate ` and ` HashLookup ` get the same timeout , but ` ListUpdate ` probably takes a lot longer in most cases and is not in the blocking path . Instead , it would be nice to have a variant of LookupURLs that took a context , and would pass that through to ` HashLookup ` , and from there to the HTTP client so that the caller could set a timeout with more granularity .
Create a new release for macOS homebrew __EoT__ I 've created a [ homebrew PR ] ( https : //github.com/Homebrew/homebrew-core/pull/19177 ) that successfully builds google-authenticator-libpam on macOS . Since homebrew only allows builds from released versions , could you issue a new release , to take advantage of recent code changes , including PR # 80 ? This will resolve a workaround in the formula . Thanks !
Please make it possible to run tests on local jars __EoT__ Current wycherproof tries to download BouncyCastle from the Internet . I need wycherproof to run tests on the local BouncyCastle jars . How can I do it ?
testVectors in DsaTest not using the message parameter __EoT__ At https : //github.com/google/wycheproof/blob/223332fde6f83c6421c605189abe48a332da6a31/java/com/google/security/wycheproof/testcases/DsaTest.java # L351 message is being passed in but is n't being used to calculate messageBytes . It looks like this is only passed in as `` Hello '' currently but it would be nice to have the message correctly used .
testVectors in DsaTest not using the message parameter __EoT__ At https : //github.com/google/wycheproof/blob/223332fde6f83c6421c605189abe48a332da6a31/java/com/google/security/wycheproof/testcases/DsaTest.java # L351 message is being passed in but is n't being used to calculate messageBytes . It looks like this is only passed in as `` Hello '' currently but it would be nice to have the message correctly used .
Repeated input values for primality test __EoT__ I noticed some values are checked more than once by the primality test , and do n't see any obvious reason for the test to repeat for just those inputs . The values 164280218643672633986221 , 318665857834031151167461 , and 360681321802296925566181 all appear multiple times in the test data in https : //github.com/google/wycheproof/blob/master/java/com/google/security/wycheproof/testcases/BigIntegerTest.java Maybe they can be removed , or maybe there should be a comment explaining the duplication . Thanks for this great set of tests .
Failure to gyp leveldb.gyp on mac __EoT__ Original [ issue 8 ] ( https : //code.google.com/p/leveldb/issues/detail ? id=8 ) created by kkowalczyk on 2011-06-07T04:17:41.000Z : I tried to generate XCode project file with gyp ( latest svn sources r932 ) but get this error : kjkmacpro : leveldb kkowalczyk $ gyp Traceback ( most recent call last ) : File & quot ; /usr/local/bin/gyp & quot ; , line 18 , in & lt ; module & gt ; sys.exit ( gyp.main ( sys.argv [ 1 : ] ) ) File & quot ; /Library/Python/2.6/site-packages/gyp/**init**.py & quot ; , line 448 , in main options.circular_check ) File & quot ; /Library/Python/2.6/site-packages/gyp/**init**.py & quot ; , line 87 , in Load depth , generator_input_info , check , circular_check ) File & quot ; /Library/Python/2.6/site-packages/gyp/input.py & quot ; , line 2224 , in Load depth , check ) File & quot ; /Library/Python/2.6/site-packages/gyp/input.py & quot ; , line 379 , in LoadTargetBuildFile build_file_path ) File & quot ; /Library/Python/2.6/site-packages/gyp/input.py & quot ; , line 998 , in ProcessVariablesAndConditionsInDict build_file ) File & quot ; /Library/Python/2.6/site-packages/gyp/input.py & quot ; , line 1013 , in ProcessVariablesAndConditionsInList ProcessVariablesAndConditionsInDict ( item , is_late , variables ,
memory runs out __EoT__ Below is the core dump when I try to write 100m records into leveldb . I found no way for leveldb to limit the usage of memory , it seems that leveldb will not stopping writting records into memory . Program terminated with signal 6 , Aborted . # 0 0x00c41410 in __kernel_vsyscall ( ) ( gdb ) bt # 0 0x00c41410 in __kernel_vsyscall ( ) # 1 0x00496b10 in raise ( ) from /lib/libc.so.6 # 2 0x00498421 in abort ( ) from /lib/libc.so.6 # 3 0x00394fa0 in __gnu_cxx : :__verbose_terminate_handler ( ) at ../../.././libstdc++-v3/libsupc++/vterminate.cc:98 # 4 0x00392845 in __cxxabiv1 : :__terminate ( handler=0x394e50 < __gnu_cxx : :__verbose_terminate_handler ( ) > ) at ../../.././libstdc++-v3/libsupc++/eh_terminate.cc:43 # 5 0x00392882 in std : :terminate ( ) at ../../.././libstdc++-v3/libsupc++/eh_terminate.cc:53 # 6 0x003929aa in __cxxabiv1 : :__cxa_throw ( obj=0xab501508 , tinfo=0x3bf474 , dest=0x392ee0 std : :bad_alloc : :~bad_alloc ( ) ) at ../../.././libstdc++-v3/libsupc++/eh_throw.cc:76 # 7 0x00392fbe in operator new ( sz=1048560 ) at ../../.././libstdc++-v3/libsupc++/new_op.cc:63 # 8 0x0036cd01 in allocate ( __n= < optimized out > , this= < optimized out > ) at /home/nemo/gcc-4.2.0/i686-pc-linux-gnu/libstdc++-v3/include/ext/new_allocator.h:91 # 9 std : :string : :_Rep : :_S_create ( __capacity=1048547 , __old_capacity=24649 , __alloc= ... )
memory runs out __EoT__ Below is the core dump when I try to write 100m records into leveldb . I found no way for leveldb to limit the usage of memory , it seems that leveldb will not stopping writting records into memory . Program terminated with signal 6 , Aborted . # 0 0x00c41410 in __kernel_vsyscall ( ) ( gdb ) bt # 0 0x00c41410 in __kernel_vsyscall ( ) # 1 0x00496b10 in raise ( ) from /lib/libc.so.6 # 2 0x00498421 in abort ( ) from /lib/libc.so.6 # 3 0x00394fa0 in __gnu_cxx : :__verbose_terminate_handler ( ) at ../../.././libstdc++-v3/libsupc++/vterminate.cc:98 # 4 0x00392845 in __cxxabiv1 : :__terminate ( handler=0x394e50 < __gnu_cxx : :__verbose_terminate_handler ( ) > ) at ../../.././libstdc++-v3/libsupc++/eh_terminate.cc:43 # 5 0x00392882 in std : :terminate ( ) at ../../.././libstdc++-v3/libsupc++/eh_terminate.cc:53 # 6 0x003929aa in __cxxabiv1 : :__cxa_throw ( obj=0xab501508 , tinfo=0x3bf474 , dest=0x392ee0 std : :bad_alloc : :~bad_alloc ( ) ) at ../../.././libstdc++-v3/libsupc++/eh_throw.cc:76 # 7 0x00392fbe in operator new ( sz=1048560 ) at ../../.././libstdc++-v3/libsupc++/new_op.cc:63 # 8 0x0036cd01 in allocate ( __n= < optimized out > , this= < optimized out > ) at /home/nemo/gcc-4.2.0/i686-pc-linux-gnu/libstdc++-v3/include/ext/new_allocator.h:91 # 9 std : :string : :_Rep : :_S_create ( __capacity=1048547 , __old_capacity=24649 , __alloc= ... )
memory runs out __EoT__ Below is the core dump when I try to write 100m records into leveldb . I found no way for leveldb to limit the usage of memory , it seems that leveldb will not stopping writting records into memory . Program terminated with signal 6 , Aborted . # 0 0x00c41410 in __kernel_vsyscall ( ) ( gdb ) bt # 0 0x00c41410 in __kernel_vsyscall ( ) # 1 0x00496b10 in raise ( ) from /lib/libc.so.6 # 2 0x00498421 in abort ( ) from /lib/libc.so.6 # 3 0x00394fa0 in __gnu_cxx : :__verbose_terminate_handler ( ) at ../../.././libstdc++-v3/libsupc++/vterminate.cc:98 # 4 0x00392845 in __cxxabiv1 : :__terminate ( handler=0x394e50 < __gnu_cxx : :__verbose_terminate_handler ( ) > ) at ../../.././libstdc++-v3/libsupc++/eh_terminate.cc:43 # 5 0x00392882 in std : :terminate ( ) at ../../.././libstdc++-v3/libsupc++/eh_terminate.cc:53 # 6 0x003929aa in __cxxabiv1 : :__cxa_throw ( obj=0xab501508 , tinfo=0x3bf474 , dest=0x392ee0 std : :bad_alloc : :~bad_alloc ( ) ) at ../../.././libstdc++-v3/libsupc++/eh_throw.cc:76 # 7 0x00392fbe in operator new ( sz=1048560 ) at ../../.././libstdc++-v3/libsupc++/new_op.cc:63 # 8 0x0036cd01 in allocate ( __n= < optimized out > , this= < optimized out > ) at /home/nemo/gcc-4.2.0/i686-pc-linux-gnu/libstdc++-v3/include/ext/new_allocator.h:91 # 9 std : :string : :_Rep : :_S_create ( __capacity=1048547 , __old_capacity=24649 , __alloc= ... )
Fairplay support patch __EoT__ **Shaka Packager Version** : b451d3a7ca6f454b4905043d56dd9620247d8c71 **Packager Command** : `` ` ./packager \ 'input=udp : //127.0.0.1:11000 , stream=3 , segment_template=fairplay_video- $ Number $ .ts , playlist_name=video.m3u8 ' \ 'input=udp : //127.0.0.1:11000 , stream=0 , segment_template=fairplay_audio1- $ Number $ .ts , playlist_name=audio1.m3u8 , hls_group_id=audio1 , hls_name=EN ' \ -- enable_fixed_key_encryption \ -- clear_lead 0 \ -- key ... \ -- key_id ... \ -- iv ... \ -- hls_playlist_type LIVE \ -- hls_master_playlist_output= '' playlist.m3u8 '' `` ` I would like to properly patch shaka-packager to produce SAMPLE-AES crypted content . The end result is having this kind of header in HLS chunklist : `` ` # EXT-X-KEY : METHOD=SAMPLE-AES , URI= '' skd : //entry '' , KEYFORMAT= '' com.apple.streamingkeydelivery '' , KEYFORMATVERSIONS= '' 1 '' `` ` instead of this one which requires transfering key in clear format : `` ` # EXT-X-KEY : METHOD=SAMPLE-AES , URI= '' data : text/plain ; base64 , ... '' , IV=0x ... , KEYFORMAT= '' identity '' `` ` I already have it working by hardcoding the header into media_playlist . I would like to submit a proper patch for this . Any pointers on how should I approach
Release new gjf version __EoT__ With [ -- dry-run ] ( https : //github.com/google/google-java-format/pull/106 ) option the major FR was implemented . For all projects that depend on the gjf fork it 's time to switch using upstream . Please , release a new version ASAP .
JDK 9 module-info.java handling __EoT__ Having a ` module-info.java ` in the source set leads to unexpected format exceptions starting with `` ` src/main/java/module-info.java:1:1 : error : Syntax error on token `` module '' , package expected `` ` Solution 1 Skip ` module-info.java ` although it ends with ` .java ` Line ` Main.java ` : [ 118 ] ( https : //github.com/google/google-java-format/blob/master/core/src/main/java/com/google/googlejavaformat/java/Main.java # L118 ) already skips non-Java files . Could easily be handled here . Solution 2 Support ` module-info.java ` [ syntax ] ( http : //cr.openjdk.java.net/~mr/jigsaw/spec/lang-vm.html ) and reformat it as well .
Could not cast value of type # MyClass to NSObject __EoT__ I ran to this runtime error . Does it mean my class has to be inherited from NSObject to be used as Promise type ? It stops at this library code `` ` swift static func asValue ( _ value : AnyObject ? ) - > Value ? { // Swift nil becomes NSNull during bridging . return value as ? Value ? ? NSNull ( ) as AnyObject as ? Value } `` ` callstack : ... # 4 0x000000010f00ccfb in swift : :swift_dynamicCastFailure ( void const* , char const* , void const* , char const* , char const* ) ( ) # 5 0x000000010f00cd60 in swift : :swift_dynamicCastFailure ( swift : :TargetMetadata < swift : :InProcess > const* , swift : :TargetMetadata < swift : :InProcess > const* , char const* ) ( ) # 6 0x000000010f049d8e in swift_dynamicCastObjCClassUnconditional ( ) # 7 0x000000010f00f03d in swift_dynamicCast ( ) # 8 0x000000010f4eaf1a in swift_rt_swift_dynamicCast ( ) # 9 0x000000010f546f64 in specialized _setDownCastConditional < A , B > ( _ : ) ( ) # 10 0x000000010f5478c2 in specialized static Set._conditionallyBridgeFromObjectiveC ( _ : result : ) ( )
Could not cast value of type # MyClass to NSObject __EoT__ I ran to this runtime error . Does it mean my class has to be inherited from NSObject to be used as Promise type ? It stops at this library code `` ` swift static func asValue ( _ value : AnyObject ? ) - > Value ? { // Swift nil becomes NSNull during bridging . return value as ? Value ? ? NSNull ( ) as AnyObject as ? Value } `` ` callstack : ... # 4 0x000000010f00ccfb in swift : :swift_dynamicCastFailure ( void const* , char const* , void const* , char const* , char const* ) ( ) # 5 0x000000010f00cd60 in swift : :swift_dynamicCastFailure ( swift : :TargetMetadata < swift : :InProcess > const* , swift : :TargetMetadata < swift : :InProcess > const* , char const* ) ( ) # 6 0x000000010f049d8e in swift_dynamicCastObjCClassUnconditional ( ) # 7 0x000000010f00f03d in swift_dynamicCast ( ) # 8 0x000000010f4eaf1a in swift_rt_swift_dynamicCast ( ) # 9 0x000000010f546f64 in specialized _setDownCastConditional < A , B > ( _ : ) ( ) # 10 0x000000010f5478c2 in specialized static Set._conditionallyBridgeFromObjectiveC ( _ : result : ) ( )
Add FileSystem.path __EoT__ `` ` import 'package : path/path.dart ' as path ; abstract class FileSystem { path.Context get path ; } `` ` This will allow users of any file system to reliably be able to use ` path ` APIs against the correct path ` Style ` . Side note : this will make the ` pathSeparator ` field in ` FileSystem ` redundant .
RecordingFileSystem & ReplayFileSystem __EoT__ ` RecordingFileSystem ` will delegate to an underlying ` FileSystem ` , recording all activity that passes through it . ` ReplayFileSystem ` will take a serialized recording and respond to matching calls with recorded info ( and throw for calls not in the recording ) . This will enable record/replay tests of code that accesses the file system .
Feature Request __EoT__ UBS plugged and unplugged events : I am unable to locate plugged and unplugged events in this package . It would be nice to know if a usb device is plugged or unplugged so that implementors can better manage reconnection when an unplugged device is later plugged-in . I looked through the code but could hardly find anything close to that , is it there and I am just missing it ? If not , is this something that could be implemented ?
libusb.free ( ) race __EoT__ libusb.free ( ) frees the libusbTransfer pointer and only then removes it from xferDoneMap ( under a lock ) . If libusb.alloc ( ) is called from another goroutine in between these two operations , it might allocate the same pointer that was just freed . The new element might then be removed from xferDoneMap prematurely , resulting in a deadlock when xferCallback tries to notify a nil channel . I believe changing the order in free ( ) should fix this .
libusb.free ( ) race __EoT__ libusb.free ( ) frees the libusbTransfer pointer and only then removes it from xferDoneMap ( under a lock ) . If libusb.alloc ( ) is called from another goroutine in between these two operations , it might allocate the same pointer that was just freed . The new element might then be removed from xferDoneMap prematurely , resulting in a deadlock when xferCallback tries to notify a nil channel . I believe changing the order in free ( ) should fix this .
Provide means of getting device 's serial number __EoT__ In my practice , it is often necessary to distinguish between several devices with the same VID , PID . The standard way of doing that is by serial number , however i do n't see any way of getting it . ` Context.OpenDeviceWithVIDPIDSerial ` would be ideal , but if ` DeviceDesc ` provided the ` SerialNumber ` field , then filtering in the ` OpenDevices ` callback would also be acceptable . Even post-filtering of opened devices would be ok . In any case , there needs to be a way to query device 's serial number , that 's the main thing .
Support TCP-MD5 authentication for BGP __EoT__ **Is this a bug report or a feature request ? ** : Feature Request **What happened** : We would like to be able to set a password field in the BGP configuration , needed when working with some peers : [ Like Vultr 's BGP ] ( https : //www.vultr.com/docs/high-availability-on-vultr-with-floating-ip-and-bgp ) , which accept a password field to authenticate a user , see the ` protocol bgp vultr ` section in documentation link above , there is a ` password ` line . Looking through the code , seems like it would go to some place like [ this ] ( https : //github.com/google/metallb/blob/7489989faf9a3cc47da8b55c8b60ec7a4db6f40b/internal/config/config.go # L39 ) though of course im not a go programmer yet . **What you expected to happen** : We cant set the password , so we will obviously not be able to use their BGP **How to reproduce it ( as minimally and precisely as possible ) ** : Always **Anything else we need to know ? ** : Not sure if there is a downside , but we only see the upsides . **Environment** : - MetalLB version : 0.5.0 - Kubernetes version : 1.9X - BGP router
arp-speaker is still spammy __EoT__ **Is this a bug report or a feature request ? ** : feature **What happened** : Logs are still spammy for arp-speaker **What you expected to happen** : I 'll remove some of the glogs , we do n't care about all those arps flying by .
Preserve comments in the AST __EoT__ The parser should have an option to capture the comments and attach them to AST nodes ( https : //talks.go-zh.org/2015/gofmt-en.slide # 31 ) . For convergence , we probably want to do like in Buildifier ( https : //github.com/bazelbuild/buildtools/blob/master/build/syntax.go # L59 ) . Comments are useful for a wide range of applications ( linter , formatting , refactoring ... ) .
Preserve comments in the AST __EoT__ The parser should have an option to capture the comments and attach them to AST nodes ( https : //talks.go-zh.org/2015/gofmt-en.slide # 31 ) . For convergence , we probably want to do like in Buildifier ( https : //github.com/bazelbuild/buildtools/blob/master/build/syntax.go # L59 ) . Comments are useful for a wide range of applications ( linter , formatting , refactoring ... ) .
globals provided to exec can be overwritten by module __EoT__ In skylark , repeated assignments to global variables are supposed to be a compile time error . E.g . ` a= '' foo '' ; a= '' bar '' ` is an error -- ` `` can not reassign global a declared at [ ... ] '' ` . However , if I define a global in my go code that is calling ` skylark.Exec ( ... ) ` , and then assign to that in my skylark script , there 's no error . This seems inconsistent and surprising . It seems like the globals I provide to Exec are treated roughly as if they 're in the [ universe block ] ( https : //github.com/google/skylark/blob/master/doc/spec.md # built-in-constants-and-functions ) in that assignments can shadow them ... but they 're also *not* treated like the universe block because they 'll still be in the globals StringDict after the Exec returns . I 'm not sure if this is a bug or just a documentation request . : ) ( In case it 's relevant , here 's why I 'm asking : I 'd like to be providing some more
No way to add shortened options ( e.g . '-R/ -- recursive ' ) __EoT__ Would it be possible to create short names for default boolean arguments ? For example , if you have the following function : `` ` python def hello ( greeting , uppercase=False ) : greeting = greeting.upper ( ) if uppercase else greeting print ( greeting ) `` ` It would be nice to have the following usages : ` python hello.py Hey ! -u ` ` python hello.py Hey ! -- uppercase `
No way to add shortened options ( e.g . '-R/ -- recursive ' ) __EoT__ Would it be possible to create short names for default boolean arguments ? For example , if you have the following function : `` ` python def hello ( greeting , uppercase=False ) : greeting = greeting.upper ( ) if uppercase else greeting print ( greeting ) `` ` It would be nice to have the following usages : ` python hello.py Hey ! -u ` ` python hello.py Hey ! -- uppercase `
Cut a new release for python-fire __EoT__ Hi @ dbieber - would you be able to cut a new release for python-fire ? I 'd really love to be able to use that parsing fix ! : ) Also : if there 's any way I can help speed that process up , please tell me !
date string being parsed as integer math expression __EoT__ Using fire 0.1.2 ... $ python mycode.py my_function -- daydate 2017-10-09 - > The daydate parameter of my_function is passed the string value '2017-10-09 ' as expected and desired . $ python mycode.py my_function -- daydate 2017-10-10 - > The daydate parameter of my function is passed the integer value 1997 . Apparently '2017-10-10 ' is being parsed as an integer math expression ? I 'm not sure why anyone would ever want that , and I 'm very sure that no one would expect that . I understand that I can solve this by placing my argument value in both double and single quotes , but I 'd really rather not have to do that .
date string being parsed as integer math expression __EoT__ Using fire 0.1.2 ... $ python mycode.py my_function -- daydate 2017-10-09 - > The daydate parameter of my_function is passed the string value '2017-10-09 ' as expected and desired . $ python mycode.py my_function -- daydate 2017-10-10 - > The daydate parameter of my function is passed the integer value 1997 . Apparently '2017-10-10 ' is being parsed as an integer math expression ? I 'm not sure why anyone would ever want that , and I 'm very sure that no one would expect that . I understand that I can solve this by placing my argument value in both double and single quotes , but I 'd really rather not have to do that .
HelloCompute running failed under Win10 . __EoT__ E : \CI-Cor-Ready\ai\face-demo-work\nxt-standalone\build\Debug > HelloCompute.exe Assertion failure at E : \CI-Cor-Ready\ai\face-demo-work\nxt-standalone\src\utils\D3D12Binding.cpp:49 ( ASSERT_SUCCESS ) : ( ( ( HRESULT ) ( hr ) ) > = 0 )
Travis CI Sporadic Build Failures __EoT__ Following # 31 , Travis CI builds started sporadically failing with g++ building Spirv cross . This seems like a common [ issue ] ( https : //github.com/travis-ci/travis-ci/issues/1972 ) where Travis kills g++ because it uses too much memory
RenderPassLoadOpTests.ColorClearThenLoadAndDraw/OpenGL fails __EoT__ RenderPassLoadOpTests.ColorClearThenLoadAndDraw/OpenGL ( to be introduced in # 107 ) currently fails on the OpenGL backend . Need to investigate .
Depth Stencil State roadmap __EoT__ - [ X ] Implement depth stencil state - [ X ] Make changes to next.json and state-tracking and null backend - [ X ] Implement on the OpenGL backend - [ X ] Implement on the Metal backend - [ X ] Add tests for depth stencil state - [ X ] Add validation tests - [ X ] For DepthStencilState creation # 26 - [ X ] ~~That it can not be used on a compute pipeline ( we are probably going to split the graphics and compute pipelines types down the line ) ~~ graphics and compute split # 80 - [ x ] Add end2end tests # 92 - [ ] Write documentation - [ ] Add a description of the API in the NXT doc - [ ] Write an investigation and explanation of why we designed the API this way .
Unmarshalling an array of objects ? __EoT__ If I 've got an array back as my Json body , and I want to unmarshal that into an array of structs , it appears , according to your docs , that I ca n't do this . Is this feature coming soon ?
Unmarshalling an array of objects ? __EoT__ If I 've got an array back as my Json body , and I want to unmarshal that into an array of structs , it appears , according to your docs , that I ca n't do this . Is this feature coming soon ?
Running into an error while unmarshalling requests __EoT__ Hello , I was putting together a simple proof of concept API using this package for encoding/decoding JSON-API endpoints but I am coming across an issue related to ID numbers . According to the spec [ here ] ( http : //jsonapi.org/format/ # crud-updating ) the ` id ` and ` type ` keys must be present within ` data ` . So following those guidelines , I send a request like so : `` ` json PATCH /path-to-endpoint HTTP/1.1 Content-Type : application/vnd.api+json Accept : application/vnd.api+json { `` data '' : { `` type '' : `` enterprises '' , `` id '' : `` 1154833676970755108 '' , `` attributes '' : { `` description '' : `` This is a test enterprise '' , `` name '' : `` Test Enterprise '' } } } `` ` My struct looks like this : `` ` go type Enterprise struct { ID int64 ` jsonapi : '' primary , enterprises '' ` InsertedByID *int64 ` jsonapi : '' attr , inserted-by-id '' ` UpdatedByID *int64 ` jsonapi : '' attr , updated-by-id '' ` Name *string ` jsonapi : '' attr ,
Link support __EoT__ It seems there is no links support . Any chance to have it ?
Link support __EoT__ It seems there is no links support . Any chance to have it ?
maktaba # ensure # IsCallable and maktaba # value # IsEnum broken in Vim > = 7.4.1875 , breaking Maktaba entirely __EoT__ I 'm an Arch Linux user , just after I updated Vim to the latest version by pacman , maktaba fails . I have already updated maktaba to latest version by vundle , sadly still got same error messages below . For this reason , I can not use codefmt in Vim . ERROR ( BadValue ) : Dictionary with keys [ 'dict ' , 'func ' , 'Apply ' , 'Call ' , 'WithContext ' , 'arglist ' , 'WithArgs ' ] is not a maktaba function . ERROR ( NotFound ) : Flag `` clang_format_executable '' not defined in codefmt . ERROR ( NotFound ) : Flag `` yapf_executable '' not defined in codefmt . ERROR ( NotFound ) : Flag `` js_beautify_executable '' not defined in codefmt . ERROR ( NotFound ) : Flag `` gofmt_executable '' not defined in codefmt .
maktaba # path # Split should preserve trailing slash __EoT__ A trailing slash on the input to ` maktaba # path # Split ` is treated the same way as input without a trailing slasht . The last path component in the returned list of components will have no trailing slash either way : `` ` vim echo maktaba # path # Split ( 'foo/bar/ ' ) `` ` ` [ 'foo ' , 'bar ' ] ` Maktaba should try to preserve trailing slashes in path manipulations since they help distinguish paths that represent a directory from paths that may represent a file . For instance , Join ( Split ( X ) ) loses the trailing slash : `` ` vim echo maktaba # path # Join ( maktaba # path # Split ( 'foo/bar/ ' ) ) `` ` ` foo/bar ` Instead , Split should maintain the slash in the last component : `` ` vim echo maktaba # path # Split ( 'foo/bar/ ' ) `` ` ` [ 'foo ' , 'bar/ ' ] ` It could instead include the empty string as a final component , but I do n't think that 's
# path # MakeRelative should n't strip trailing slash from { path } argument __EoT__ The output of ` maktaba # path # MakeRelative ` does n't have a trailing slash even if the ` { path } ` argument had one : ` maktaba # path # MakeRelative ( '/foo ' , '/foo/bar/ ' ) ` â†’ ` 'bar ' ` Since trailing slashes are significant in maktaba 's path handling , explicitly signifying a directory path instead of a file path , it would be better to preserve any trailing slash from input to output .
# path # MakeRelative should n't strip trailing slash from { path } argument __EoT__ The output of ` maktaba # path # MakeRelative ` does n't have a trailing slash even if the ` { path } ` argument had one : ` maktaba # path # MakeRelative ( '/foo ' , '/foo/bar/ ' ) ` â†’ ` 'bar ' ` Since trailing slashes are significant in maktaba 's path handling , explicitly signifying a directory path instead of a file path , it would be better to preserve any trailing slash from input to output .
Add helper to get current visual selection __EoT__ Plugins sometimes need to grab highlighted text , e.g . to implement visual-mode mappings that do something with highlighted text , but vim does n't seem to have any convenient support built in . Maktaba could have a helper like ` maktaba # buffer # GetVisualSelection ( ) ` that returns the text between the ` < ` and ` > ` marks .
syz-hub : propagate crashers __EoT__ We need to make syz-hub propagate crashers between different managers .
size ( int ) for CollectionSubject and MapSubject __EoT__ Example : { `` a '' : 1 , `` b '' : 2 } Right now there is no way to check if the map only contains key `` a '' . Add to CollectionSubject for consistency .
` ct.SignedCertificateTimestamp ` missing JSON struct tags __EoT__ Hi folks : wave : , While implementing a mock CT server for some tests using this project I noticed the ` ct.SignedCertificateTimestamp ` type from ` types.go ` is missing ` json ` struct tags : https : //github.com/google/certificate-transparency-go/blob/a961b728bb4444704ae1947ea0496ce7563c3705/types.go # L292-L301 The RFC 6962 sections referenced in that comment ( 3.2 , 4.1 and 4.2 ) provide JSON encoded responses . In my own code I work around this by creating a throw-away type with the required JSON tags but it seems like this type could provide the tags itself and save users some work : - )
ct_server.main does not properly check the return value of setupAndRegister __EoT__ The ` main ` function in ` ct_server\main.go ` does not properly check the return value of invoking ` setupAndRegister ` . Specifically , https : //github.com/google/certificate-transparency-go/blob/master/trillian/ctfe/ct_server/main.go # L151 does not assign the return value of the invocation of ` setupAndRegister ` to `` err '' . This defeats the following error check , leading to subsequent undesired behavior ( such as SIGSEGVs , etc . ) instead of failing fast with an appropriate error message and exiting .
Reuse Trillian Merkle Tree verifiers __EoT__ Proof verification code in [ merkletree/ ] ( https : //github.com/google/certificate-transparency-go/tree/master/merkletree ) looks the same as in Trillian 's [ merkle/ ] ( https : //github.com/google/trillian/tree/master/merkle ) folder . Could we just reuse Trillian code here ? Some features in Trillian are also needed in this repo , e.g . [ range inclusion verification ] ( https : //github.com/google/trillian/pull/1108 ) will be used in Migrillian to verify the fetched batches .
Invalid SCTs for PreCerts __EoT__ We 've seen some evidence that Trillian CTFE is generating SCTs for pre-certs that are not valid . This is being investigated .
Invalid SCTs for PreCerts __EoT__ We 've seen some evidence that Trillian CTFE is generating SCTs for pre-certs that are not valid . This is being investigated .
windows/go-1.11 compile error __EoT__ I am seeing the following error trying to use this package on Windows using go-1.11beta1 : > 13:11:05 ... .\vendor\github.com\google\certificate-transparency\go\x509\root_windows.go:112:3 : can not use uintptr ( unsafe.Pointer ( sslPara ) ) ( type uintptr ) as type syscall.Pointer in field value This is apparently caused by the following change in golang : https : //github.com/golang/go/commit/4869ec00e87ef49db2646c25d28d5c7e4f1caff8 and requires a one-line fix identical to the one in here : https : //github.com/golang/go/commit/4869ec00e87ef49db2646c25d28d5c7e4f1caff8 ? diff=unified # diff-9a0dd81b66ea632b270ed4c40cb71e43 which of course will break compilation with golang < 1.11 , so a more complicated fix is required . Found while trying to compile Docker with go-1.11beta1 on Windows ( https : //github.com/moby/moby/pull/37358 # issuecomment-401923825 )
NegativeArraySizeException in collection factory methods __EoT__ The immutable collections ` ImmutableDoubleArray ` , ` ImmutableLongArray ` , ` ImmutableIntArray ` , ` ImmutableList ` , and ` ImmutatbleSet ` all provide methods called ` of ` . If an array of length near ` Integer.MAX_VALUE ` is passed as the last argument , an unexpected ` NegativeArraySizeException ` is thrown . To reproduce , run this code on a computer with enough RAM : `` ` java import com.google.common.collect.ImmutableList ; public class NegArrSize { public static void main ( String [ ] args ) { Object [ ] array = new Object [ Integer.MAX_VALUE - 11 ] ; ImmutableList.of ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , array ) ; } } `` ` The program throws ` NegativeArraySizeException ` . The problem is that these methods do an unchecked addition to compute the size of an array that will be allocated . The methods work for most argument array sizes , but it would be helpful to document and check the restriction on the argument .
Test setValue ( null ) ( both success and failure cases ) __EoT__
Test setValue ( null ) ( both success and failure cases ) __EoT__
ImmutableEnum { Set , Map } Collectors __EoT__ Add collectors for ` enum ` -specialized ` ImmutableSet ` and ` ImmutableMap ` : - ` toImmutableEnumSet ( ) ` - ` toImmutableEnumMap ( keyFunction , valueFunction ) ` - ` toImmutableEnumMap ( keyFunction , valueFunction , mergeFunction ) ` Providing these collectors in Guava would be a matter of convenience . Users could implement functionally-equivalent collectors themselves , but I think most users would not do so . I think ` enum ` -specialized collections are underused and that it is worthwhile to promote their usage and remove friction / barriers to entry . For example , if a user has code like this : `` ` java ImmutableSet < Month > months = stream.collect ( ImmutableSet.toImmutableSet ( ) ) ; `` ` ... then it would be cool if they could optimize their code like this : `` ` java ImmutableSet < Month > months = stream.collect ( ImmutableSet.toImmutableEnumSet ( ) ) ; `` ` ... and it would be lame if they had to do this ( it would be so lame that they would n't bother ) : `` ` java ImmutableSet < Month > months =
non-blocking ConcurrentBloomFilter ? __EoT__ _ [ Original issue ] ( https : //code.google.com/p/guava-libraries/issues/detail ? id=1090 ) created by **ishaaq** on 2012-07-31 at 02:57 AM_ -- - Is it possible to create a Concurrent version of BloomFilter ? Semantics I am looking for : 1 . Ability for multiple threads to call put ( ) simultaneously without blocking 2 . A put ( x ) will ensure that all mightContain ( x ) calls that occur _after_ it will return true
SSLError : [ Errno 8 ] _ssl.c:510 : EOF occurred in violation of protocol __EoT__ We encountered this problem before , so we switched fromr ` urlfetch ` to ` requests ` in # 65 . It seems to work for us . However , one of our users has informed us that he is seeing this exception : `` ` Traceback ( most recent call last ) : File `` /usr/local/lib/python2.7/dist-packages/clusterfuzz/stackdriver_logging.py '' , line 128 , in wrapped func ( *args , **kwargs ) File `` /usr/local/lib/python2.7/dist-packages/clusterfuzz/commands/reproduce.py '' , line 227 , in execute response = get_testcase_info ( testcase_id ) File `` /usr/local/lib/python2.7/dist-packages/clusterfuzz/commands/reproduce.py '' , line 180 , in get_testcase_info return json.loads ( send_request ( CLUSTERFUZZ_TESTCASE_INFO_URL , data ) .text ) File `` /usr/local/lib/python2.7/dist-packages/clusterfuzz/commands/reproduce.py '' , line 162 , in send_request allow_redirects=True , data=data ) File `` /usr/local/lib/python2.7/dist-packages/requests/api.py '' , line 110 , in post return request ( 'post ' , url , data=data , json=json , **kwargs ) File `` /usr/local/lib/python2.7/dist-packages/requests/api.py '' , line 56 , in request return session.request ( method=method , url=url , **kwargs ) File `` /usr/local/lib/python2.7/dist-packages/requests/sessions.py '' , line 488 , in request resp = self.send ( prep , **send_kwargs ) File `` /usr/local/lib/python2.7/dist-packages/requests/sessions.py
Inconsistent behavior when llvm-symbolizer is not found __EoT__ For some reason , libFuzzer ASan build does n't have ` llvm-symbolizer ` . What happens when I try to reproduce such testcase with ` chromium ` mode : `` ` $ clusterfuzz reproduce 6083453533814784 -b chromium Reproduce 6083453533814784 ( current=False ) Downloading testcase information ... Running : python goma_ctl.py ensure_start Using goma VERSION=122 ( latest ) GOMA version 1058e0cf418d445b5735ab5243b31d887a1e8633 @ 1487568363 goma is already running . Downloading build data ... Running : gsutil cp gs : // < ... > . | [ 1 files ] [ 6.4 GiB/ 6.4 GiB ] 77.9 MiB/s Operation completed over 1 objects/6.4 GiB . Extracting ... Cleaning up ... Traceback ( most recent call last ) : File `` /usr/local/bin/clusterfuzz '' , line 11 , in < module > sys.exit ( execute ( ) ) File `` /usr/local/lib/python2.7/dist-packages/clusterfuzz/main.py '' , line 42 , in execute command.execute ( **arg_dict ) File `` /usr/local/lib/python2.7/dist-packages/clusterfuzz/stackdriver_logging.py '' , line 89 , in wrapped func ( *args , **kwargs ) File `` /usr/local/lib/python2.7/dist-packages/clusterfuzz/commands/reproduce.py '' , line 246 , in execute reproduce_crash ( binary_provider.get_binary_path ( ) , File `` /usr/local/lib/python2.7/dist-packages/clusterfuzz/binary_providers.py '' , line 109 , in get_binary_path return ' %
hostname case sensitivity __EoT__ We found that we can create hostnames of mixed case , both in and out of zone , through EPP . We would not expect that both ns1.UPPER.foo and ns1.upper.foo could be created for example based on our interpretation of RFC 952 that states : A `` name '' ( Net , Host , Gateway , or Domain name ) is a text string up to 24 characters ... No distinction is made between upper and lower case . ... . a mixed case domain < create > returns : Domain names can only contain a-z , 0-9 , ' . ' and '-' **On Wed , Oct 19 , 2016 at 3:46 PM , Nick Felt nickfelt @ google.com wrote : ** You 're right , this is a bug . Thanks for pointing it out . The real issue ( IMO ) is that we rely too much on Guava 's InternetDomainName.from ( ) method to do validation for us . It 's too permissive for my tastes , and for example , when constructing a domain name , it normalizes uppercase to lowercase . If you try to use this as a validity
hostname case sensitivity __EoT__ We found that we can create hostnames of mixed case , both in and out of zone , through EPP . We would not expect that both ns1.UPPER.foo and ns1.upper.foo could be created for example based on our interpretation of RFC 952 that states : A `` name '' ( Net , Host , Gateway , or Domain name ) is a text string up to 24 characters ... No distinction is made between upper and lower case . ... . a mixed case domain < create > returns : Domain names can only contain a-z , 0-9 , ' . ' and '-' **On Wed , Oct 19 , 2016 at 3:46 PM , Nick Felt nickfelt @ google.com wrote : ** You 're right , this is a bug . Thanks for pointing it out . The real issue ( IMO ) is that we rely too much on Guava 's InternetDomainName.from ( ) method to do validation for us . It 's too permissive for my tastes , and for example , when constructing a domain name , it normalizes uppercase to lowercase . If you try to use this as a validity
hostname case sensitivity __EoT__ We found that we can create hostnames of mixed case , both in and out of zone , through EPP . We would not expect that both ns1.UPPER.foo and ns1.upper.foo could be created for example based on our interpretation of RFC 952 that states : A `` name '' ( Net , Host , Gateway , or Domain name ) is a text string up to 24 characters ... No distinction is made between upper and lower case . ... . a mixed case domain < create > returns : Domain names can only contain a-z , 0-9 , ' . ' and '-' **On Wed , Oct 19 , 2016 at 3:46 PM , Nick Felt nickfelt @ google.com wrote : ** You 're right , this is a bug . Thanks for pointing it out . The real issue ( IMO ) is that we rely too much on Guava 's InternetDomainName.from ( ) method to do validation for us . It 's too permissive for my tastes , and for example , when constructing a domain name , it normalizes uppercase to lowercase . If you try to use this as a validity
hostname case sensitivity __EoT__ We found that we can create hostnames of mixed case , both in and out of zone , through EPP . We would not expect that both ns1.UPPER.foo and ns1.upper.foo could be created for example based on our interpretation of RFC 952 that states : A `` name '' ( Net , Host , Gateway , or Domain name ) is a text string up to 24 characters ... No distinction is made between upper and lower case . ... . a mixed case domain < create > returns : Domain names can only contain a-z , 0-9 , ' . ' and '-' **On Wed , Oct 19 , 2016 at 3:46 PM , Nick Felt nickfelt @ google.com wrote : ** You 're right , this is a bug . Thanks for pointing it out . The real issue ( IMO ) is that we rely too much on Guava 's InternetDomainName.from ( ) method to do validation for us . It 's too permissive for my tastes , and for example , when constructing a domain name , it normalizes uppercase to lowercase . If you try to use this as a validity
hostname case sensitivity __EoT__ We found that we can create hostnames of mixed case , both in and out of zone , through EPP . We would not expect that both ns1.UPPER.foo and ns1.upper.foo could be created for example based on our interpretation of RFC 952 that states : A `` name '' ( Net , Host , Gateway , or Domain name ) is a text string up to 24 characters ... No distinction is made between upper and lower case . ... . a mixed case domain < create > returns : Domain names can only contain a-z , 0-9 , ' . ' and '-' **On Wed , Oct 19 , 2016 at 3:46 PM , Nick Felt nickfelt @ google.com wrote : ** You 're right , this is a bug . Thanks for pointing it out . The real issue ( IMO ) is that we rely too much on Guava 's InternetDomainName.from ( ) method to do validation for us . It 's too permissive for my tastes , and for example , when constructing a domain name , it normalizes uppercase to lowercase . If you try to use this as a validity
from __future__ import print_function __EoT__ None of the future features are implemented , including the print function .
can not find package `` grumpy/lib/math '' __EoT__ Trying to wrap my hand around the above error . Does his expect to provide grumpy math library implementation similarly to say , time.py ?
Build gets into strange state when first invocation uses wrong version of Python __EoT__ [ @ localhost grumpy ] $ make build/src/grumpy/lib/itertools/module.go:5 : ca n't find import : `` grumpy/lib/sys '' make : *** [ build/pkg/linux_amd64/grumpy/lib/itertools.a ] Error 1
Contest proposal and release of training code and paper __EoT__ The proposal mentions that defense training code must be made public , which I agree will help make attackers as efficient as possible . I expect that some defenses will involve a careful combination of architecture and hyper-parameter selection , therefore a potential unintended side effect of this is that it could discourage defenders if someone else shows up at the last minute with more resources and runs more or less the same code with additional hyper-parameter tuning . Given that weights and test time code will already be public , would a reasonable compromise be to submit the training code to the review board only , for release at the conclusion of each 90 day period ? This might also limit attempts to obfuscate the code . I 'm assuming the process of staking a defense with an accompanying paper is meant to prevent the main scenario outlined above , but can you provide a bit more information about how this works in the case of seemingly similar defenses . And if a defense is broken ( the most probable outcome ) , how are re-submissions treated in terms
Add `` common corruption '' transformations as a warmup attack __EoT__ I think that the transformations from [ Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations by Dan Hendrycks and Tom Dietterich ] ( https : //arxiv.org/abs/1807.01697 ) would be a good addition to the warmup attacks . ! [ image ] ( https : //user-images.githubusercontent.com/306655/45835540-3d6b0780-bcbf-11e8-82d9-6b3bd8939260.png ) Three reasons why I think that this would be a good addition : 1 . Their corruptions clearly illustrate a set of non-lp-restricted attacks 2 . These corruptions are strictly easier than an unrestricted adversary 3 . They have a [ simple and well-designed codebase ] ( https : //github.com/hendrycks/robustness ) # # # # Tasks - [ x ] Check their license ( ðŸ‘ Apache is good ) - [ x ] Convert [ their library ] ( https : //github.com/hendrycks/robustness ) into a pip module ( either in place or copy it into this repository ) - [ x ] Add all transformations at severity 1 as a warmup attack on the ` bird-or-bicycle ` dataset ( Perhaps 10 transformations per image ) - [ x ] Evaluate baseline defenses - [ x ] Inspect confident misclassifications to ensure
Add `` common corruption '' transformations as a warmup attack __EoT__ I think that the transformations from [ Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations by Dan Hendrycks and Tom Dietterich ] ( https : //arxiv.org/abs/1807.01697 ) would be a good addition to the warmup attacks . ! [ image ] ( https : //user-images.githubusercontent.com/306655/45835540-3d6b0780-bcbf-11e8-82d9-6b3bd8939260.png ) Three reasons why I think that this would be a good addition : 1 . Their corruptions clearly illustrate a set of non-lp-restricted attacks 2 . These corruptions are strictly easier than an unrestricted adversary 3 . They have a [ simple and well-designed codebase ] ( https : //github.com/hendrycks/robustness ) # # # # Tasks - [ x ] Check their license ( ðŸ‘ Apache is good ) - [ x ] Convert [ their library ] ( https : //github.com/hendrycks/robustness ) into a pip module ( either in place or copy it into this repository ) - [ x ] Add all transformations at severity 1 as a warmup attack on the ` bird-or-bicycle ` dataset ( Perhaps 10 transformations per image ) - [ x ] Evaluate baseline defenses - [ x ] Inspect confident misclassifications to ensure
Add `` common corruption '' transformations as a warmup attack __EoT__ I think that the transformations from [ Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations by Dan Hendrycks and Tom Dietterich ] ( https : //arxiv.org/abs/1807.01697 ) would be a good addition to the warmup attacks . ! [ image ] ( https : //user-images.githubusercontent.com/306655/45835540-3d6b0780-bcbf-11e8-82d9-6b3bd8939260.png ) Three reasons why I think that this would be a good addition : 1 . Their corruptions clearly illustrate a set of non-lp-restricted attacks 2 . These corruptions are strictly easier than an unrestricted adversary 3 . They have a [ simple and well-designed codebase ] ( https : //github.com/hendrycks/robustness ) # # # # Tasks - [ x ] Check their license ( ðŸ‘ Apache is good ) - [ x ] Convert [ their library ] ( https : //github.com/hendrycks/robustness ) into a pip module ( either in place or copy it into this repository ) - [ x ] Add all transformations at severity 1 as a warmup attack on the ` bird-or-bicycle ` dataset ( Perhaps 10 transformations per image ) - [ x ] Evaluate baseline defenses - [ x ] Inspect confident misclassifications to ensure
Record source image filename for misclassified images __EoT__ Running ` evaluate_bird_or_bicycle_model ` gives you images , but currently no information about where the image came from . We should make that easier to do . As an example of why this is important , I found this ambiguous image in the test set , and want to track down how it got into the dataset . ! [ image ] ( https : //user-images.githubusercontent.com/306655/45469328-e1f8b280-b6dd-11e8-8ccb-7f1808420953.png )
SpatialAttack in eval_kit is sometimes extremely slow __EoT__ This attack should take no more than 1 hour . I ran it for 24 hours on the ` undefended_keras_resnet ` and it only completed 50 % . This replaces the attack with a faster implementation that is pending in cleverhans . https : //github.com/tensorflow/cleverhans/pull/623 I 've copied the attack into our repo for now , and will swap to the cleverhans version after it is released .
SpatialAttack in eval_kit is sometimes extremely slow __EoT__ This attack should take no more than 1 hour . I ran it for 24 hours on the ` undefended_keras_resnet ` and it only completed 50 % . This replaces the attack with a faster implementation that is pending in cleverhans . https : //github.com/tensorflow/cleverhans/pull/623 I 've copied the attack into our repo for now , and will swap to the cleverhans version after it is released .
SpatialAttack in eval_kit is sometimes extremely slow __EoT__ This attack should take no more than 1 hour . I ran it for 24 hours on the ` undefended_keras_resnet ` and it only completed 50 % . This replaces the attack with a faster implementation that is pending in cleverhans . https : //github.com/tensorflow/cleverhans/pull/623 I 've copied the attack into our repo for now , and will swap to the cleverhans version after it is released .
Upgrade to TypeScript 2.8 __EoT__ Opening issues for the 1.0 milestone . TypeScript 2.8 seems like a good target : https : //twitter.com/typescriptlang/status/974394621601095680 . This needs to happen in a semver major , so 1.0 seems like the right time to do this .
Allow leading underscores in variable names __EoT__ The tsconfig file provided here includes the ` noUnusedParameters ` compiler option , which is useful . However , sometimes when implementing an interface , some parameters are not actually useful . In that case , if the argument name starts with an underscore , the compiler will not emit an error ( see [ this documentation ] ( https : //github.com/Microsoft/TypeScript/wiki/What % 27s-new-in-TypeScript # flag-unused-declarations-with -- -nounusedparameters-and -- -nounusedlocals ) ) . Currently , if I try to do this , then I try to lint my files using the TSLint configuration provided here , it fails because the lint configuration does not allow any underscores in non-constant names . Please fix this by adding the ` `` allow-leading-underscore '' ` option to the TSLint ` `` variable-name '' ` rule configuration .
`` Can not find module 'typescript ' '' when running ` npx google-ts-style @ 0.2 init ` __EoT__ Despite installing ` typescript ` , the module requires ` typescript ` to be installed locally first . I think this was n't the behavior in 0.1.2 . It seems like some cases , this might not work even with locally installed ` typescript ` .
Should gts fix or gts check find unused variables ? __EoT__ I had the following test case where ` durationMillis ` is unused . If I run ` gts fix ` or ` gts check ` I do n't get any output . Should this problem be caught ? `` ` it ( 'should throw error when disabled ' , async ( ) = > { const durationMillis = 10 * 1000 ; const intervalBytes = 1024 * 512 ; const stackDepth = 32 ; const profiler = new HeapProfiler ( intervalBytes , stackDepth ) ; profiler.disable ( ) ; try { const profile = await profiler.profile ( ) ; assert.fail ( 'Expected error to be thrown . ' ) ; } catch ( err ) { assert.equal ( err.message , 'Heap profiler is not enabled . ' ) ; } } ) ; `` `
Should gts fix or gts check find unused variables ? __EoT__ I had the following test case where ` durationMillis ` is unused . If I run ` gts fix ` or ` gts check ` I do n't get any output . Should this problem be caught ? `` ` it ( 'should throw error when disabled ' , async ( ) = > { const durationMillis = 10 * 1000 ; const intervalBytes = 1024 * 512 ; const stackDepth = 32 ; const profiler = new HeapProfiler ( intervalBytes , stackDepth ) ; profiler.disable ( ) ; try { const profile = await profiler.profile ( ) ; assert.fail ( 'Expected error to be thrown . ' ) ; } catch ( err ) { assert.equal ( err.message , 'Heap profiler is not enabled . ' ) ; } } ) ; `` `
test : add system test for ` gts fix ` __EoT__
Thread 2 `` FEngine : :loop '' received signal SIGSEGV , Segmentation fault . __EoT__ OK ... vk_strobecolor works , but when I try vk_hellotriangle , I get a SIGSEGV : ( gdb ) run Starting program : /home/kjh/filament/out/cmake-debug/samples/vk_hellotriangle Missing separate debuginfos , use : dnf debuginfo-install glibc-2.27-30.fc28.x86_64 [ Thread debugging using libthread_db enabled ] Using host libthread_db library `` /lib64/libthread_db.so.1 '' . FEngine ( 64 bits ) created at 0x7ffff5be7010 [ New Thread 0x7ffff4fe4700 ( LWP 31293 ) ] FEngine resolved backend : Vulkan INTEL-MESA : warning : Bay Trail Vulkan support is incomplete Selected physical device : Intel ( R ) Bay Trail warning : Loadable section `` .note.gnu.property '' outside of ELF segments warning : Loadable section `` .note.gnu.property '' outside of ELF segments warning : Loadable section `` .note.gnu.property '' outside of ELF segments warning : Loadable section `` .note.gnu.property '' outside of ELF segments Missing separate debuginfo for /lib64/libGLX_mesa.so.0 Try : dnf -- enablerepo='*debug* ' install /usr/lib/debug/.build-id/aa/7b9e010d068bc3af7d14f1491eb0e37809bff0.debug Missing separate debuginfo for /lib64/libglapi.so.0 Try : dnf -- enablerepo='*debug* ' install /usr/lib/debug/.build-id/c9/9c6d22a33c958d0a790732dabd8c38d1c22d1e.debug warning : Loadable section `` .note.gnu.property '' outside of ELF segments Missing separate debuginfo for /usr/lib64/dri/i965_dri.so Try : dnf -- enablerepo='*debug* ' install /usr/lib/debug/.build-id/21/a8eb9367b7c6065170ed4ce6690914c0be6f8f.debug [
Windows CI builds have filament-java.jar built with the Java 9 SDK __EoT__ We should explicitly check for Java 8
glTF viewer not showing metal/rough textures __EoT__ **Describe the bug** When testing a couple metal/rough models in glTF ( ` .glb ` ) format with the ` gltf_viewer ` sample , it appears that the metal/rough materials are not being applied . **To Reproduce** Steps to reproduce the behavior : 1 . I tried viewing the following models with ` gltf_viewer ` . Both are collections of spheres with varying metal/roughness values . 2 . ` samples/gltf_viewer -- ibl samples/envs/desert/ test_spheres.glb ` [ Archive.zip ] ( https : //github.com/google/filament/files/2598059/Archive.zip ) **Expected behavior** A clear and concise description of what you expected to happen . Result should look something like this : ! [ screen shot 2018-11-19 at 8 21 07 pm ] ( https : //user-images.githubusercontent.com/1848368/48751330-ab3c9c80-ec38-11e8-8c97-f514f593e3ae.png ) **Screenshots** If applicable , add screenshots to help explain your problem . Actual results do not show metal/rough values : | test_spheres | MetalRoughSpheres | | -- -| -- -| | ! [ screen shot 2018-11-19 at 8 22 20 pm ] ( https : //user-images.githubusercontent.com/1848368/48751370-d8894a80-ec38-11e8-995f-3783048f7669.png ) | ! [ screen shot 2018-11-19 at 8 22 07 pm ] ( https : //user-images.githubusercontent.com/1848368/48751371-daeba480-ec38-11e8-90d4-3796341f0da7.png ) | **Desktop ( please complete the following information )
Add utility for computing tangents __EoT__ Per discussion with Mathias and Aaron , we 'd like to add a static utility method to ` VertexBuffer ` that makes it easier for clients to supply a buffer for ` TANGENTS ` . It could look like this : `` ` cpp /** * Convenience function that consumes normal vectors ( and , optionally , tangent vectors ) * and produces quaternions that can be passed into a TANGENTS buffer . * * The given target buffer must be preallocated with at least count * sizeof ( math : :quat ) bytes . * * Normals are required but tangents are optional , in which case this function tries to generate * reasonable tangents . The given normals should be unit length . * * If supplied , the tangent vectors should be unit length and should be orthogonal to the normals . * The w component of the tangent is a sign ( -1 or +1 ) indicating handedness of the basis . * * Note that some applications and file formats ( e.g . Blender and glTF ) use mikktspace , which * requires full topology information and therefore can
Vulkan Memory Allocator usage in VulkanStagePool class __EoT__ Hello , I 'm the author of Vulkan Memory Allocator library . Thanks for using the library in your project . I would like to suggest that in file ` VulkanStagePool.cpp ` , in function ` VulkanStagePool : :acquireStage ` , memory usage should be ` VMA_MEMORY_USAGE_CPU_ONLY ` rather than ` VMA_MEMORY_USAGE_CPU_TO_GPU ` because that 's the flag recommended for buffers that are used only as a source of transfer and not directly accessed by GPU while rendering . By the way , I would also suggest to use version of the library from `` development '' branch , not from `` master '' , as this release is quite old . You can find there functions to flush and invalidate memory ( ` vmaFlushAllocation ` , ` vmaInvalidateAllocation ` ) , which you should do before/after mapping for memory types that are not coherent . This is important on mobile platforms , and I could n't find any such calls ( like ` vkFlushMappedMemoryRanges ` / ` vkInvalidateMappedMemoryRanges ` ) in your current code .
Set presentation time on Android right before eglSwapBuffers ( ) __EoT__ To facilitate media recording , etc . we should set the presentation time just before swapping buffers .
Set presentation time on Android right before eglSwapBuffers ( ) __EoT__ To facilitate media recording , etc . we should set the presentation time just before swapping buffers .
Clearer API enforcement using proguard and/or doclava __EoT__ Currently Volley release builds are n't proguard-obfuscated/optimized . This means that any public method becomes part of the public API . This can be unfortunate when we want to keep something private to the base package and the .toolbox package without making it a public API , as we have no choice but to copy the method or go ahead and make it public . There are also a number of ad-hoc unit tests which reflect to access every expected public method and make sure they exist , which is a maintenance burden . It 'd be good to : - Proguard the builds we can `` @ hide '' public methods we do n't want to include in builds - Auto-generate a file containing the full API `` signature '' across all of Volley , so we can more clearly monitor it for changes . Fail builds if the signature changes without updating this file . This is similar to the Android API process ( the current.txt file ) .
Add main fileld to package.json __EoT__ Is it possible to add `` main '' field to package.json configuration file ? We are currently setting it to `` bin/prettify.min.js '' after npm update . This is necessary in order to use this library with angular-cli . If approved , I can provide a pull request .
Add main fileld to package.json __EoT__ Is it possible to add `` main '' field to package.json configuration file ? We are currently setting it to `` bin/prettify.min.js '' after npm update . This is necessary in order to use this library with angular-cli . If approved , I can provide a pull request .
/api/teams is O ( n ) again ! __EoT__ /api/teams has fallen back to O ( n ) somehow .
/api/teams is O ( n ) again ! __EoT__ /api/teams has fallen back to O ( n ) somehow .
/api/teams is O ( n ) again ! __EoT__ /api/teams has fallen back to O ( n ) somehow .
/api/teams is O ( n ) again ! __EoT__ /api/teams has fallen back to O ( n ) somehow .
/api/teams is O ( n ) again ! __EoT__ /api/teams has fallen back to O ( n ) somehow .
Support for many to many attachments __EoT__ To quote from : https : //github.com/Matir/pwnableweb-scoreboard/issues/89 '' Complexity : Deleting one attachment from one challenge - do we cascade delete ? leave them as is ? '' It might be worthwhile having a new admin section called `` Attachments '' where admins can upload new files , and then having a drop down style box . For orphaned files - this could be identified by having the number of links to the attachment displayed - ala foo.zip ( 1 ) vs bar.zip ( 0 )
Support for many to many attachments __EoT__ To quote from : https : //github.com/Matir/pwnableweb-scoreboard/issues/89 '' Complexity : Deleting one attachment from one challenge - do we cascade delete ? leave them as is ? '' It might be worthwhile having a new admin section called `` Attachments '' where admins can upload new files , and then having a drop down style box . For orphaned files - this could be identified by having the number of links to the attachment displayed - ala foo.zip ( 1 ) vs bar.zip ( 0 )
Create invite-only mode __EoT__ Challenges ca n't be seen without login , and signing up requires an invite code .
Doc : write glsl-reduce walkthrough __EoT__
Doc : write glsl-reduce walkthrough __EoT__
Doc : write glsl-reduce walkthrough __EoT__
Doc : write glsl-reduce walkthrough __EoT__
Doc : write glsl-reduce walkthrough __EoT__
Doc : write glsl-reduce walkthrough __EoT__
Doc : write glsl-reduce walkthrough __EoT__
Doc : write glsl-reduce walkthrough __EoT__
Doc : write glsl-reduce intro __EoT__
Doc : write glsl-reduce intro __EoT__
Doc : write glsl-reduce intro __EoT__
Doc : write glsl-reduce intro __EoT__
Doc : write glsl-reduce intro __EoT__
Doc : write glsl-reduce intro __EoT__
Doc : write glsl-reduce intro __EoT__
Layer.Digest ( ) takes an absurdly long time ? __EoT__ I have some code that retrieves the layers of an image and unpacks them individually on disk . Running this on a normal image takes ~2 seconds : `` ` for _ , layer : = range layers { contents , _ : = layer.Uncompressed ( ) return unpackTar ( tar.NewReader ( contents ) ) } `` ` `` ` INFO [ 0000 ] getting image for name daemon : //gcr.io/google-appengine/python : latest INFO [ 0010 ] time elapsed retrieving image from daemon : 10.660759s INFO [ 0011 ] time elapsed retrieving layer : 0.349497s INFO [ 0011 ] time elapsed retrieving layer : 0.012715s INFO [ 0011 ] time elapsed retrieving layer : 0.012462s INFO [ 0011 ] time elapsed retrieving layer : 0.655020s INFO [ 0011 ] time elapsed retrieving layer : 0.251492s INFO [ 0012 ] time elapsed retrieving layer : 0.179278s INFO [ 0012 ] time elapsed retrieving layer : 0.199690s INFO [ 0012 ] time elapsed retrieving layer : 0.015826s INFO [ 0012 ] time elapsed retrieving layer : 0.015350s INFO [ 0012 ] time elapsed retrieving layer : 0.149267s INFO [ 0012 ] time
Refactor v1.Image to use a v1.Layer abstraction __EoT__ I am considering refactoring ` v1.Image ` to use a second interface type ` v1.Layer ` to cut down on the cross-product of methods currently present . `` ` go // Layer is an interface for accessing the properties of a particular layer of a v1.Image type Layer interface { // Digest returns the Hash of the compressed layer . Digest ( ) ( Hash , error ) // DiffID returns the Hash of the uncompressed layer . DiffID ( ) ( Hash , error ) // Compressed returns an io.ReadCloser for the compressed layer contents . Compressed ( ) ( io.ReadCloser , error ) // Uncompressed returns an io.ReadCloser for the uncompressed layer contents . Uncompressed ( ) ( io.ReadCloser , error ) // Size returns the compressed size of the Layer . Size ( ) ( int64 , error ) } // Image defines the interface for interacting with an OCI v1 image . type Image interface { // Layers returns the ordered collection of filesystem layers that comprise this image . // The order of the list is most-recent first , and oldest base layer last . Layers (
Manifest digest mismatch against Dockerhub __EoT__ `` ` package main import ( `` log '' `` net/http '' `` github.com/google/go-containerregistry/authn '' `` github.com/google/go-containerregistry/name '' `` github.com/google/go-containerregistry/v1/remote '' ) func main ( ) { ref , err : = name.NewTag ( `` mirror.gcr.io/library/ubuntu '' , name.WeakValidation ) if err ! = nil { log.Fatalf ( `` NewTag : % v '' , err ) } img , err : = remote.Image ( ref , authn.Anonymous , http.DefaultTransport ) if err ! = nil { log.Fatalf ( `` Image : % v '' , err ) } dig , err : = img.Digest ( ) if err ! = nil { log.Fatalf ( `` Digest : % v '' , err ) } log.Println ( dig ) } `` ` Running this program I get : `` ` 2018/04/11 22:40:24 Digest : manifest digest : sha256:52286464db54577a128fa1b1aa3c115bd86721b490ff4cbd0cd14d190b66c570 does not match Docker-Content-Digest : sha256 : e348fbbea0e0a0e73ab0370de151e7800684445c509d46195aef73e090a49bd6 exit status 1 `` ` Changing the reference to ` mirror.gcr.io/library/ubuntu ` prints the digest , ` 52286484db ... ` without any error .
Fix rebase_cloudbuild.yaml __EoT__ I broke it : https : //github.com/google/go-containerregistry/pull/217
Unified CLI surface __EoT__ The ` cmd ` directory currently has four separate binaries a user can use to interact with a remote registry : ` puller ` , ` pusher ` , ` poke ` ( which gets only metadata ) , and ` deleter ` . It looks like we 'll be adding ` appender ` soon in # 63 . I think we should head off the proliferation of separate binaries and move them into a single command with subcommands for each operation . This will allow users to install a single binary into their path , and could allow some code reuse and help standardize conventions for the commands . This would also make it easier to package as a [ builder image ] ( https : //github.com/GoogleCloudPlatform/cloud-builders ) for GCB . # # Subcommands * ` get ` ( instead of ` poke ` ) * ` pull ` * ` push ` * ` delete ` * ` append ` * ` tag ( list|add|remove ) ` * ` label ( list|add|remove ) ` * [ ` flatten ` ] ( https : //github.com/google/containerregistry # flattenpar ) * [ ` rebase ` ] (
Experiment : //cmd/ko __EoT__ **tl ; dr** I want a more purpose built variation of what you get with ` bazel ` + ` rules_go ` + ` rules_docker ` + ` rules_k8s ` that simply wraps the Go toolchain directly . # # # Background : State of Bazel Today in Bazel , my build is described in ` BUILD ` files ( largely generated by Gazelle ) and extended to handle containerization and kubernetesification via : `` ` python # BEGIN generated by Gazelle go_library ( name = `` go_default_library '' , ... ) go_test ( name = `` go_default_test '' , ... ) go_binary ( name = `` name-of-directory '' , ... ) # END generated by Gazelle # Wrap the Go binary into a minimal container image . go_image ( name = `` image '' , binary = `` : name-of-directory '' , ) # Helper for interacting with my K8s `` Deployment '' k8s_object ( name = `` deployment '' , template = `` deployment.yaml '' , images = { # Associate the image reference in deployment.yaml with the associated binary target . `` gcr.io/foo/bar : baz '' : `` : image '' , }
Semantic difference between v1.Image.Layers & serialization format forces people to think __EoT__ ` v1.Image.Layers ( ) ` returns the layers with the most recent being first . In contrast , the serialization format ( [ OCI Image Spec ] ( https : //github.com/opencontainers/image-spec/blob/master/manifest.md # image-manifest-property-descriptions ) ) orders layers with the base being first . Similarly in the [ config spec ] ( https : //github.com/opencontainers/image-spec/blob/master/config.md # properties ) ` rootfs.diff_ids ` are ordered to match the manifest layers . This difference led to some bugs : [ pull/63/commits/bc06df7c5dc16c72b9f1968a3e46a8aedfd4a878 ] ( https : //github.com/google/go-containerregistry/pull/63/commits/bc06df7c5dc16c72b9f1968a3e46a8aedfd4a878 ) I think we should ~~change the OCI Image Spec~~ be consistent and have ` v1.Image.Layers ( ) ` return the base layer first .
ko resolve should not include output from the push __EoT__
Support registry-less minikube development in ko via daemon.Write __EoT__ Today ko 's [ ` resolveFile ` ] ( https : //github.com/google/go-containerregistry/blob/master/cmd/ko/resolve.go # L77 ) unconditionally uses ` publish.NewDefault ` . ` rules_k8s ` [ never cracked minikube development ] ( https : //github.com/bazelbuild/rules_k8s/issues/9 ) , but the goal was to publish images to the exposed minikube daemon . The python library never supported ` daemon.Write ` , but we do : ) I 'd like us to expose an implementation of [ ` publish.Interface ` ] ( https : //github.com/google/go-containerregistry/blob/master/ko/publish/publish.go # L23 ) that publishes to an exposed daemon via ` daemon.Write ` , and hook into this in ` ko ` via a flag : ` -- mk ` or ` -- minikube ` . WDYT ?
Rename d8s __EoT__ ` d8s ` is a placeholder name , we should find a better name before this one sticks . ` imgctl ` and ` regctl ` seem to be the current leaders . Your ideas welcome .
[ windows ] flutter_location needs a .bat version __EoT__ Currently tools/flutter_location is only a bash script , so Windows still requires that the Flutter tree be at a known location for the tooling to work . We need a .bat port , at which point update_flutter_engine.bat can use it the way update_flutter_engine uses flutter_location .
[ windows ] flutter_location needs a .bat version __EoT__ Currently tools/flutter_location is only a bash script , so Windows still requires that the Flutter tree be at a known location for the tooling to work . We need a .bat port , at which point update_flutter_engine.bat can use it the way update_flutter_engine uses flutter_location .
Windows Support __EoT__ It seems like there should be an issue tracking all major operating systems . Windows support would also be awesome ! : )
MissingPluginException : the plugin works but an exception is throwed __EoT__ Hi , first : thanks for sharing this project ! it 's really exciting : ) I started to play and tried to use plugins , but even if the call works , I get a MissingPluginException . `` ` dart Future _colorPick ( ) async { final channel = new MethodChannel ( 'flutter/colorpanel ' , const JSONMethodCodec ( ) ) ; final res = await channel.invokeMethod ( 'ColorPanel.Show ' ) ; } `` ` For instance when I use this method to open the colorPicker , the picker is opened , but I see an exception log . `` ` [ ERROR : topaz/lib/tonic/logging/dart_error.cc ( 16 ) ] Unhandled exception : MissingPluginException ( No implementation found for method ColorPanel.Show on channel flutter/colorpanel ) # 0 MethodChannel.invokeMethod ( package : flutter/src/services/platform_channel.dart:153 ) < asynchronous suspension > # 1 _MyHomePageState._colorPick ( file : ///Users/rxlabz/dev/labz/flutter/desktop/flutter_desktop2/example_flutter/lib/main.dart:81 ) < asynchronous suspension > `` ` Did I missed something ?
MissingPluginException : the plugin works but an exception is throwed __EoT__ Hi , first : thanks for sharing this project ! it 's really exciting : ) I started to play and tried to use plugins , but even if the call works , I get a MissingPluginException . `` ` dart Future _colorPick ( ) async { final channel = new MethodChannel ( 'flutter/colorpanel ' , const JSONMethodCodec ( ) ) ; final res = await channel.invokeMethod ( 'ColorPanel.Show ' ) ; } `` ` For instance when I use this method to open the colorPicker , the picker is opened , but I see an exception log . `` ` [ ERROR : topaz/lib/tonic/logging/dart_error.cc ( 16 ) ] Unhandled exception : MissingPluginException ( No implementation found for method ColorPanel.Show on channel flutter/colorpanel ) # 0 MethodChannel.invokeMethod ( package : flutter/src/services/platform_channel.dart:153 ) < asynchronous suspension > # 1 _MyHomePageState._colorPick ( file : ///Users/rxlabz/dev/labz/flutter/desktop/flutter_desktop2/example_flutter/lib/main.dart:81 ) < asynchronous suspension > `` ` Did I missed something ?
[ linux ] Support raw key events __EoT__
[ linux ] Support raw key events __EoT__
Vulkan : Support SSBO __EoT__
Destroy Vulkan resources properly __EoT__ We implemented Shutdown ( ) methods of classes but did not call them at the end of execution .
vkscript spec : stencil value is an integer in Vulkan and Dawn __EoT__ The stencil value should be an unsigned integer . See https : //www.khronos.org/registry/vulkan/specs/1.0-wsi_extensions/html/vkspec.html # VkClearDepthStencilValue Similarly , Dawn has stencil value as uint32_t : RenderPassDescriptorBuilder const & SetDepthStencilAttachmentClearValue ( float clearDepth , uint32_t clearStencil ) const ; docs/vk_script.md currently says stencil defaults to 0.0 I think it should default to 0 ( an integer ) .
Cleanup Requirements code __EoT__ The current code which passes the [ require ] information into the engines is ... not good . Need to refactor the code to make it easier to extend .
Cleanup Requirements code __EoT__ The current code which passes the [ require ] information into the engines is ... not good . Need to refactor the code to make it easier to extend .
Cleanup Requirements code __EoT__ The current code which passes the [ require ] information into the engines is ... not good . Need to refactor the code to make it easier to extend .
Cleanup Requirements code __EoT__ The current code which passes the [ require ] information into the engines is ... not good . Need to refactor the code to make it easier to extend .
Cleanup Requirements code __EoT__ The current code which passes the [ require ] information into the engines is ... not good . Need to refactor the code to make it easier to extend .
vkWaitForFences timeout for slow implementations __EoT__ The ` CommandBuffer : :SubmitAndReset ` method uses a hardcoded timeout of 100 ms for the ` vkWaitForFences ` call . I 'm sure this works great for all real Vulkan implementations , but for poor old emulators like Talvos this causes problems . For the ` draw_triangles ` test case I can scrape through if I bump the timeout to 200 ms ( or switch to a Release build ) , but I 'm guessing it might start failing again for larger test cases ( or slower CPUs ) . Assuming that you do n't want to raise this hardcoded limit to O ( seconds ) just to accommodate Talvos ' inefficiencies , would you perhaps accept a PR to enable the limit to be dynamically increased via an environment variable or command-line flag ? P.S . This looks to be an incredibly useful project , thanks for making it available !
Windows build __EoT__ Make sure everything builds and works on Windows . ( This does not include the Dawn backend at this time ) .
Windows build __EoT__ Make sure everything builds and works on Windows . ( This does not include the Dawn backend at this time ) .
Support for gradle experimental android plugin __EoT__ Hi ! Is it possible to use this plugin with the [ gradle experimental android plugin ] ( http : //tools.android.com/tech-docs/new-build-system/gradle-experimental ) ? Since the experimental plugin requires gradle 2.10 , I 've tried it with release 0.7.5 of this plugin , but it does n't seem to work , it says it requires the android plugin . Is there a way to work around this ?
Error on build in Android Studio 3 Canary & Gradle 4 __EoT__ Just testing the new canary release of Android Studio 3 , and I am getting this error on build . `` ` Error : Could not determine the dependencies of task ' : app : extractIncludeDebugProto ' . > Resolving configuration 'debugCompile ' directly is not allowed `` ` It might be possible to work around in ` app/build.gradle ` with the protobuf compilation configuration . However , my gradle-fu is insufficient to figure it out right now .
Error on build in Android Studio 3 Canary & Gradle 4 __EoT__ Just testing the new canary release of Android Studio 3 , and I am getting this error on build . `` ` Error : Could not determine the dependencies of task ' : app : extractIncludeDebugProto ' . > Resolving configuration 'debugCompile ' directly is not allowed `` ` It might be possible to work around in ` app/build.gradle ` with the protobuf compilation configuration . However , my gradle-fu is insufficient to figure it out right now .
Error on build in Android Studio 3 Canary & Gradle 4 __EoT__ Just testing the new canary release of Android Studio 3 , and I am getting this error on build . `` ` Error : Could not determine the dependencies of task ' : app : extractIncludeDebugProto ' . > Resolving configuration 'debugCompile ' directly is not allowed `` ` It might be possible to work around in ` app/build.gradle ` with the protobuf compilation configuration . However , my gradle-fu is insufficient to figure it out right now .
Add TravisCI support __EoT__ Add TravisCI support Right after cloning the project , simply running ` gradlew tasks ` fails : `` ` FAILURE : Build failed with an exception . * What went wrong : A problem occurred configuring project ' : testProject ' . > Could not resolve all dependencies for configuration ' : testProject : classpath ' . > Could not find com.google.protobuf : protobuf-gradle-plugin:0.7.8-SNAPSHOT . Searched in the following locations : https : //jcenter.bintray.com/com/google/protobuf/protobuf-gradle-plugin/0.7.8-SNAPSHOT/maven-metadata.xml https : //jcenter.bintray.com/com/google/protobuf/protobuf-gradle-plugin/0.7.8-SNAPSHOT/protobuf-gradle-plugin-0.7.8-SNAPSHOT.pom https : //jcenter.bintray.com/com/google/protobuf/protobuf-gradle-plugin/0.7.8-SNAPSHOT/protobuf-gradle-plugin-0.7.8-SNAPSHOT.jar file : /Users/noname/.m2/repository/com/google/protobuf/protobuf-gradle-plugin/0.7.8-SNAPSHOT/maven-metadata.xml file : /Users/noname/.m2/repository/com/google/protobuf/protobuf-gradle-plugin/0.7.8-SNAPSHOT/protobuf-gradle-plugin-0.7.8-SNAPSHOT.pom file : /Users/noname/.m2/repository/com/google/protobuf/protobuf-gradle-plugin/0.7.8-SNAPSHOT/protobuf-gradle-plugin-0.7.8-SNAPSHOT.jar Required by : protobuf-gradle-plugin : testProject : unspecified * Try : Run with -- stacktrace option to get the stack trace . Run with -- info or -- debug option to get more log output . BUILD FAILED Total time : 9.575 secs `` ` Adding CI will fix these issues .
Add TravisCI support __EoT__ Add TravisCI support Right after cloning the project , simply running ` gradlew tasks ` fails : `` ` FAILURE : Build failed with an exception . * What went wrong : A problem occurred configuring project ' : testProject ' . > Could not resolve all dependencies for configuration ' : testProject : classpath ' . > Could not find com.google.protobuf : protobuf-gradle-plugin:0.7.8-SNAPSHOT . Searched in the following locations : https : //jcenter.bintray.com/com/google/protobuf/protobuf-gradle-plugin/0.7.8-SNAPSHOT/maven-metadata.xml https : //jcenter.bintray.com/com/google/protobuf/protobuf-gradle-plugin/0.7.8-SNAPSHOT/protobuf-gradle-plugin-0.7.8-SNAPSHOT.pom https : //jcenter.bintray.com/com/google/protobuf/protobuf-gradle-plugin/0.7.8-SNAPSHOT/protobuf-gradle-plugin-0.7.8-SNAPSHOT.jar file : /Users/noname/.m2/repository/com/google/protobuf/protobuf-gradle-plugin/0.7.8-SNAPSHOT/maven-metadata.xml file : /Users/noname/.m2/repository/com/google/protobuf/protobuf-gradle-plugin/0.7.8-SNAPSHOT/protobuf-gradle-plugin-0.7.8-SNAPSHOT.pom file : /Users/noname/.m2/repository/com/google/protobuf/protobuf-gradle-plugin/0.7.8-SNAPSHOT/protobuf-gradle-plugin-0.7.8-SNAPSHOT.jar Required by : protobuf-gradle-plugin : testProject : unspecified * Try : Run with -- stacktrace option to get the stack trace . Run with -- info or -- debug option to get more log output . BUILD FAILED Total time : 9.575 secs `` ` Adding CI will fix these issues .
Add TravisCI support __EoT__ Add TravisCI support Right after cloning the project , simply running ` gradlew tasks ` fails : `` ` FAILURE : Build failed with an exception . * What went wrong : A problem occurred configuring project ' : testProject ' . > Could not resolve all dependencies for configuration ' : testProject : classpath ' . > Could not find com.google.protobuf : protobuf-gradle-plugin:0.7.8-SNAPSHOT . Searched in the following locations : https : //jcenter.bintray.com/com/google/protobuf/protobuf-gradle-plugin/0.7.8-SNAPSHOT/maven-metadata.xml https : //jcenter.bintray.com/com/google/protobuf/protobuf-gradle-plugin/0.7.8-SNAPSHOT/protobuf-gradle-plugin-0.7.8-SNAPSHOT.pom https : //jcenter.bintray.com/com/google/protobuf/protobuf-gradle-plugin/0.7.8-SNAPSHOT/protobuf-gradle-plugin-0.7.8-SNAPSHOT.jar file : /Users/noname/.m2/repository/com/google/protobuf/protobuf-gradle-plugin/0.7.8-SNAPSHOT/maven-metadata.xml file : /Users/noname/.m2/repository/com/google/protobuf/protobuf-gradle-plugin/0.7.8-SNAPSHOT/protobuf-gradle-plugin-0.7.8-SNAPSHOT.pom file : /Users/noname/.m2/repository/com/google/protobuf/protobuf-gradle-plugin/0.7.8-SNAPSHOT/protobuf-gradle-plugin-0.7.8-SNAPSHOT.jar Required by : protobuf-gradle-plugin : testProject : unspecified * Try : Run with -- stacktrace option to get the stack trace . Run with -- info or -- debug option to get more log output . BUILD FAILED Total time : 9.575 secs `` ` Adding CI will fix these issues .
Gradle 5.0 deprecated api __EoT__ Gradle 4.7 complains about the protobuf plugin using a deprecated API . `` ` > Task : < myproject > : generateProto UP-TO-DATE Using TaskInputs.file ( ) with something that does n't resolve to a File object has been deprecated and is scheduled to be removed in Gradle 5.0 . Use TaskInputs.files ( ) instead . `` `
Version 0.8.4 : Can not compile on Windows when username has a space __EoT__ On my computer , my username is `` Tan Yee Fan '' and hence the path to the Protobuf compiler contains spaces : `` `` C : \Users\Tan Yee Fan\.gradle\caches\modules-2\files-2.1\com.google.protobuf\protoc\3.5.1-1\416df7b7f577fada867e9edd1864d45184e3ff41\protoc-3.5.1-1-windows-x86_64.exe `` `` When using version 0.8.4 of the plugin , the compilation failed with the following error message : `` `` * What went wrong : Execution failed for task ' : acv-service-common : generateProto ' . > java.io.IOException : Can not run program `` C : \Users\Tan '' : CreateProcess error=193 , % 1 is not a valid Win32 application `` `` Downgrading to version 0.8.3 of the plugin resolved the problem . System information : * Operating system : Windows 10 * Java : 1.8.0_161 * Gradle : 4.5.1 * Protobuf Plugin for Gradle : 0.8.4 * Protobuf compiler : 3.5.1-1
Version 0.8.4 : Can not compile on Windows when username has a space __EoT__ On my computer , my username is `` Tan Yee Fan '' and hence the path to the Protobuf compiler contains spaces : `` `` C : \Users\Tan Yee Fan\.gradle\caches\modules-2\files-2.1\com.google.protobuf\protoc\3.5.1-1\416df7b7f577fada867e9edd1864d45184e3ff41\protoc-3.5.1-1-windows-x86_64.exe `` `` When using version 0.8.4 of the plugin , the compilation failed with the following error message : `` `` * What went wrong : Execution failed for task ' : acv-service-common : generateProto ' . > java.io.IOException : Can not run program `` C : \Users\Tan '' : CreateProcess error=193 , % 1 is not a valid Win32 application `` `` Downgrading to version 0.8.3 of the plugin resolved the problem . System information : * Operating system : Windows 10 * Java : 1.8.0_161 * Gradle : 4.5.1 * Protobuf Plugin for Gradle : 0.8.4 * Protobuf compiler : 3.5.1-1
Version 0.8.4 : Can not compile on Windows when username has a space __EoT__ On my computer , my username is `` Tan Yee Fan '' and hence the path to the Protobuf compiler contains spaces : `` `` C : \Users\Tan Yee Fan\.gradle\caches\modules-2\files-2.1\com.google.protobuf\protoc\3.5.1-1\416df7b7f577fada867e9edd1864d45184e3ff41\protoc-3.5.1-1-windows-x86_64.exe `` `` When using version 0.8.4 of the plugin , the compilation failed with the following error message : `` `` * What went wrong : Execution failed for task ' : acv-service-common : generateProto ' . > java.io.IOException : Can not run program `` C : \Users\Tan '' : CreateProcess error=193 , % 1 is not a valid Win32 application `` `` Downgrading to version 0.8.3 of the plugin resolved the problem . System information : * Operating system : Windows 10 * Java : 1.8.0_161 * Gradle : 4.5.1 * Protobuf Plugin for Gradle : 0.8.4 * Protobuf compiler : 3.5.1-1
Doesnt work with Gradle 4.3 RC1 __EoT__ `` ` org.gradle.api.tasks.TaskExecutionException : Execution failed for task ' : XXX : generateProto ' . at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute ( ValidatingTaskExecuter.java:50 ) at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute ( SkipEmptySourceFilesTaskExecuter.java:97 ) at org.gradle.api.internal.tasks.execution.CleanupStaleOutputsExecuter.execute ( CleanupStaleOutputsExecuter.java:87 ) at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute ( ResolveTaskArtifactStateTaskExecuter.java:52 ) at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute ( SkipTaskWithNoActionsExecuter.java:52 ) at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute ( SkipOnlyIfTaskExecuter.java:54 ) at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute ( ExecuteAtMostOnceTaskExecuter.java:43 ) at org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute ( CatchExceptionTaskExecuter.java:34 ) at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter $ EventFiringTaskWorker $ 1.run ( DefaultTaskGraphExecuter.java:248 ) at org.gradle.internal.progress.DefaultBuildOperationExecutor $ RunnableBuildOperationWorker.execute ( DefaultBuildOperationExecutor.java:336 ) at org.gradle.internal.progress.DefaultBuildOperationExecutor $ RunnableBuildOperationWorker.execute ( DefaultBuildOperationExecutor.java:328 ) at org.gradle.internal.progress.DefaultBuildOperationExecutor.execute ( DefaultBuildOperationExecutor.java:199 ) at org.gradle.internal.progress.DefaultBuildOperationExecutor.run ( DefaultBuildOperationExecutor.java:110 ) at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter $ EventFiringTaskWorker.execute ( DefaultTaskGraphExecuter.java:241 ) at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter $ EventFiringTaskWorker.execute ( DefaultTaskGraphExecuter.java:230 ) at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor $ TaskExecutorWorker.processTask ( DefaultTaskPlanExecutor.java:123 ) at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor $ TaskExecutorWorker.access $ 200 ( DefaultTaskPlanExecutor.java:79 ) at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor $ TaskExecutorWorker $ 1.execute ( DefaultTaskPlanExecutor.java:104 ) at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor $ TaskExecutorWorker $ 1.execute ( DefaultTaskPlanExecutor.java:98 ) at org.gradle.execution.taskgraph.DefaultTaskExecutionPlan.execute ( DefaultTaskExecutionPlan.java:626 ) at org.gradle.execution.taskgraph.DefaultTaskExecutionPlan.executeWithTask ( DefaultTaskExecutionPlan.java:581 ) at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor $ TaskExecutorWorker.run ( DefaultTaskPlanExecutor.java:98 ) at org.gradle.internal.concurrent.ExecutorPolicy $ CatchAndRecordFailures.onExecute ( ExecutorPolicy.java:63 ) at org.gradle.internal.concurrent.ManagedExecutorImpl $ 1.run ( ManagedExecutorImpl.java:46 ) at org.gradle.internal.concurrent.ThreadFactoryImpl $ ManagedThreadRunnable.run ( ThreadFactoryImpl.java:55 ) Caused by : org.gradle.internal.typeconversion.UnsupportedNotationException : Can not convert the provided notation to a File or URI : main Proto
Doesnt work with Gradle 4.3 RC1 __EoT__ `` ` org.gradle.api.tasks.TaskExecutionException : Execution failed for task ' : XXX : generateProto ' . at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute ( ValidatingTaskExecuter.java:50 ) at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute ( SkipEmptySourceFilesTaskExecuter.java:97 ) at org.gradle.api.internal.tasks.execution.CleanupStaleOutputsExecuter.execute ( CleanupStaleOutputsExecuter.java:87 ) at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute ( ResolveTaskArtifactStateTaskExecuter.java:52 ) at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute ( SkipTaskWithNoActionsExecuter.java:52 ) at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute ( SkipOnlyIfTaskExecuter.java:54 ) at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute ( ExecuteAtMostOnceTaskExecuter.java:43 ) at org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute ( CatchExceptionTaskExecuter.java:34 ) at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter $ EventFiringTaskWorker $ 1.run ( DefaultTaskGraphExecuter.java:248 ) at org.gradle.internal.progress.DefaultBuildOperationExecutor $ RunnableBuildOperationWorker.execute ( DefaultBuildOperationExecutor.java:336 ) at org.gradle.internal.progress.DefaultBuildOperationExecutor $ RunnableBuildOperationWorker.execute ( DefaultBuildOperationExecutor.java:328 ) at org.gradle.internal.progress.DefaultBuildOperationExecutor.execute ( DefaultBuildOperationExecutor.java:199 ) at org.gradle.internal.progress.DefaultBuildOperationExecutor.run ( DefaultBuildOperationExecutor.java:110 ) at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter $ EventFiringTaskWorker.execute ( DefaultTaskGraphExecuter.java:241 ) at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter $ EventFiringTaskWorker.execute ( DefaultTaskGraphExecuter.java:230 ) at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor $ TaskExecutorWorker.processTask ( DefaultTaskPlanExecutor.java:123 ) at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor $ TaskExecutorWorker.access $ 200 ( DefaultTaskPlanExecutor.java:79 ) at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor $ TaskExecutorWorker $ 1.execute ( DefaultTaskPlanExecutor.java:104 ) at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor $ TaskExecutorWorker $ 1.execute ( DefaultTaskPlanExecutor.java:98 ) at org.gradle.execution.taskgraph.DefaultTaskExecutionPlan.execute ( DefaultTaskExecutionPlan.java:626 ) at org.gradle.execution.taskgraph.DefaultTaskExecutionPlan.executeWithTask ( DefaultTaskExecutionPlan.java:581 ) at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor $ TaskExecutorWorker.run ( DefaultTaskPlanExecutor.java:98 ) at org.gradle.internal.concurrent.ExecutorPolicy $ CatchAndRecordFailures.onExecute ( ExecutorPolicy.java:63 ) at org.gradle.internal.concurrent.ManagedExecutorImpl $ 1.run ( ManagedExecutorImpl.java:46 ) at org.gradle.internal.concurrent.ThreadFactoryImpl $ ManagedThreadRunnable.run ( ThreadFactoryImpl.java:55 ) Caused by : org.gradle.internal.typeconversion.UnsupportedNotationException : Can not convert the provided notation to a File or URI : main Proto
Doesnt work with Gradle 4.3 RC1 __EoT__ `` ` org.gradle.api.tasks.TaskExecutionException : Execution failed for task ' : XXX : generateProto ' . at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute ( ValidatingTaskExecuter.java:50 ) at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute ( SkipEmptySourceFilesTaskExecuter.java:97 ) at org.gradle.api.internal.tasks.execution.CleanupStaleOutputsExecuter.execute ( CleanupStaleOutputsExecuter.java:87 ) at org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute ( ResolveTaskArtifactStateTaskExecuter.java:52 ) at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute ( SkipTaskWithNoActionsExecuter.java:52 ) at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute ( SkipOnlyIfTaskExecuter.java:54 ) at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute ( ExecuteAtMostOnceTaskExecuter.java:43 ) at org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute ( CatchExceptionTaskExecuter.java:34 ) at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter $ EventFiringTaskWorker $ 1.run ( DefaultTaskGraphExecuter.java:248 ) at org.gradle.internal.progress.DefaultBuildOperationExecutor $ RunnableBuildOperationWorker.execute ( DefaultBuildOperationExecutor.java:336 ) at org.gradle.internal.progress.DefaultBuildOperationExecutor $ RunnableBuildOperationWorker.execute ( DefaultBuildOperationExecutor.java:328 ) at org.gradle.internal.progress.DefaultBuildOperationExecutor.execute ( DefaultBuildOperationExecutor.java:199 ) at org.gradle.internal.progress.DefaultBuildOperationExecutor.run ( DefaultBuildOperationExecutor.java:110 ) at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter $ EventFiringTaskWorker.execute ( DefaultTaskGraphExecuter.java:241 ) at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter $ EventFiringTaskWorker.execute ( DefaultTaskGraphExecuter.java:230 ) at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor $ TaskExecutorWorker.processTask ( DefaultTaskPlanExecutor.java:123 ) at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor $ TaskExecutorWorker.access $ 200 ( DefaultTaskPlanExecutor.java:79 ) at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor $ TaskExecutorWorker $ 1.execute ( DefaultTaskPlanExecutor.java:104 ) at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor $ TaskExecutorWorker $ 1.execute ( DefaultTaskPlanExecutor.java:98 ) at org.gradle.execution.taskgraph.DefaultTaskExecutionPlan.execute ( DefaultTaskExecutionPlan.java:626 ) at org.gradle.execution.taskgraph.DefaultTaskExecutionPlan.executeWithTask ( DefaultTaskExecutionPlan.java:581 ) at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor $ TaskExecutorWorker.run ( DefaultTaskPlanExecutor.java:98 ) at org.gradle.internal.concurrent.ExecutorPolicy $ CatchAndRecordFailures.onExecute ( ExecutorPolicy.java:63 ) at org.gradle.internal.concurrent.ManagedExecutorImpl $ 1.run ( ManagedExecutorImpl.java:46 ) at org.gradle.internal.concurrent.ThreadFactoryImpl $ ManagedThreadRunnable.run ( ThreadFactoryImpl.java:55 ) Caused by : org.gradle.internal.typeconversion.UnsupportedNotationException : Can not convert the provided notation to a File or URI : main Proto
Feature Request : support Java protoc plugins and allow specifying main class __EoT__ We have an existing codegen plugin for protobuf that is invoked in a maven build thusly : `` ` < protocPlugins > < protocPlugin > < id > foo-protobuf < /id > < groupId > com.foo.protobuf < /groupId > < artifactId > foo-protobuf-plugin < /artifactId > < version > HEAD-SNAPSHOT < /version > < mainClass > com.foo.protobuf.compiler.plugin.PluginMain < /mainClass > < /protocPlugin > < /protocPlugins > `` ` If I try to add this same plugin the way this project specifies that plugins should be added : `` ` plugins { foo { artifact = 'com.foo.protobuf : foo-protobuf-plugin : HEAD-SNAPSHOT' } `` ` The build fails ( target ` protobufToolsLocator_foo ` ) because it 's trying to download an executable artifact ` foo-protobuf-plugin-osx-x86_64.exe ` which does n't exist ( the artifact is actually ` foo-protobuf-plugin-HEAD-SNAPSHOT.jar ` ) . I think I can work around this by creating a local executable that downloads the jar and runs it with the classname specified , but it would be rad if this library included this functionality .
Handle for statement __EoT__ **Goal** * Support basic ` for ` statements * Support nested ` for ` statements * No need to support early returns and loop breaks for now
Snippets are not compatible with UIAutomator __EoT__ 'am instrument ' only sets up a UiAutomationConnection in -w mode ( I do n't know why ) . Snippets are not run in wait mode , so UiAutomationConnection is null . This crashes uiautomator , which needs that proxy object for privileged operations back to the shell . We need to start the snippets in -w mode , using something like ` start_standing_subprocess ` .
Need to resolve versioning of mobly api documentation generation through readthedocs.org __EoT__ Currently we have a pip requirements file ( docs/rtd_requirements.txt ) that is used by the readthedocs.org to generate mobly api documentation . This causes the code thats used to generate the documentation to come from the latest version of mobly thats available through pip , and not from the latest code in master branch . We need to fix the documentation setup so that readthedocs build grabs the right version of code from the branch .
snippet launching is broken in py3 __EoT__ When trying to launch snippet , it fails with : `` ` File `` /Users/angli/Developer/mobly/tools/snippet_shell.py '' , line 43 , in _start_services self._ad.load_snippet ( name='snippet ' , package=self._package ) File `` /Users/angli/Developer/mobly/mobly/controllers/android_device.py '' , line 727 , in load_snippet client.start_app_and_connect ( ) File `` /Users/angli/Developer/mobly/mobly/controllers/android_device_lib/snippet_client.py '' , line 92 , in start_app_and_connect persists_shell_cmd = self._get_persist_command ( ) File `` /Users/angli/Developer/mobly/mobly/controllers/android_device_lib/snippet_client.py '' , line 312 , in _get_persist_command if command in self._adb.shell ( 'which % s ' % command ) : TypeError : a bytes-like object is required , not 'str' `` ` Looks like this is related to recent change # 251
snippet launching is broken in py3 __EoT__ When trying to launch snippet , it fails with : `` ` File `` /Users/angli/Developer/mobly/tools/snippet_shell.py '' , line 43 , in _start_services self._ad.load_snippet ( name='snippet ' , package=self._package ) File `` /Users/angli/Developer/mobly/mobly/controllers/android_device.py '' , line 727 , in load_snippet client.start_app_and_connect ( ) File `` /Users/angli/Developer/mobly/mobly/controllers/android_device_lib/snippet_client.py '' , line 92 , in start_app_and_connect persists_shell_cmd = self._get_persist_command ( ) File `` /Users/angli/Developer/mobly/mobly/controllers/android_device_lib/snippet_client.py '' , line 312 , in _get_persist_command if command in self._adb.shell ( 'which % s ' % command ) : TypeError : a bytes-like object is required , not 'str' `` ` Looks like this is related to recent change # 251
Deprecate old output file format __EoT__ Once new format is rolled out . Targeting release 1.7
Docstring errors found by sphinx api doc generation tool __EoT__ /mobly/asserts.py : docstring of mobly.asserts.abort_all:7 : ERROR : Unexpected indentation . /mobly/asserts.py : docstring of mobly.asserts.abort_all_if:8 : ERROR : Unexpected indentation . /mobly/asserts.py : docstring of mobly.asserts.abort_class:10 : ERROR : Unexpected indentation . /mobly/asserts.py : docstring of mobly.asserts.abort_class_if:11 : ERROR : Unexpected indentation . /mobly/asserts.py : docstring of mobly.asserts.assert_equal:10 : ERROR : Unexpected indentation . /mobly/asserts.py : docstring of mobly.asserts.assert_false:7 : ERROR : Unexpected indentation . /mobly/asserts.py : docstring of mobly.asserts.assert_true:7 : ERROR : Unexpected indentation . /mobly/asserts.py : docstring of mobly.asserts.explicit_pass:10 : ERROR : Unexpected indentation . /mobly/asserts.py : docstring of mobly.asserts.fail:6 : ERROR : Unexpected indentation . /mobly/asserts.py : docstring of mobly.asserts.skip:6 : ERROR : Unexpected indentation . /mobly/asserts.py : docstring of mobly.asserts.skip_if:7 : ERROR : Unexpected indentation . /mobly/base_test.py : docstring of mobly.base_test.BaseTestClass:12 : ERROR : Unexpected indentation . /mobly/base_test.py : docstring of mobly.base_test.BaseTestClass:13 : WARNING : Block quote ends without a blank line ; unexpected unindent . /mobly/base_test.py : docstring of mobly.base_test.BaseTestClass:15 : ERROR : Unexpected indentation . /mobly/base_test.py : docstring of mobly.base_test.BaseTestClass:16 : WARNING : Block quote ends without a blank line ; unexpected unindent . /mobly/base_test.py : docstring of mobly.base_test.BaseTestClass.run:7 : ERROR : Unexpected
Make Mobly run on Windows __EoT__ Mobly is currently unix-only . It would be nice to have Windows support .
The timeout value for ` AndroidDevice._wait_for_device ` is incorrectly implemented __EoT__ ` AndroidDevice._wait_for_device ` calls ` self.adb.wait_for_device ` , which is a blocking call to adb binary . This call might block for longer than ` timeout ` specified ( or forever ) , in which case the timeout value would not be accurate . E.g . if ` AndroidDevice._wait_for_device ` is called with 10s , the ` adb wait_for_device ` can still block for 15s before returning .
Procedure functions triggered incorrectly __EoT__ If a test throws an exception , and the teardown also throws an exception , ` on_fail ` is executed twice .
Correct handling of unknown proto3 enum values __EoT__ Currently proto-lens returns an error when decoding unknown enum values . It should instead accept and preserve such values . Quoting the [ proto3 docs ] ( https : //developers.google.com/protocol-buffers/docs/proto3 ) : > During deserialization , unrecognized enum values will be preserved in the message , though how this is represented when the message is deserialized is language-dependent . In languages that support open enum types with values outside the range of specified symbols , such as C++ and Go , the unknown enum value is simply stored as its underlying integer representation . In languages with closed enum types such as Java , a case in the enum is used to represent an unrecognized value , and the underlying integer can be accessed with special accessors . In either case , if the message is serialized the unrecognized value will still be serialized with the message .
showMessage and related functions use Haskell not C string escaping conventions __EoT__ showMessage and the related pprintMessage and showMessageShort functions use the Haskell string escaping conventions instead of the C ones . This means that non-printing characters get written as , e.g . `` \SOH '' , which https : //github.com/google/protobuf/blob/master/src/google/protobuf/io/tokenizer.cc # L1039 wo n't parse . Worse , in Haskell the escape `` \101 '' means decimal 101 whereas the tokenizer.cc code ( following C convention ) interprets that as octal , i.e . decimal 97 .
Generate ` Ord ` instances for messages . __EoT__ It would be nice if protolens automatically generated ` Ord ` instances of messages . This could be hidden behind a cabal flag if it turned out to be egregiously slow .
In proto3 , repeated fields of scalar numeric types use packed encoding by default . __EoT__ See https : //developers.google.com/protocol-buffers/docs/proto3 # specifying-field-rules . proto-lens generated code does not implement this correctly . Trying to decode repeated fields of scalar numeric types ( like int32 and float ) results in a message like `` Field 1 expects wire type 0 but found 2 '' , because packed repeated fields are wire type 2 ( see https : //developers.google.com/protocol-buffers/docs/encoding # structure ) .
Support enum aliases __EoT__ Enums can have `` aliases '' where two different constructors may map to the same int value . ( In both proto2 and proto3 ) . This breaks our codegen , in particular the ` fromEnum ` instances . Documentation : https : //developers.google.com/protocol-buffers/docs/proto3 # enum The user enables this feature by adding ` option allow_alias = true ` to the enum declaration . I do n't know whether the protobuf compiler is the one doing the checking , or if our proto-lens-protoc plugin needs check it manually .
Define our own `` def '' rather than the one from data-default-class __EoT__ The ` data-default-class ` package is ... somewhat controversial . The main criticism is that ` def ` does n't satisfy any laws , so tends to be abused . See for example the following haskell-cafe thread : https : //mail.haskell.org/pipermail/haskell-cafe/2018-May/129053.html Indeed , from our experience using ` proto-lens ` on a larger team , we 've found that ` data-default ` creeps into places it 's not really intended for ; e.g. , using it for an integer value instead of an explicit ` 0 ` . However , for protobufs we *do* have a useful law , namely , ` encodeMessage def == `` '' ` . Note that the ` proto-lens ` library already reexports ` def ` directly : ( ` Data.ProtoLens.Message ` and ` Data.ProtoLens ` ) I propose the following changes : - Remove all our dependencies on ` data-default-class ` ( and ` data-default ` ) - Change the ` Message ` class to define its own method ` def ` : class Message m where ... def : : m Note that this is a breaking change , so
Inconsistent naming of coproduct oneof fields __EoT__ `` ` message AcmeObservation { oneof status { ActionWin win = 2 ; CompletedHurdleStatus completed_hurdle = 3 ; QualifyTransaction qualify_transaction = 4 ; } } `` ` results in the generated haskell : `` ` haskell data AcmeObservation'Status = AcmeObservation'Win ! ActionWin | AcmeObservation'Completed_hurdle ! CompletedHurdleStatus | AcmeObservation'Qualify_transaction ! QualifyTransaction deriving ( Prelude.Show , Prelude.Eq ) `` ` Notice ` AcmeObservation'Completed_hurdle ` , which should become ` AcmeObservation'CompletedHurdle ` according to the renaming of all other snake case identifiers .
Inconsistent naming of coproduct oneof fields __EoT__ `` ` message AcmeObservation { oneof status { ActionWin win = 2 ; CompletedHurdleStatus completed_hurdle = 3 ; QualifyTransaction qualify_transaction = 4 ; } } `` ` results in the generated haskell : `` ` haskell data AcmeObservation'Status = AcmeObservation'Win ! ActionWin | AcmeObservation'Completed_hurdle ! CompletedHurdleStatus | AcmeObservation'Qualify_transaction ! QualifyTransaction deriving ( Prelude.Show , Prelude.Eq ) `` ` Notice ` AcmeObservation'Completed_hurdle ` , which should become ` AcmeObservation'CompletedHurdle ` according to the renaming of all other snake case identifiers .
Apostrophe in filenames is not agreeable with Bazel builds __EoT__ When trying to build proto-lens package with house-made bazel skylark extension we run into a limitation of valid filenames . In particular the error is reported as : `` ` ERROR : v_0_3_0_0/BUILD:25:1 : v_0_3_0_0 : proto-lens-lib : invalid label 'src/Proto/Google/Protobuf/Compiler/Plugin'Fields.hs ' in element 8 of attribute 'srcs ' in 'haskell_library ' rule : invalid target name 'src/Proto/Google/Protobuf/Compiler/Plugin'Fields.hs ' : target names may not contain `` ' . `` ` Maybe before 0.3.0.0 of proto-lens is released there 's still time to change the separators to be something else ? E.g . put the files into a sub-directory and the module name would just become : ` Proto.Google.Protobuf.Compiler.Plugin.Fields ` I suspect _ separator was excluded because it can clash with package a valid name inside a proto file ?
TypeError : console.err is not a function __EoT__ https : //github.com/google/clasp/blob/4cf4cc95a83acdb9faf78d1b46a34f96ff309658/index.js # L417 This has logged an error in terminal ( see below ) . Should be **console.error ( ) ** instead ? TypeError : console.err is not a function at fs.writeFile ( /usr/local/lib/node_modules/ @ google/clasp/index.js:417:23 ) at /usr/local/lib/node_modules/ @ google/clasp/node_modules/graceful-fs/graceful-fs.js:43:10 at FSReqWrap.oncomplete ( fs.js:135:15 )
grey_replaceText ( ) does n't send UITextFieldTextDidChangeNotification nor UIControlEventEditingChanged __EoT__ grey_replaceText ( ) does n't send UITextFieldTextDidChangeNotification When using grey_replaceText ( ) on a UITextField , it should send the UITextFieldTextDidChangeNotification notification . As of today : https : //github.com/google/EarlGrey/blob/38b490e5a156f16a00dd7777b4c4cdcffe6a4e24/EarlGrey/Action/GREYActions.m # L332 But setText : does n't send the notification . You might send the notification yourself : `` ` [ element setText : text ] ; NSNotification *notification = [ NSNotification notificationWithName : UITextFieldTextDidChangeNotification object : element ] ; [ NSNotificationCenter.defaultCenter postNotification : notification ] ; `` `
Carthage docs & example __EoT__ Now that Carthage is able to fetch and build EarlGrey , it 'd be nice to have an example similar to the existing cocoapods demo . Also the ` install-and-run ` doc should be updated to include the steps necessary to get started with carthage .
Add API for shaking device __EoT__ Hello , I 've noticed that Earl Grey does not have `` shake device '' API . I plan to add this as a PR soon . The proposed API will be supported on both device and simulator .
Improve earlgrey gem to set up the project easily __EoT__ Proposing the following improvements in the earlgrey gem : 1. remove post_install , earlgrey.pod would automatically set up the target : `` ` ruby target `` ExampleTests '' do pod 'EarlGrey' end # remove post_install `` ` This will add the copy script and modify the schemes for the target `` ExampleTests '' only when it is a unit test bundle . 2. config xcscheme files per target It would apply required changes to all the schemes that include the given target ; currently , it targets only the given test target ; sometimes , the app target can contain the test target as well . 3. copy EarlGrey.swift when the test target has a swift source code or ` swift_version ` is defined in podfile : `` ` ruby target `` ExampleTests '' do pod 'EarlGrey' swift_version = '3.0' end `` ` Note if the project has ` SWIFT_VERSION ` or ` Use Legacy Swift Language Version ` defined , it will also include the swift file
Custom action Swift example __EoT__ It 'd be nice to have a simple example of how to do a custom action using Swift .
SSL/TLS Layer __EoT__ Is there any plan to provide a SSL/TLS layer that provides version information + interprets Client/Server HELLOs and Certificates ? Is this already available elsewhere ?
dhcpv6 __EoT__ Add support for handling dhcpv6 packets https : //en.wikipedia.org/wiki/DHCPv6
dhcpv6 __EoT__ Add support for handling dhcpv6 packets https : //en.wikipedia.org/wiki/DHCPv6
dhcpv6 __EoT__ Add support for handling dhcpv6 packets https : //en.wikipedia.org/wiki/DHCPv6
Error in serialization of the IPv6HopByHop layer __EoT__ I 'm experiencing an error when serializing IPv6HopByHop packets . The problem , to be exact , is that the HopByHop header is written twice to the buffer . It 's very easy to replicate this problem : `` ` go // in icmp6hopbyhop_test.go func TestPacketICMPv6WithHopByHop ( t *testing.T ) { // ... // add this after the original code p : = gopacket.NewPacket ( icmp6HopByHopData , LinkTypeEthernet , gopacket.Default ) if p.ErrorLayer ( ) ! = nil { t.Error ( `` Failed to decode packet : '' , p.ErrorLayer ( ) .Error ( ) ) } checkLayers ( p , [ ] gopacket.LayerType { LayerTypeEthernet , LayerTypeIPv6 , LayerTypeIPv6HopByHop , LayerTypeICMPv6 } , t ) checkSerialization ( p , t ) } // I 've added the checkSerialization ( ) function to ` base_test.go ` in my code // Checks that when a serialized version of p is decoded , p and the serialized version of p are the same . // Does not work for packets where the order of options can change , like icmpv6 router advertisements , dhcpv6 , etc . func checkSerialization ( p gopacket.Packet , t
bidirectional TCP monitoring /examples/bidirectional/main.go - hogging memory __EoT__ The example file here : https : //github.com/google/gopacket/blob/master/examples/bidirectional/main.go is hogging memory like crazy and crashes in the end , when theres a lot of traffic on the network . Any suggestions on what might be causing this ?
Turbiniactl set task status closed __EoT__ If tasks fail in a way that we do n't close them ( hopefully this happens only during development , because we should make sure we 're getting error data back about failures during other runs ) , it can leave a lot of open tasks in datastore that show up in 'turbiniactl status ' commands . We should have a turbiniactl command that can force close those tasks ( probably by setting successful=False , and status='Task force closed by $ foo ' ) . This will require a new GCF function .
Selective imports of Celery/Redis __EoT__ In # 182 we added support for alternate local backends ( yay ! ) , and in setup.py we have the dependencies properly segregated , but some of the modules common to both installation types still load the celery/redis modules . We should find a way to guard these against import in the non-local case . @ ericzinnikas Any chance you want to take a look at this ? : ) . I have n't looked to closely , so I 'm not sure what the best solution is , but we could try to check the config and selectively load them . Or we could load them further down in the file closer to the code that uses it . At worst we could add a try/except around them . LMK if you might have time for this , otherwise I 'll try to take a look at it soon . Thanks !
Tests do n't run without GCP credentials __EoT__ We need to mock the calls to create_state_manager .
Add cloud storage output option __EoT__ Right now the output for jobs is put into a locally mounted filesystem , and we should allow for the option to put the output directly into cloud storage .
Print version on start __EoT__ We should output the version when Turbinia starts to make it easier to determine what is running ( especially if it 's been running for a while ) .
Schedule : Filter - remove breadcrumbs when open __EoT__ Breadcrumbs shouldn â€™ t be visible w/ filter open ( only when filter is closed ) since you can see your selections when open . @ jeffposnick
Firebase database ( s ) wipeout __EoT__ Per the privacy policy we agreed on , we need to set up a periodic ( daily ? ) cron that deletes the user data iff the user 's ` last_activity_timestamp ` > 30 days from the time we check . We need something asap IOSched 's launch might be blocked on this being in place . It would be smart to reuse our existing GAE backend for this since 1 . ) we 're already using cron and 2 . ) the backend will already be talking to FB . @ crhym3 is this something you can whip up ? If not , we 'll need a stop-gap until you can or implement a parallel server . Not ideal though . cc @ shailen @ nicolasgarnier ( FB TL )
QA PH3_Accessibility : Sign In/Sign Out is inaccessible with keyboard only navigation __EoT__ The Sign In and Sign out links are inaccessible when using the keyboard only to navigate . Steps : 1 . Go to https : //io-webapp-staging.appspot.com/io2016/ 2 . Use the keyboard only to navigate 3 . Tap tab until landed on Sign In 4 . Tap enter 5 . Note issue - no response 6 . Use the mouse to sign in 7 . Once signed in tab to account settings > tap enter > settings expand 8 . Continue to tap tab 9 . Notice Sign Out is not landed on Chrome 49 Mac OS X 10.11 ! [ gio_632 ] ( https : //cloud.githubusercontent.com/assets/15128124/14443563/d452bd62-fff4-11e5-8bf0-d83daa2424dd.png ) ! [ gio_632b ] ( https : //cloud.githubusercontent.com/assets/15128124/14443655/6d82f466-fff5-11e5-815b-41851bea69dd.png )
Wire up AuthServiceProviderRegistry __EoT__ Wire up AuthServiceProviderRegistry , perhaps similarly to the TransferServiceProviderRegistry
Simplify dependency injection with Guice ( and remove static variables ) __EoT__ Remove statics and add DI via Guice .
Simplify dependency injection with Guice ( and remove static variables ) __EoT__ Remove statics and add DI via Guice .
Travis fails on master when we merge __EoT__ Our builds on master have been failing https : //travis-ci.org/google/data-transfer-project/builds The builds on the PRs have succeeded , before merging to master We seem to have lost a bunch of build history on Travis ( or do n't retain it ) past 7 days ago , but it does show up for 6+ months ago . So hard to say when this started happening but it seems like it could be related to the new dockerhub push in https : //github.com/google/data-transfer-project/pull/474
Handle albums better in Photos implementations __EoT__ - FlickrImporter assumes that all photos are associated with an album and that we 've imported all albums needed and have data for them . We do n't handle if a photo is n't in an album or if the album doesnt exist yet . - Google photos importer puts everything into a default album because album support is buggy . needs debugging and implementation ( and handling of the case above with flickr ) .
Add support for InputStreams to Google Cloud Storage __EoT__
Update Website and documentation __EoT__ Update developer and integration guide .
Next step of UI/UX improvements 4 __EoT__ # 388
Next step of UI/UX improvements 4 __EoT__ # 388
Fix reviewer webapp README __EoT__ - new line bug - use ` npm ` instead of ` ng ` - more steps # 296
Display diff threads __EoT__ # 296
Move completion.sh to tools __EoT__ This one is really trivial - Move https : //github.com/google/startup-os/blob/master/completion.sh To ` tools `
Move completion.sh to tools __EoT__ This one is really trivial - Move https : //github.com/google/startup-os/blob/master/completion.sh To ` tools `
Add dependencies.yaml formatting to multi-language formatter __EoT__ Essentially this command : ` bazel run @ bazel_deps// : parse -- format-deps -- overwrite -- deps ` pwd ` /dependencies.yaml ` Should be run as part of ` SimpleFormatter ` , possibly using a flag ( unless @ bazel_deps// : parse uses a standard formatter for yaml files ) .
Ca n't change label colors __EoT__ Hi everyone . I 'm trying to change the default text color from black to white , but everytime i use the ArcLabelDecorator on my pie chart , it says this : > |flutter : â•â•â•¡ EXCEPTION CAUGHT BY RENDERING LIBRARY â•žâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• flutter : The following NoSuchMethodError was thrown during paint ( ) : flutter : The method '/ ' was called on null . flutter : Receiver : null flutter : Tried calling : / ( 2 ) flutter : flutter : When the exception was thrown , this was the stack : flutter : # 0 Object.noSuchMethod ( dart : core/runtime/libobject_patch.dart:48:5 ) flutter : # 1 ArcLabelDecorator._drawOutsideLabel ( package : charts_common/src/chart/pie/arc_label_decorator.dart:282:67 ) flutter : # 2 ArcLabelDecorator.decorate ( package : charts_common/src/chart/pie/arc_label_decorator.dart:197:21 ) flutter : # 3 ArcRenderer.paint. < anonymous closure > . < anonymous closure > ( package : charts_common/src/chart/pie/arc_renderer.dart:374:19 ) flutter : # 4 Iterable.forEach ( dart : core/iterable.dart:277:30 ) flutter : # 5 ArcRenderer.paint. < anonymous closure > ( package : charts_common/src/chart/pie/arc_renderer.dart:373:12 ) flutter : # 6 __InternalLinkedHashMap & _HashVMBase & MapMixin & _LinkedHashMapMixin.forEach ( dart : collection/runtime/libcompact_hash.dart:365:8 ) flutter : # 7 ArcRenderer.paint ( package : charts_common/src/chart/pie/arc_renderer.dart:332:19 ) flutter
Ca n't change label colors __EoT__ Hi everyone . I 'm trying to change the default text color from black to white , but everytime i use the ArcLabelDecorator on my pie chart , it says this : > |flutter : â•â•â•¡ EXCEPTION CAUGHT BY RENDERING LIBRARY â•žâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• flutter : The following NoSuchMethodError was thrown during paint ( ) : flutter : The method '/ ' was called on null . flutter : Receiver : null flutter : Tried calling : / ( 2 ) flutter : flutter : When the exception was thrown , this was the stack : flutter : # 0 Object.noSuchMethod ( dart : core/runtime/libobject_patch.dart:48:5 ) flutter : # 1 ArcLabelDecorator._drawOutsideLabel ( package : charts_common/src/chart/pie/arc_label_decorator.dart:282:67 ) flutter : # 2 ArcLabelDecorator.decorate ( package : charts_common/src/chart/pie/arc_label_decorator.dart:197:21 ) flutter : # 3 ArcRenderer.paint. < anonymous closure > . < anonymous closure > ( package : charts_common/src/chart/pie/arc_renderer.dart:374:19 ) flutter : # 4 Iterable.forEach ( dart : core/iterable.dart:277:30 ) flutter : # 5 ArcRenderer.paint. < anonymous closure > ( package : charts_common/src/chart/pie/arc_renderer.dart:373:12 ) flutter : # 6 __InternalLinkedHashMap & _HashVMBase & MapMixin & _LinkedHashMapMixin.forEach ( dart : collection/runtime/libcompact_hash.dart:365:8 ) flutter : # 7 ArcRenderer.paint ( package : charts_common/src/chart/pie/arc_renderer.dart:332:19 ) flutter
Ca n't change label colors __EoT__ Hi everyone . I 'm trying to change the default text color from black to white , but everytime i use the ArcLabelDecorator on my pie chart , it says this : > |flutter : â•â•â•¡ EXCEPTION CAUGHT BY RENDERING LIBRARY â•žâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• flutter : The following NoSuchMethodError was thrown during paint ( ) : flutter : The method '/ ' was called on null . flutter : Receiver : null flutter : Tried calling : / ( 2 ) flutter : flutter : When the exception was thrown , this was the stack : flutter : # 0 Object.noSuchMethod ( dart : core/runtime/libobject_patch.dart:48:5 ) flutter : # 1 ArcLabelDecorator._drawOutsideLabel ( package : charts_common/src/chart/pie/arc_label_decorator.dart:282:67 ) flutter : # 2 ArcLabelDecorator.decorate ( package : charts_common/src/chart/pie/arc_label_decorator.dart:197:21 ) flutter : # 3 ArcRenderer.paint. < anonymous closure > . < anonymous closure > ( package : charts_common/src/chart/pie/arc_renderer.dart:374:19 ) flutter : # 4 Iterable.forEach ( dart : core/iterable.dart:277:30 ) flutter : # 5 ArcRenderer.paint. < anonymous closure > ( package : charts_common/src/chart/pie/arc_renderer.dart:373:12 ) flutter : # 6 __InternalLinkedHashMap & _HashVMBase & MapMixin & _LinkedHashMapMixin.forEach ( dart : collection/runtime/libcompact_hash.dart:365:8 ) flutter : # 7 ArcRenderer.paint ( package : charts_common/src/chart/pie/arc_renderer.dart:332:19 ) flutter
Area And Line Line Chart Example __EoT__ Area And Line Line Chart Example is not working
Area And Line Line Chart Example __EoT__ Area And Line Line Chart Example is not working
Area And Line Line Chart Example __EoT__ Area And Line Line Chart Example is not working
Area And Line Line Chart Example __EoT__ Area And Line Line Chart Example is not working
Area And Line Line Chart Example __EoT__ Area And Line Line Chart Example is not working
Area And Line Line Chart Example __EoT__ Area And Line Line Chart Example is not working
Area And Line Line Chart Example __EoT__ Area And Line Line Chart Example is not working
Area And Line Line Chart Example __EoT__ Area And Line Line Chart Example is not working
Area And Line Line Chart Example __EoT__ Area And Line Line Chart Example is not working
Area And Line Line Chart Example __EoT__ Area And Line Line Chart Example is not working
Issue on flutter project __EoT__ Hello , something goes wrong with the passed type with the exemple ( from https : //gist.github.com/mit-mit/08168a773a56bb58c7abfd9b8e82f467 ) : `` ` â•â•â•¡ EXCEPTION CAUGHT BY WIDGETS LIBRARY â•žâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• The following assertion was thrown building RawGestureDetector ( state : RawGestureDetectorState # 957ba ( gestures : [ tap ] ) ) : type ' ( ClicksPerYear , int ) = > String ' is not a subtype of type ' ( dynamic , int ) = > String ' of 'value' Either the assertion indicates an error in the framework itself , or we should provide substantially more information in this error message to help you determine and fix the underlying cause . In either case , please report this assertion by filing a bug on GitHub : https : //github.com/flutter/flutter/issues/new When the exception was thrown , this was the stack : # 0 MutableSeries.domainFn= ( package : charts_common/src/chart/common/processed_series.dart ) # 1 new MutableSeries ( package : charts_common/src/chart/common/processed_series.dart:54:5 ) # 2 BaseChart.makeSeries ( package : charts_common/src/chart/common/base_chart.dart:337:19 ) # 3 CartesianChart.makeSeries ( package : charts_common/src/chart/cartesian/cartesian_chart.dart:125:35 ) # 4 BaseChart.draw. < anonymous closure > ( package : charts_common/src/chart/common/base_chart.dart:292:49 ) # 5 MappedListIterable.elementAt ( dart : _internal/iterable.dart:414:29 ) # 6
unwrap issue when destination buffer is full __EoT__ Hi , I am seeing the following issue in the conscrypt on maven ( on mac os X ) . If unwrap is called with no space left in destination buffer . Unwrap calls : ENGINE_SSL_read_direct with empty buffer After calls to SSL_read , SSL_get_error , it hits the following : `` ` case SSL_ERROR_SYSCALL : { // A problem occurred during a system call , but this is not // necessarily an error . if ( result == 0 ) { // TODO ( nmittler ) : Can this happen with memory BIOs ? // Connection closed without proper shutdown . Tell caller we // have reached end-of-stream . conscrypt : :jniutil : :throwException ( env , `` java/io/EOFException '' , `` Read error '' ) ; break ; } `` ` And throws EOFException . If I prevent the calling code from calling unwrap when destination buffer is full then the problem goes away . I wonder if this is due to : refactoring in : https : //github.com/google/conscrypt/commit/d153b6000977e5e36448e4faf1a5c602c461057 As that is where the throw EOFException on SSL_ERROR_SYSCALL was added . Previously it was : `` ` else { //
Need to support Java 6 __EoT__ Both gRPC and Netty target Java 1.6 . We should too .
Investigate handshake performance __EoT__ We seem to be trailing Netty tcnative in handshake performance . Profiling shows that the we 're spending a lot of time in ` sun.security.ssl.X509TrustManagerImpl.checkServerTrusted ( X509Certificate [ ] , String , SSLEngine ) ` . I suspect that Netty may be handling server verification differently . `` ` Benchmark ( a_cipher ) ( b_buffer ) ( c_engine ) Mode Cnt Score Error Units JmhEngineHandshakeBenchmark.hs TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 HEAP JDK thrpt 10 162.348 Â± 10.382 ops/s JmhEngineHandshakeBenchmark.hs TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 HEAP CONSCRYPT_UNPOOLED thrpt 10 716.748 Â± 102.178 ops/s JmhEngineHandshakeBenchmark.hs TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 HEAP CONSCRYPT_POOLED thrpt 10 742.854 Â± 150.313 ops/s JmhEngineHandshakeBenchmark.hs TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 HEAP NETTY thrpt 10 1236.226 Â± 9.631 ops/s JmhEngineHandshakeBenchmark.hs TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 HEAP NETTY_REF_CNT thrpt 10 1199.334 Â± 97.748 ops/s JmhEngineHandshakeBenchmark.hs TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 DIRECT JDK thrpt 10 163.654 Â± 9.940 ops/s JmhEngineHandshakeBenchmark.hs TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 DIRECT CONSCRYPT_UNPOOLED thrpt 10 755.786 Â± 142.212 ops/s JmhEngineHandshakeBenchmark.hs TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 DIRECT CONSCRYPT_POOLED thrpt 10 755.845 Â± 135.823 ops/s JmhEngineHandshakeBenchmark.hs TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 DIRECT NETTY thrpt 10 1247.186 Â± 9.735 ops/s JmhEngineHandshakeBenchmark.hs TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 DIRECT NETTY_REF_CNT thrpt 10 1217.178 Â± 79.153 ops/s `` `
Consider alternative default Provider name __EoT__ Right now Conscrypt is using ` AndroidOpenSSL ` as its provider name . This appears to cause a conflict on Android , because a different version of Conscrypt is already in the provider list . Should Conscrypt use ` Conscrypt ` or something else as its name ? P.S . I 'm dealing right now with how to detect and handle Conscrypt . I was heavily leaning toward detecting it based on its name , since its class name is supposed to be an implementation detail and I do n't see any method in Conscrypt.java to determine if a ` Provider ` instance is Conscrypt . ( The jury 's still out on this though ; I 'm still investigating . It also does n't solve the problem of determining whether the implementation supports ALPN and/or NPN , if I 'm trying to support older Android and Google Play Services versions )
Swap to using TLS_with_buffers_method __EoT__ http : //commondatastorage.googleapis.com/chromium-boringssl-docs/ssl.h.html # TLS_with_buffers_method This should improve performance .
Linux binary for 1.0.0.RC1 is bloated with debug information __EoT__ libconscrypt_openjdk_jni-linux-x86_64.so is 8 MB . The equivalent for OS X and Windows is 2 MB . Running ` strip ` on the .so produces a 2 MB result . This may be caused by ` -g ` being used in the build on Linux .
Update AppVeyor to MSVC 2017 and Platform SDK 10 __EoT__ BoringSSL ( and chromium ) are switching to MSVC 2017 . Probably a good idea to get ahead of this .
NPE on Cancel during Connect __EoT__ From an in development PR of okhttp https : //github.com/square/okhttp/pull/3262 CallTest.cancelDuringHttpsConnect:1849- > cancelDuringConnect:1868 Â» NullPointer `` ` java.lang.NullPointerException : fd == null at org.conscrypt.NativeCrypto.SSL_shutdown ( Native Method ) at org.conscrypt.OpenSSLSocketImpl.shutdownAndFreeSslNative ( OpenSSLSocketImpl.java:1178 ) at org.conscrypt.OpenSSLSocketImpl.startHandshake ( OpenSSLSocketImpl.java:439 ) at okhttp3.internal.connection.RealConnection.connectTls ( RealConnection.java:268 ) at okhttp3.internal.connection.RealConnection.establishProtocol ( RealConnection.java:238 ) at okhttp3.internal.connection.RealConnection.connect ( RealConnection.java:149 ) at okhttp3.internal.connection.StreamAllocation.findConnection ( StreamAllocation.java:192 ) at okhttp3.internal.connection.StreamAllocation.findHealthyConnection ( StreamAllocation.java:121 ) at okhttp3.internal.connection.StreamAllocation.newStream ( StreamAllocation.java:100 ) at okhttp3.internal.connection.ConnectInterceptor.intercept ( ConnectInterceptor.java:42 ) at okhttp3.internal.http.RealInterceptorChain.proceed ( RealInterceptorChain.java:92 ) at okhttp3.internal.http.RealInterceptorChain.proceed ( RealInterceptorChain.java:67 ) at okhttp3.internal.cache.CacheInterceptor.intercept ( CacheInterceptor.java:93 ) at okhttp3.internal.http.RealInterceptorChain.proceed ( RealInterceptorChain.java:92 ) at okhttp3.internal.http.RealInterceptorChain.proceed ( RealInterceptorChain.java:67 ) at okhttp3.internal.http.BridgeInterceptor.intercept ( BridgeInterceptor.java:93 ) at okhttp3.internal.http.RealInterceptorChain.proceed ( RealInterceptorChain.java:92 ) at okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept ( RetryAndFollowUpInterceptor.java:120 ) at okhttp3.internal.http.RealInterceptorChain.proceed ( RealInterceptorChain.java:92 ) at okhttp3.internal.http.RealInterceptorChain.proceed ( RealInterceptorChain.java:67 ) at okhttp3.RealCall.getResponseWithInterceptorChain ( RealCall.java:179 ) at okhttp3.RealCall.execute ( RealCall.java:63 ) at okhttp3.CallTest.cancelDuringConnect ( CallTest.java:1868 ) at okhttp3.CallTest.cancelDuringHttpsConnect ( CallTest.java:1849 ) at sun.reflect.NativeMethodAccessorImpl.invoke0 ( Native Method ) at sun.reflect.NativeMethodAccessorImpl.invoke ( NativeMethodAccessorImpl.java:62 ) at sun.reflect.DelegatingMethodAccessorImpl.invoke ( DelegatingMethodAccessorImpl.java:43 ) at java.lang.reflect.Method.invoke ( Method.java:498 ) at org.junit.runners.model.FrameworkMethod $ 1.runReflectiveCall ( FrameworkMethod.java:50 ) at org.junit.internal.runners.model.ReflectiveCallable.run ( ReflectiveCallable.java:12 ) at org.junit.runners.model.FrameworkMethod.invokeExplosively ( FrameworkMethod.java:47 ) at org.junit.internal.runners.statements.InvokeMethod.evaluate ( InvokeMethod.java:17 ) at org.junit.internal.runners.statements.RunBefores.evaluate ( RunBefores.java:26 ) at org.junit.internal.runners.statements.RunAfters.evaluate (
Add integration tests __EoT__ We need a separate integration tests that will be run against platform ( i.e . bundled ) Conscrypt . Why : We 're currently using our unit tests for testing the native platform , but even though the packages are the same for the platform and tests ( i.e . org.conscrypt ) they are considered separate packages due to the fact that they came from separate class loaders ( platform is loaded by the boot classloader ) . In order to allow us to lockdown the API ( # 142 ) by making internal classes package-private , we 'll need to ensure that the tests run on Android do not attempt to access those classes . One option would be to always use java reflection in our unit tests , but this seems like overkill . Instead , we 're going to cherry-pick a few tests to a new ` integration-test ` module and have those be the tests that we run on Android .
Add integration tests __EoT__ We need a separate integration tests that will be run against platform ( i.e . bundled ) Conscrypt . Why : We 're currently using our unit tests for testing the native platform , but even though the packages are the same for the platform and tests ( i.e . org.conscrypt ) they are considered separate packages due to the fact that they came from separate class loaders ( platform is loaded by the boot classloader ) . In order to allow us to lockdown the API ( # 142 ) by making internal classes package-private , we 'll need to ensure that the tests run on Android do not attempt to access those classes . One option would be to always use java reflection in our unit tests , but this seems like overkill . Instead , we 're going to cherry-pick a few tests to a new ` integration-test ` module and have those be the tests that we run on Android .
Enable static module linking for Java 1.8 __EoT__ We can copy [ this solution ] ( https : //github.com/netty/netty-tcnative/pull/149 ) from @ carl-mastrangelo
Enable static module linking for Java 1.8 __EoT__ We can copy [ this solution ] ( https : //github.com/netty/netty-tcnative/pull/149 ) from @ carl-mastrangelo
Open Sans Cond light & light italic have the wrong family names __EoT__ These two weights are labeled as `` Open Sans , '' not `` Open Sans Condensed . '' https : //code.google.com/archive/p/googlefontdirectory/issues/274
Update Mogra __EoT__ Per https : //github.com/googlefonts/fontbakery/issues/1335 Mogra needs fixing ( I think just ttfautohinting fresh will do ) and updating .
[ URGENT ] Comfortaa BUG , one cyrillic letter overlaps in any browser or locally on ALL SITES ! __EoT__ Hello . This font is awesome , but it have a bug in one cyrillic letter , it overlaps on another letter ( not all letters ) Example : Ð’Ñ‹ - capital 'Ð’ ' overlaps 'Ñ‹ ' , this is for most next characters Ð’Ð° - this case all is fine . A few characters are ok. Much people using this font on russian sites , and all of them have this bug . I tested it in Chrome , Firefox , installed locally in libreoffice , etc . I tried to edit this font in fontforge , but width seems to be ok ... Please fix it asap or tell me how to fix it in fontforge and I can use it locally . Thanks !
Open Sans font-weight : 13px and different weights issue __EoT__ When having different font-weights next to each other ( see example ) and setting ` font-weight : 13px ` the semi-bold speciments of Open Sans are rendered smaller than the normal / light weight `` ` < p style= '' font-size : 13px '' > < span style= '' font-weight : 300 '' > 1 < /span > < span style= '' font-weight : 400 '' > 2 < /span > < span style= '' font-weight : 600 '' > 3 < /span > 45678 < span style= '' font-weight : 600 '' > 9 < /span > 789 < /p > `` ` output : ! [ bildschirmfoto 2015-11-16 um 17 23 00 ] ( https : //cloud.githubusercontent.com/assets/2707029/11187457/bfa2fe22-8c86-11e5-86c2-d151fe5904c7.png ) This is not happening at smaller or larger font-sizes ( up to 24px )
Open Sans Condensed , error when including font without specifying a weight __EoT__ I 'm using the following to include fonts in a fontpicker . All of the fonts load except for **Open Sans Condensed** , which throws a 400 error _ '' The requested font families are not available . `` _ `` ` javascript WebFont.load ( { google : { families : [ 'Arvo ' , 'Dosis ' , 'Droid Serif ' , 'Droid Sans ' , 'Glegoo ' , 'Inconsolata ' , 'Josefin Slab ' , 'Kalam ' , 'Lato ' , 'Libre Baskerville ' , 'Lobster ' , 'Merriweather ' , 'Montserrat ' , 'Muli ' , 'Nobile ' , 'Nunito ' , 'Open Sans ' , 'Open Sans Condensed ' , 'Oswald ' , 'Playball ' , 'Playfair Display ' , 'Rajdhani ' , 'Raleway ' , 'Roboto ' , 'Roboto Slab ' , 'Source Sans Pro ' , 'Rokkitt ' , 'Varela Round ' , ] } } ) `` ` I 'm assuming this has something to do with that it 's looking for a 400 weight , when the default for this font is 300 . This should be fixed so that
Ubuntu-Regular not using local font __EoT__ Hi , On my system ( macOS Sierra ) , I have installed the Ubuntu font via ` brew cask install font-ubuntu ` . The `` Regular '' font family is listed on my system under the font name `` Ubuntu '' . However , the generated CSS for Ubuntu expects the local name to be either `` Ubuntu Regular '' or `` Ubuntu-Regular '' ( see here ) . Adding local ( 'Ubuntu ' ) to the list fixes the issue completely . Therefore , I simply propose updating your font CSS generator to handle this case . Here 's the URL I 'm using : https : //fonts.googleapis.com/css ? family=Ubuntu:400 And here 's the relevant CSS for that resource : `` ` css /* latin */ @ font-face { font-family : 'Ubuntu ' ; font-style : normal ; font-weight : 400 ; src : local ( 'Ubuntu Regular ' ) , local ( 'Ubuntu-Regular ' ) , url ( https : //fonts.gstatic.com/s/ubuntu/v10/zvCUQcxqeoKhyOlbifSAaevvDin1pK8aKteLpeZ5c0A.woff2 ) format ( 'woff2 ' ) ; unicode-range : U+0000-00FF , U+0131 , U+0152-0153 , U+02C6 , U+02DA , U+02DC , U+2000-206F , U+2074 , U+20AC , U+2212 , U+2215
Update Sansita __EoT__ https : //github.com/Omnibus-Type/Sansita
Reformatting 'README.md ' in tools/encodings/GF 2016 Glyph Sets/ __EoT__ # # # # 1 . Re-Arrange Text Move the 'Language support ' list section to after the 'Structure and Heirarchy ' section . # # # # 2 . Update Some Character Set Descriptions **Note** : Instead of listing the Unicode Ranges under the 'Google Latin Plus ' section , it might be a useful to create a 'Unicode Ranges ' sub-section and list all the ranges that the largest set ( Expert ) covers . This is mainly because the unicode ranges list may be a bit misleading . For e.g . the Latin Extended Additional block supports things like 'Medievalist Additions ' . The Latin Extended B block supports 'African Linguistics ' , and 'Non-European and Historic Latin ' . So the new contents would be : # # # # # Google Latin Plus ( Total Glyphs : 600 ) - Western & Central European - Vietnamese - Currencies ( â‚¡ â‚£ â‚¤ â‚¦ â‚§ â‚© â‚« â‚­ â‚± â‚² â‚µ â‚¹ â‚º â‚¼ â‚½ ) - Alternate Numerals : Proportonal Lining # # # # # Google Latin Pro ( Total Glyphs : 151 ) -
Poppins-Bold.ttf has wrong font name `` Poppins '' __EoT__ Installing the Poppins-Bold.ttf font locally leads to websites rendering the `` Poppins Regular '' font as `` Poppins Bold '' instead . For example , installing Poppins-Bold.ttf and Poppins-Regular.ttf locally , and opening the [ the attached webpage ] ( https : //github.com/google/fonts/files/526563/rendered_bold_instead_of_regular.zip ) , will render the fonts bold : ! [ bold ] ( https : //cloud.githubusercontent.com/assets/128554/19342042/6d0b6a46-9130-11e6-863d-152a5b2dd8c1.png ) . The reason is shown here , the name within the ttf is set to `` Poppins '' for the bold font instead of `` Poppins Bold '' : ! [ here ] ( https : //cloud.githubusercontent.com/assets/128554/19341998/3bdcb920-9130-11e6-8786-61d5fa04c262.png )
Missing Montserrat Weights __EoT__ First off , thanks so much for posting this to Github . I 've been trying to use some alternate weights on the Montserrat typeface in addition to the 400 and 700 that are currently up . I see that there are three extra weights in this repository and the [ DESCRIPTION.html ] ( https : //github.com/google/fonts/blob/master/ofl/montserrat/DESCRIPTION.en_us.html # L27 ) file has an added paragraph acknowledging the update . But this paragraph is n't reflected in the [ live font specimen page ] ( http : //www.google.com/fonts/specimen/Montserrat # charset ) , and neither do the extra weights . I 'm not able to manually call them from the CDN either . Is there some kind of licensing thing preventing you guys from adding those weights ? Thanks ! ! !
[ 2016 Glyph Sets ] oneinferior should be one.subs __EoT__ oneinferior should be one.subs twoinferior should be two.subs etc ... . As per https : //www.glyphsapp.com/tutorials/superscript-and-subscript-figures
Host the Overpass font family __EoT__ https : //github.com/RedHatBrand/overpass
nunitosans : MM compatibility in asterisk __EoT__ We got a mail through which had this image attached . < img width= '' 604 '' alt= '' nunitosans_mm_compat '' src= '' https : //cloud.githubusercontent.com/assets/7525512/22108302/5e743ed8-de4a-11e6-9747-fef9692d383e.png '' > I 've confirmed this issue . Thanks @ fontHausen I 'll submit a pr now .
Update Cormorant __EoT__ A new release of Cormorant is out , fixing a Glyphs export issue .
Jdotless vs dotlessj __EoT__ I found a dotlessj in the Plus and Pro encoding . This should be jdotless , which works better with Glyphs .
Update Barlow font family ( 1.101-Pre Release - > 1.208 ) __EoT__ It will be nice if you can update Barlow font family to latest version , because current one have some issues with non english characters/languages . Git Repo : https : //github.com/jpt/barlow Detailed changelog : https : //github.com/jpt/barlow/releases Changes summary : - kerning fixed on a number of non-English characters - various spacing and other kerring tweaks - optimisation to Condensed Black variant - and more ... Refs # 1330
Open quote character in Neuton 300 is wrong __EoT__ When typing the character â€œ in Neuton 300 it misaligns with the next character as you can see in the following screenshot : ! [ screen shot 2015-04-19 at 7 05 01 pm ] ( https : //cloud.githubusercontent.com/assets/474894/7221762/0193b33c-e6c7-11e4-9d03-40e55f54c204.png )
Open quote character in Neuton 300 is wrong __EoT__ When typing the character â€œ in Neuton 300 it misaligns with the next character as you can see in the following screenshot : ! [ screen shot 2015-04-19 at 7 05 01 pm ] ( https : //cloud.githubusercontent.com/assets/474894/7221762/0193b33c-e6c7-11e4-9d03-40e55f54c204.png )
Review copyright notice emails __EoT__ Per https : //github.com/google/fonts/pull/425 we should review copyright notice emails - [ ] https : //github.com/google/fonts/pull/385 - the email on this needs an update
GF Latin Pro : Remove /firsttonechinese __EoT__ Pinyin is generated using /macroncomb in Glyphs.app . If /firsttonechinese should be used instead , what about the other tones ?
profile : detect and reject concatenated profiles __EoT__ golang/go # 22036 reported a simple problem with a mysterious result . 437 concatenated pprof profiles were passed to pprof as one file and produced the error : malformed profile : mismatch : sample has : 4 values vs. 1748 types Concatenated profiles are certainly not supported today , and I ca n't really see that changing given the details of the profile format . A change like the below seems like it would produce a better error . Will leave the actual change to you . `` ` $ git diff diff -- git a/src/cmd/vendor/github.com/google/pprof/profile/encode.go b/src/cmd/vendor/github.com/google/pprof/profile/encode.go index 622319484a..9453fa19cd 100644 -- - a/src/cmd/vendor/github.com/google/pprof/profile/encode.go +++ b/src/cmd/vendor/github.com/google/pprof/profile/encode.go @ @ -214,7 +214,13 @ @ var profileDecoder = [ ] decoder { // int64 keep_frames = 8 func ( b *buffer , m message ) error { return decodeInt64 ( b , & m. ( *Profile ) .keepFramesX ) } , // int64 time_nanos = 9 - func ( b *buffer , m message ) error { return decodeInt64 ( b , & m. ( *Profile ) .TimeNanos ) } , + func ( b *buffer , m message ) error { + p : =
https+insecure does n't extend to the symbolizer __EoT__ https : //go-review.googlesource.com/ # /c/33157/ changed ` httpGet ` to skip certificate verification on HTTP GET , but symbolization makes an extra request using HTTP POST , which silently fails . https : //github.com/google/pprof/blob/6addbe4/internal/symbolizer/symbolizer.go # L82 cc @ bdarnell EDIT : the real error is : `` ` shell $ pprof https+insecure : //127.0.0.1:8080/debug/pprof/profile Fetching profile over HTTP from https+insecure : //127.0.0.1:8080/debug/pprof/profile pprof : http post https+insecure : //127.0.0.1:8080/debug/pprof/symbol : Post https+insecure : //127.0.0.1:8080/debug/pprof/symbol : unsupported protocol scheme `` https+insecure '' `` `
Web UI should support flame graphs __EoT__ ! [ image ] ( http : //www.brendangregg.com/FlameGraphs/cpu-mysql-updated.svg ) [ Flame graphs ] ( http : //www.brendangregg.com/flamegraphs.html ) allow you to move in a specific ancestry path from a very compact UI and lets the users to zoom in/out specific paths easily . Flame graphs became one of the defato representation of profiling data in the industry in the recent years , e.g . [ go-torch ] ( https : //github.com/uber/go-torch ) is a highly successful tool and often user 's entry point to visualize pprof data in Go . Consider supporting flame graphs in addition to the dot visualization on the new web UI , so we can provide this representation out-of-the-box with minimal setup and external dependencies . ( If you need this feature as well , you can ðŸ‘ so the team knows about your interest . )
Web UI should support flame graphs __EoT__ ! [ image ] ( http : //www.brendangregg.com/FlameGraphs/cpu-mysql-updated.svg ) [ Flame graphs ] ( http : //www.brendangregg.com/flamegraphs.html ) allow you to move in a specific ancestry path from a very compact UI and lets the users to zoom in/out specific paths easily . Flame graphs became one of the defato representation of profiling data in the industry in the recent years , e.g . [ go-torch ] ( https : //github.com/uber/go-torch ) is a highly successful tool and often user 's entry point to visualize pprof data in Go . Consider supporting flame graphs in addition to the dot visualization on the new web UI , so we can provide this representation out-of-the-box with minimal setup and external dependencies . ( If you need this feature as well , you can ðŸ‘ so the team knows about your interest . )
Web UI should support flame graphs __EoT__ ! [ image ] ( http : //www.brendangregg.com/FlameGraphs/cpu-mysql-updated.svg ) [ Flame graphs ] ( http : //www.brendangregg.com/flamegraphs.html ) allow you to move in a specific ancestry path from a very compact UI and lets the users to zoom in/out specific paths easily . Flame graphs became one of the defato representation of profiling data in the industry in the recent years , e.g . [ go-torch ] ( https : //github.com/uber/go-torch ) is a highly successful tool and often user 's entry point to visualize pprof data in Go . Consider supporting flame graphs in addition to the dot visualization on the new web UI , so we can provide this representation out-of-the-box with minimal setup and external dependencies . ( If you need this feature as well , you can ðŸ‘ so the team knows about your interest . )
tagfocus needs better support for picking which tag to focus on __EoT__ Mentioned offhand in # 153 : A slightly different problem is that there 's no easy way to specify request ranges without having to write out full numbers . I 'd be very happy if I could specify , say : -tagfocus=/request/256kb:512kb to mean `` I want to see a tag , whose name matches `` request '' , whose value is between 256kb and 512kb . If we ca n't manage the unit conversion and I have to specify /request/262144:524288 , I can work with that for now ( though it 'd be very nice ... . ) This is becoming more important as we add more tags to generated profiles , some of which are n't really comparable ; they 're not tweaked values , they 're things with totally different meanings . ( think `` requested alignment '' ; I do not want tagfocus=:512 to pick up every unaligned alloc even if it was a 2 MiB request !
-traces should include tags __EoT__ Attached a profile ( zipped gzip is silly , but Github wo n't let me upload `` heapz '' . ) `` ` pprof -raw heapz ... Samples : objects/count space/bytes [ dflt ] 7438 60936269 : 1 2 3 4 5 6 bytes : [ 8192 ] request : [ 8191 ] 238 62466710 : 7 8 9 10 5 6 bytes : [ 262144 ] request : [ 262143 ] 3341 54739221 : 7 8 9 10 5 6 bytes : [ 16384 ] request : [ 16383 ] 2064 67634517 : 1 2 3 4 5 6 bytes : [ 32768 ] request : [ 32767 ] 29710 60847108 : 1 2 3 4 5 6 bytes : [ 2048 ] request : [ 2047 ] 396 51920894 : 1 2 3 4 5 6 bytes : [ 131072 ] request : [ 131071 ] 63503 65027585 : 7 8 9 10 5 6 bytes : [ 1024 ] request : [ 1023 ] 1105 72423082 : 7 8 9 10 5 6 bytes : [ 65536 ] request : [ 65535 ] 108 56884975 : 11 12 13 3 4 5
Drop frames with an anonymous namespace do n't get dropped __EoT__ If a profile has a drop frame matching against an anonymous namespace , the match never happens . I think the culprit is this line ( https : //github.com/google/pprof/blob/master/profile/prune.go # L39 ) , which checks for brackets before applying the drop frame RE , and only applies the RE to the symbol before the brackets . I 'm assuming this is for symbols that still have parameters . Here 's an example [ 1 ] , where I create a profile with a drop frame RE `` Foo : :.* : :Bar '' . If I comment that line out , things behave as expected [ 2 ] . Since this is already a hack , I 'll just special-case `` ( anonymous namespace ) '' , but there are probably other things with a legit `` ( `` as a part of the function name ( std : :function objects ? ) . [ 1 ] $ pprof -traces tmp.pb ... -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
internal/driver : test timed out after 3m0s on freebsd , openbsd , windows __EoT__ Encountered in https : //go-review.googlesource.com/c/go/+/89715 , after trying to update the vendored copy to pull https : //github.com/google/pprof/pull/298 . https : //storage.googleapis.com/go-build-log/b9cbc869/freebsd-amd64-11_1_dbe018c1.log Relevant bit of output : `` ` Generating report in /tmp/profile_proto401663026 Generating report in /tmp/profile_output286659561 Generating report in /tmp/profile_proto375996468 Generating report in /tmp/profile_output864397315 Generating report in /tmp/profile_proto039962502 Generating report in /tmp/profile_output187100461 Generating report in /tmp/profile_proto409183912 Ignoring local file /path/to/buildid : build-id mismatch ( buildid ! = abcdef ) Generating report in /tmp/profile_output785726951 Generating report in /tmp/profile_proto884247066 Ignoring local file /path/to/buildid : build-id mismatch ( buildid ! = abcdef ) Generating report in /tmp/profile_output637371569 Generating report in /tmp/profile_proto247173724 Ignoring local file /path/to/buildid : build-id mismatch ( buildid ! = abcdef ) Generating report in /tmp/profile_output097694731 Generating report in /tmp/profile_proto492795374 Ignoring local file /path/to/buildid : build-id mismatch ( buildid ! = abcdef ) Generating report in /tmp/profile_output034252917 Generating report in /tmp/profile_proto368253776 Generating report in /tmp/profile_output322350703 Generating report in /tmp/profile_proto977961218 Generating report in /tmp/profile_output894972025 Generating report in /tmp/profile_proto420701572 Generating report in /tmp/profile_output650228499 Generating report in /tmp/profile_proto732512086 Hide expression matched no samples Generating report in /tmp/profile_output521628349 Generating report in /tmp/profile_proto277399288 Ignoring local file /path/to/buildid : build-id mismatch (
internal/driver : test timed out after 3m0s on freebsd , openbsd , windows __EoT__ Encountered in https : //go-review.googlesource.com/c/go/+/89715 , after trying to update the vendored copy to pull https : //github.com/google/pprof/pull/298 . https : //storage.googleapis.com/go-build-log/b9cbc869/freebsd-amd64-11_1_dbe018c1.log Relevant bit of output : `` ` Generating report in /tmp/profile_proto401663026 Generating report in /tmp/profile_output286659561 Generating report in /tmp/profile_proto375996468 Generating report in /tmp/profile_output864397315 Generating report in /tmp/profile_proto039962502 Generating report in /tmp/profile_output187100461 Generating report in /tmp/profile_proto409183912 Ignoring local file /path/to/buildid : build-id mismatch ( buildid ! = abcdef ) Generating report in /tmp/profile_output785726951 Generating report in /tmp/profile_proto884247066 Ignoring local file /path/to/buildid : build-id mismatch ( buildid ! = abcdef ) Generating report in /tmp/profile_output637371569 Generating report in /tmp/profile_proto247173724 Ignoring local file /path/to/buildid : build-id mismatch ( buildid ! = abcdef ) Generating report in /tmp/profile_output097694731 Generating report in /tmp/profile_proto492795374 Ignoring local file /path/to/buildid : build-id mismatch ( buildid ! = abcdef ) Generating report in /tmp/profile_output034252917 Generating report in /tmp/profile_proto368253776 Generating report in /tmp/profile_output322350703 Generating report in /tmp/profile_proto977961218 Generating report in /tmp/profile_output894972025 Generating report in /tmp/profile_proto420701572 Generating report in /tmp/profile_output650228499 Generating report in /tmp/profile_proto732512086 Hide expression matched no samples Generating report in /tmp/profile_output521628349 Generating report in /tmp/profile_proto277399288 Ignoring local file /path/to/buildid : build-id mismatch (
graph separate functions by filename+name by default __EoT__ # # # What version of pprof are you using ? latest - whatever `` go get ... '' gave me on 8/23/2017 # # # What did you do ? I have created profile file manually . Not some cpu sampler . But for a good reason . Anyway , there are function names that collide . Prefixing them with the full thing would be very long , ugly and useless ( for this case ) . I have put the prefixes as file_name field Function message in the proto . Colliding function names are common . Name mangling is ugly . How about colliding functions with the same name but different filename ? Has anybody seen it yet ? And it is the way the proto is structured already . # # # What did you expect to see ? I would expect the graph rendering not to merge the two functions by name but rather by file_name + name . Note : `` pprof -- files ... '' does not take file_name from Function but rather from Mapping .
`` For tag bytes used unit bytes , also encountered unit ( s ) '' error message when opening a heap profile __EoT__ Steps to reproduce - see below . Note the `` For tag bytes used unit bytes , also encountered unit ( s ) '' message that should n't be printed . `` ` $ go run pprof.go -top ./profile/testdata/cppbench.heap Local symbolization failed for cppbench_server_main : stat /home/cppbench_server_main : no such file or directory Local symbolization failed for libpthread-2.15.so : stat /lib/libpthread-2.15.so : no such file or directory Local symbolization failed for libc-2.15.so : stat /lib/libc-2.15.so : no such file or directory Some binary filenames not available . Symbolization may be incomplete . Try setting PPROF_BINARY_PATH to the search path for local binaries . For tag bytes used unit bytes , also encountered unit ( s ) File : cppbench_server_main Type : space Showing nodes accounting for 77.19MB , 100 % of 77.19MB total flat flat % sum % cum cum % 77.19MB 100 % 100 % 77.19MB 100 % [ cppbench_server_main ] 0 0 % 100 % 2MB 2.59 % [ libc-2.15.so ] 0 0 % 100 % 73.18MB 94.81 % [ libpthread-2.15.so ] `` `
`` For tag bytes used unit bytes , also encountered unit ( s ) '' error message when opening a heap profile __EoT__ Steps to reproduce - see below . Note the `` For tag bytes used unit bytes , also encountered unit ( s ) '' message that should n't be printed . `` ` $ go run pprof.go -top ./profile/testdata/cppbench.heap Local symbolization failed for cppbench_server_main : stat /home/cppbench_server_main : no such file or directory Local symbolization failed for libpthread-2.15.so : stat /lib/libpthread-2.15.so : no such file or directory Local symbolization failed for libc-2.15.so : stat /lib/libc-2.15.so : no such file or directory Some binary filenames not available . Symbolization may be incomplete . Try setting PPROF_BINARY_PATH to the search path for local binaries . For tag bytes used unit bytes , also encountered unit ( s ) File : cppbench_server_main Type : space Showing nodes accounting for 77.19MB , 100 % of 77.19MB total flat flat % sum % cum cum % 77.19MB 100 % 100 % 77.19MB 100 % [ cppbench_server_main ] 0 0 % 100 % 2MB 2.59 % [ libc-2.15.so ] 0 0 % 100 % 73.18MB 94.81 % [ libpthread-2.15.so ] `` `
`` For tag bytes used unit bytes , also encountered unit ( s ) '' error message when opening a heap profile __EoT__ Steps to reproduce - see below . Note the `` For tag bytes used unit bytes , also encountered unit ( s ) '' message that should n't be printed . `` ` $ go run pprof.go -top ./profile/testdata/cppbench.heap Local symbolization failed for cppbench_server_main : stat /home/cppbench_server_main : no such file or directory Local symbolization failed for libpthread-2.15.so : stat /lib/libpthread-2.15.so : no such file or directory Local symbolization failed for libc-2.15.so : stat /lib/libc-2.15.so : no such file or directory Some binary filenames not available . Symbolization may be incomplete . Try setting PPROF_BINARY_PATH to the search path for local binaries . For tag bytes used unit bytes , also encountered unit ( s ) File : cppbench_server_main Type : space Showing nodes accounting for 77.19MB , 100 % of 77.19MB total flat flat % sum % cum cum % 77.19MB 100 % 100 % 77.19MB 100 % [ cppbench_server_main ] 0 0 % 100 % 2MB 2.59 % [ libc-2.15.so ] 0 0 % 100 % 73.18MB 94.81 % [ libpthread-2.15.so ] `` `
In the web UI the Refine menu does not enable after a search string is typed __EoT__ Please answer these questions before submitting your issue . Thanks ! # # # What version of pprof are you using ? Master at 9cd381157e8842177d38d881690003a328e9c3ba . # # # What operating system and processor architecture are you using ? Linux # # # What did you do ? I run ` go run pprof.go -http :8080 profile.pb.gz ` . I then enter something like `` foo '' in the search box . # # # What did you expect to see ? I expected the Refine menu to enable . It used to work . # # # What did you see instead ? The refine menu does not enable .
NSS startup crash __EoT__ Weird that this is only reproducing on ClusterFuzz and not on Docker container locally with fuzzer hash in nss . @ ttaubert - any clues ? hash : ../../fuzz/shared.h:21 : NSSDatabase : :NSSDatabase ( ) : Assertion ` NSS_NoDB_Init ( nullptr ) == SECSuccess failed . ASAN : DEADLYSIGNAL ==1==ERROR : AddressSanitizer : ABRT on unknown address 0x000000000001 ( pc 0x7f8fb75f9418 bp 0x000000c06760 sp 0x7ffcca718ea8 T0 ) SCARINESS : 10 ( signal ) # 0 0x7f8fb75f9417 in gsignal # 1 0x7f8fb75fb019 in abort # 2 0x7f8fb75f1bd6 in libc.so.6 # 3 0x7f8fb75f1c81 in __assert_fail # 4 0x51502a in NSSDatabase : :NSSDatabase ( ) /src/nss/fuzz/shared.h:21:19 # 5 0x51502a in LLVMFuzzerTestOneInput /src/nss/fuzz/hash_target.cc:19 # 6 0x969ee8 in fuzzer : :Fuzzer : :ExecuteCallback ( unsigned char const* , unsigned long ) /src/libfuzzer/FuzzerLoop.cpp:550:13 # 7 0x96906d in fuzzer : :Fuzzer : :ShuffleAndMinimize ( std : :__1 : :vector < std : :__1 : :vector < unsigned char , std : :__1 : :allocator > , std : :__1 : :allocator < std : :__1 : :vector < unsigned char , std : :__1 : :allocator > > > ) /src/libfuzzer/FuzzerLoop.cpp:477:3 # 8 0xa4790a in fuzzer : :FuzzerDriver ( int , char*** ,
Instrumentation of c-ares fuzzers __EoT__ The build is failing ( https : //oss-fuzz-build-logs.storage.googleapis.com/index.html ) since ` ares_create_query_fuzzer ` has only 42 coverage edges ... It might be true , but another fuzz target ( ` c-ares_ares_parse_reply_fuzzer ` ) has 697 of them . Need to take a closer look .
Instrumentation of c-ares fuzzers __EoT__ The build is failing ( https : //oss-fuzz-build-logs.storage.googleapis.com/index.html ) since ` ares_create_query_fuzzer ` has only 42 coverage edges ... It might be true , but another fuzz target ( ` c-ares_ares_parse_reply_fuzzer ` ) has 697 of them . Need to take a closer look .
Need some way to check project is built with coverage properly ( post build time ) . __EoT__
qubes-os : bad build , partial instrumentation __EoT__ Has been detected after lading # 838 https : //oss-fuzz-build-logs.storage.googleapis.com/index.html `` ` Step # 5 : INFO : performing bad build checks for /workspace/out/address/libqubes-rpc-filecopy . Step # 5 : Broken fuzz targets ( 1 ) : Step # 5 : libqubes-rpc-filecopy : Step # 5 : BAD BUILD : the target seems to have only partial coverage instrumentation . Step # 5 : ERROR : 100 % of fuzz targets seem to be broken . See the list above for a detailed information . `` `
fix c-ares after introducing -DFUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION __EoT__ See https : //github.com/google/oss-fuzz/pull/410 # issuecomment-281568382 `` ` configure : CFLAGS error : CFLAGS may only be used to specify C compiler flags , not macro definitions . `` ` One solution is to remove -DFUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION from global flags and use it only in those projects there needed . ( would be sad ) . Another solution is to hack https : //github.com/google/oss-fuzz/blob/master/projects/c-ares/build.sh to cut -DFUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION from CFLAGS ( ouch ! ) . Let me try this first . Also , let me ask David why is the configure script that picky ... OMG , I hate build systems .
fix c-ares after introducing -DFUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION __EoT__ See https : //github.com/google/oss-fuzz/pull/410 # issuecomment-281568382 `` ` configure : CFLAGS error : CFLAGS may only be used to specify C compiler flags , not macro definitions . `` ` One solution is to remove -DFUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION from global flags and use it only in those projects there needed . ( would be sad ) . Another solution is to hack https : //github.com/google/oss-fuzz/blob/master/projects/c-ares/build.sh to cut -DFUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION from CFLAGS ( ouch ! ) . Let me try this first . Also , let me ask David why is the configure script that picky ... OMG , I hate build systems .
Startup crash in yara fuzz target __EoT__ Has been detected after lading # 838 `` ` root @ 01b74464f8a8 : /out # ./macho_fuzzer INFO : Seed : 3726464007 INFO : Loaded 1 modules ( 6953 inline 8-bit counters ) : 6953 [ 0x9b2be8 , 0x9b4711 ) , INFO : Loaded 1 PC tables ( 6953 PCs ) : 6953 [ 0x71f7b0,0x73aa40 ) , INFO : -max_len is not provided ; libFuzzer will not generate inputs larger than 4096 bytes macho_fuzzer : /src/libfuzzer/FuzzerLoop.cpp:521 : void fuzzer : :Fuzzer : :ExecuteCallback ( const uint8_t * , size_t ) : Assertion ` Res == 0 ' failed . ==18== ERROR : libFuzzer : deadly signal # 0 0x4f7ab3 in __sanitizer_print_stack_trace /src/llvm/projects/compiler-rt/lib/asan/asan_stack.cc:38 # 1 0x5f07a5 in fuzzer : :Fuzzer : :CrashCallback ( ) /src/libfuzzer/FuzzerLoop.cpp:233:5 # 2 0x5f073c in fuzzer : :Fuzzer : :StaticCrashSignalCallback ( ) /src/libfuzzer/FuzzerLoop.cpp:206:6 # 3 0x7f53960c538f ( /lib/x86_64-linux-gnu/libpthread.so.0+0x1138f ) # 4 0x7f53956fd427 in gsignal ( /lib/x86_64-linux-gnu/libc.so.6+0x35427 ) # 5 0x7f53956ff029 in abort ( /lib/x86_64-linux-gnu/libc.so.6+0x37029 ) # 6 0x7f53956f5bd6 ( /lib/x86_64-linux-gnu/libc.so.6+0x2dbd6 ) # 7 0x7f53956f5c81 in __assert_fail ( /lib/x86_64-linux-gnu/libc.so.6+0x2dc81 ) # 8 0x5f47b2 in fuzzer : :Fuzzer : :ExecuteCallback ( unsigned char const* , unsigned long ) /src/libfuzzer/FuzzerLoop.cpp:521:3 # 9 0x5f71b9 in
h2o : startup crash __EoT__ Has been detected after lading # 838 https : //oss-fuzz-build-logs.storage.googleapis.com/index.html `` ` Step # 5 : INFO : performing bad build checks for /workspace/out/address/h2o-fuzzer-url . Step # 5 : INFO : performing bad build checks for /workspace/out/address/h2o-fuzzer-http1 . Step # 5 : INFO : performing bad build checks for /workspace/out/address/h2o-fuzzer-http2 . Step # 5 : Broken fuzz targets ( 1 ) : Step # 5 : h2o-fuzzer-http2 : Step # 5 : BAD BUILD : the fuzzer seems to have either startup crash or exit . Step # 5 : ERROR : 33 % of fuzz targets seem to be broken . See the list above for a detailed information . `` `
libpsl coverage build breaks ( due to libicu link issue ) __EoT__ The 'coverage ' build introduces 'floor ( ) ' to the libicu dependency which requires -lm for linking . The other builds ( address , undefined , ... ) do n't do such . The libicu pkg-config file does n't cover -lm . Can this be solved at oss-fuzz upstream to avoid ugly work-arounds from my side ? The error report is at https : //bugs.chromium.org/p/oss-fuzz/issues/detail ? id=10591
systemd : bad build __EoT__ Has been detected after lading # 838 https : //oss-fuzz-build-logs.storage.googleapis.com/index.html `` ` Step # 5 : INFO : performing bad build checks for /workspace/out/address/src/shared/libsystemd-shared-238.so . Step # 5 : INFO : performing bad build checks for /workspace/out/address/fuzz-dns-packet . Step # 5 : INFO : performing bad build checks for /workspace/out/address/fuzz-unit-file . Step # 5 : INFO : performing bad build checks for /workspace/out/address/fuzz-dhcp-server . Step # 5 : /usr/local/bin/bad_build_check : line 37 : 21 Segmentation fault ( core dumped ) $ FUZZER -runs= $ MIN_NUMBER_OF_RUNS & > $ FUZZER_OUTPUT Step # 5 : /usr/local/bin/bad_build_check : line 58 : ( ( : < 256 : syntax error : operand expected ( error token is `` < 256 `` ) Step # 5 : Broken fuzz targets ( 1 ) : Step # 5 : libsystemd-shared-238.so : Step # 5 : BAD BUILD : /workspace/out/address/src/shared/libsystemd-shared-238.so does not seem to be compiled with ASan . Step # 5 : BAD BUILD : the fuzzer seems to have either startup crash or exit . Step # 5 : ERROR : 25 % of fuzz targets seem to be broken . See the list above for a detailed information .
systemd : bad build __EoT__ Has been detected after lading # 838 https : //oss-fuzz-build-logs.storage.googleapis.com/index.html `` ` Step # 5 : INFO : performing bad build checks for /workspace/out/address/src/shared/libsystemd-shared-238.so . Step # 5 : INFO : performing bad build checks for /workspace/out/address/fuzz-dns-packet . Step # 5 : INFO : performing bad build checks for /workspace/out/address/fuzz-unit-file . Step # 5 : INFO : performing bad build checks for /workspace/out/address/fuzz-dhcp-server . Step # 5 : /usr/local/bin/bad_build_check : line 37 : 21 Segmentation fault ( core dumped ) $ FUZZER -runs= $ MIN_NUMBER_OF_RUNS & > $ FUZZER_OUTPUT Step # 5 : /usr/local/bin/bad_build_check : line 58 : ( ( : < 256 : syntax error : operand expected ( error token is `` < 256 `` ) Step # 5 : Broken fuzz targets ( 1 ) : Step # 5 : libsystemd-shared-238.so : Step # 5 : BAD BUILD : /workspace/out/address/src/shared/libsystemd-shared-238.so does not seem to be compiled with ASan . Step # 5 : BAD BUILD : the fuzzer seems to have either startup crash or exit . Step # 5 : ERROR : 25 % of fuzz targets seem to be broken . See the list above for a detailed information .
systemd : bad build __EoT__ Has been detected after lading # 838 https : //oss-fuzz-build-logs.storage.googleapis.com/index.html `` ` Step # 5 : INFO : performing bad build checks for /workspace/out/address/src/shared/libsystemd-shared-238.so . Step # 5 : INFO : performing bad build checks for /workspace/out/address/fuzz-dns-packet . Step # 5 : INFO : performing bad build checks for /workspace/out/address/fuzz-unit-file . Step # 5 : INFO : performing bad build checks for /workspace/out/address/fuzz-dhcp-server . Step # 5 : /usr/local/bin/bad_build_check : line 37 : 21 Segmentation fault ( core dumped ) $ FUZZER -runs= $ MIN_NUMBER_OF_RUNS & > $ FUZZER_OUTPUT Step # 5 : /usr/local/bin/bad_build_check : line 58 : ( ( : < 256 : syntax error : operand expected ( error token is `` < 256 `` ) Step # 5 : Broken fuzz targets ( 1 ) : Step # 5 : libsystemd-shared-238.so : Step # 5 : BAD BUILD : /workspace/out/address/src/shared/libsystemd-shared-238.so does not seem to be compiled with ASan . Step # 5 : BAD BUILD : the fuzzer seems to have either startup crash or exit . Step # 5 : ERROR : 25 % of fuzz targets seem to be broken . See the list above for a detailed information .
systemd : bad build __EoT__ Has been detected after lading # 838 https : //oss-fuzz-build-logs.storage.googleapis.com/index.html `` ` Step # 5 : INFO : performing bad build checks for /workspace/out/address/src/shared/libsystemd-shared-238.so . Step # 5 : INFO : performing bad build checks for /workspace/out/address/fuzz-dns-packet . Step # 5 : INFO : performing bad build checks for /workspace/out/address/fuzz-unit-file . Step # 5 : INFO : performing bad build checks for /workspace/out/address/fuzz-dhcp-server . Step # 5 : /usr/local/bin/bad_build_check : line 37 : 21 Segmentation fault ( core dumped ) $ FUZZER -runs= $ MIN_NUMBER_OF_RUNS & > $ FUZZER_OUTPUT Step # 5 : /usr/local/bin/bad_build_check : line 58 : ( ( : < 256 : syntax error : operand expected ( error token is `` < 256 `` ) Step # 5 : Broken fuzz targets ( 1 ) : Step # 5 : libsystemd-shared-238.so : Step # 5 : BAD BUILD : /workspace/out/address/src/shared/libsystemd-shared-238.so does not seem to be compiled with ASan . Step # 5 : BAD BUILD : the fuzzer seems to have either startup crash or exit . Step # 5 : ERROR : 25 % of fuzz targets seem to be broken . See the list above for a detailed information .
h2o : fuzz target crashes on startup . __EoT__ https : //oss-fuzz-build-logs.storage.googleapis.com/index.html `` ` Step # 5 : INFO : performing bad build checks for /workspace/out/address/h2o-fuzzer-url . Step # 5 : INFO : performing bad build checks for /workspace/out/address/h2o-fuzzer-http1 . Step # 5 : INFO : performing bad build checks for /workspace/out/address/h2o-fuzzer-http2 . Step # 5 : Broken fuzz targets ( 1 ) : Step # 5 : h2o-fuzzer-http1 : Step # 5 : h2o-fuzzer-http1 has a crash input in its seed coprpus : Step # 5 : Using seed corpus : h2o-fuzzer-http1_seed_corpus.zip Step # 5 : /workspace/out/address/h2o-fuzzer-http1 -rss_limit_mb=2048 -timeout=25 -runs=0 /tmp/h2o-fuzzer-http1_corpus -close_fd_mask=3 -max_len=16384 -dict=http.dict < /dev/null Step # 5 : Dictionary : 126 entries Step # 5 : INFO : Seed : 2301698242 Step # 5 : INFO : Loaded 1 modules ( 9426 inline 8-bit counters ) : 9426 [ 0xc0e350 , 0xc10822 ) , Step # 5 : INFO : Loaded 1 PC tables ( 9426 PCs ) : 9426 [ 0x933450,0x958170 ) , Step # 5 : INFO : 1709 files found in /tmp/h2o-fuzzer-http1_corpus Step # 5 : ALARM : working on the last Unit for 25 seconds Step # 5 : and the timeout value
Startup crash in augeas fuzz target __EoT__ Has been detected after lading https : //github.com/google/oss-fuzz/pull/838 Verified locally as well : `` ` # ls augeas_escape_name_fuzzer root @ 0d5da1488d9e : /out # ./augeas_escape_name_fuzzer INFO : Seed : 2381063852 INFO : Loaded 1 modules ( 11524 inline 8-bit counters ) : 11524 [ 0xa112e8 , 0xa13fec ) , INFO : Loaded 1 PC tables ( 11524 PCs ) : 11524 [ 0x76dd90,0x79add0 ) , INFO : -max_len is not provided ; libFuzzer will not generate inputs larger than 4096 bytes INFO : A corpus is not provided , starting from an empty corpus ================================================================= ==20==ERROR : AddressSanitizer : alloc-dealloc-mismatch ( malloc vs operator delete [ ] ) on 0x602000007130 # 0 0x524f50 in operator delete [ ] ( void* ) /src/llvm/projects/compiler-rt/lib/asan/asan_new_delete.cc:153 # 1 0x5285dd in escape_match ( unsigned char const* , unsigned long ) /src/augeas_escape_name_fuzzer.cc:35:5 # 2 0x528720 in LLVMFuzzerTestOneInput /src/augeas_escape_name_fuzzer.cc:41:5 # 3 0x5521b1 in fuzzer : :Fuzzer : :ExecuteCallback ( unsigned char const* , unsigned long ) /src/libfuzzer/FuzzerLoop.cpp:517:13 # 4 0x5507ef in fuzzer : :Fuzzer : :RunOne ( unsigned char const* , unsigned long , bool , fuzzer : :InputInfo* , bool* ) /src/libfuzzer/FuzzerLoop.cpp:442:3 # 5 0x55544b in fuzzer : :Fuzzer
Tracking of projects that fail to run coverage job __EoT__ - [ x ] **bad_example** Failed to get list of targets from `` https : //storage.googleapis.com/clusterfuzz-builds/bad_example/targets.list.address '' . as the target is not being run on CF - [ x ] **chakra** The same root cause : Failed to get list of targets from `` https : //storage.googleapis.com/clusterfuzz-builds/chakra/targets.list.address '' . - [ x ] **dlplibs** the disk is full , can not unpack the corpus : `` ` I Step # 4 : /corpus/mswksfuzzer/5baeffd6604290d2cbea9a2112181f6c7b91ea0c : write error ( disk full ? ) . Continue ? ( y/n/^C ) I Step # 4 : warning : /corpus/mswksfuzzer/5baeffd6604290d2cbea9a2112181f6c7b91ea0c is probably truncated I Step # 4 : checkdir : can not create extraction directory : /corpus/mswrdfuzzer I Step # 4 : No space left on device I Step # 4 : checkdir : can not create extraction directory : /corpus/multiplanfuzzer I Step # 4 : No space left on device `` ` - [ x ] **example** download_corpus failed , target is disabled on CF - [ x ] **firefox** the disk is full , can not finish compilation `` ` I Step # 2 : + find media/webrtc/trunk/webrtc/test/fuzzers/corpora/stun-corpus -type f -exec zip
Broken libass build ( MSan ) __EoT__ https : //oss-fuzz-build-logs.storage.googleapis.com/index.html `` ` Step # 10 : + check_startup_crash /workspace/out/memory/libass_fuzzer Step # 10 : + local FUZZER=/workspace/out/memory/libass_fuzzer Step # 10 : + local CHECK_PASSED=0 Step # 10 : + [ [ libfuzzer = libfuzzer ] ] Step # 10 : ++ /workspace/out/memory/libass_fuzzer -runs=4 Step # 10 : ++ egrep 'Done 4 runs ' -c Step # 10 : + CHECK_PASSED=0 Step # 10 : BAD BUILD : the fuzzer seems to have either startup crash or exit . Step # 10 : + ( ( 0 == 0 ) ) Step # 10 : + echo 'BAD BUILD : the fuzzer seems to have either startup crash or exit . ' Step # 10 : + exit 1 `` `
Do not report coverage build failures in case of a missing corpus for a new target __EoT__ Reported in https : //github.com/google/oss-fuzz/issues/1743 # issuecomment-419050479 I was hoping that we wo n't get 2 consecutive failures with a new fuzz target without corpus backup , but looks like it 's still possible . Potential solutions to the problem : 1 ) parse the log and , if there is corpus download failure , behave differently 2 ) ignore corpus download failures , in that case we 'll still be getting the reports 3 ) increase consecutive build failure threshold to 3 for coverage builds 1 ) is hacky and fragile , 3 ) would make everything even slower I 'll give this another thought and probably would go with 2 ) .
Do n't group testcases across projects __EoT__ As seen in https : //github.com/google/oss-fuzz/pull/795 # issuecomment-325331689 , we should n't be grouping across projects .
Bad Build Checks check libraries __EoT__ I saw this output on a local branch of skia : `` ` Broken fuzz targets ( 2 ) : libEGL.so : BAD BUILD : /out/libEGL.so seems to have either startup crash or exit : libGLESv2.so : BAD BUILD : /out/libGLESv2.so seems to have either startup crash or exit : 22 fuzzers total , 2 seem to be broken ( 9 % ) . Check build passed `` `
Bad Build Checks check libraries __EoT__ I saw this output on a local branch of skia : `` ` Broken fuzz targets ( 2 ) : libEGL.so : BAD BUILD : /out/libEGL.so seems to have either startup crash or exit : libGLESv2.so : BAD BUILD : /out/libGLESv2.so seems to have either startup crash or exit : 22 fuzzers total , 2 seem to be broken ( 9 % ) . Check build passed `` `
improve ots fuzzing __EoT__ @ khaledhosny , can you please help to improve ots fuzzing . See our blog post at https : //opensource.googleblog.com/2016/12/announcing-oss-fuzz-continuous-fuzzing.html 1 . Add yourself to https : //github.com/google/oss-fuzz/blob/master/projects/ots/project.yaml like this : primary_contact : `` your-email '' 2 . Move the fuzz target ( https : //github.com/google/oss-fuzz/blob/master/projects/ots/ots_fuzzer.cc ) to the upstream repository . See more about our preferred integration here : https : //github.com/google/oss-fuzz/blob/master/docs/ideal_integration.md Please see if the fuzz target code can be improved . 3 . Modify Dockerfile and build.sh in https : //github.com/google/oss-fuzz/tree/master/projects/ots and move seed corpus to somewhere in your repo . More info : https : //github.com/google/oss-fuzz/blob/master/docs/new_project_guide.md # seed-corpus 4 . Once 1-3 are done , wait for a few days and look at clusterfuzz-external.appspot.com , find ots and see if the fuzzer has good enough coverage .
firefox micro-targets __EoT__ I 'd like to know what Googlers ( and Mozillians ) think of this . There are currently two Firefox projects here : ` qcms ` and ` spidermonkey ` . Both build stand-alone and are also useful separate from Firefox . Firefox _core_ does however have many more potential targets . I 'll give a simple example : the FTP LIST parser . [ Please take a look . ] ( https : //dxr.mozilla.org/mozilla-central/source/netwerk/streamconv/converters/ParseFTPList.cpp ) For such targets Firefox has a native fuzzing interface , which is integrated into its normal build system , and produces a binary that can be run with the normal libFuzzer flags . ( ASAN-only currently . ) Quite similar to how ` spidermonkey ` seems to be integrated here right now . Is this still within the scope of ` oss-fuzz ` ?
when running just built fuzzer binaries inside docker , use the full path __EoT__ .. to avoid collisions with other things in PATH , including bash builtins . Example from nss ( https : //github.com/google/oss-fuzz/pull/316 # issuecomment-275352765 ) : `` ` testing hash Using seed corpus : hash_seed_corpus.zip hash -rss_limit_mb=2048 -timeout=25 -runs=32 /tmp/seed_corpus/ bash : line 0 : hash : -s : invalid option hash : usage : hash [ -lr ] [ -p pathname ] [ -dt ] [ name ... ] ERROR : bad exit code : 2 `` `
Coverage : add a way for projects to specify additional arguments __EoT__ The script ( https : //github.com/google/oss-fuzz/blob/master/docs/code_coverage.md # additional-arguments-for-llvm-cov ) already allows that , we just need to add some field to project.yaml that will be automatically taken and passed to the coverage job . Requested in https : //github.com/google/oss-fuzz/issues/1720 # issuecomment-413901806
Coverage : add a way for projects to specify additional arguments __EoT__ The script ( https : //github.com/google/oss-fuzz/blob/master/docs/code_coverage.md # additional-arguments-for-llvm-cov ) already allows that , we just need to add some field to project.yaml that will be automatically taken and passed to the coverage job . Requested in https : //github.com/google/oss-fuzz/issues/1720 # issuecomment-413901806
Coverage : add a way for projects to specify additional arguments __EoT__ The script ( https : //github.com/google/oss-fuzz/blob/master/docs/code_coverage.md # additional-arguments-for-llvm-cov ) already allows that , we just need to add some field to project.yaml that will be automatically taken and passed to the coverage job . Requested in https : //github.com/google/oss-fuzz/issues/1720 # issuecomment-413901806
fuzzer failing to link __EoT__ libvpx currently builds asan , msan , and another fuzzer . The first two are still building , but the last one just started having a linking error and I 'm not sure what 's going on . The part that is failing to link has not been changed recently and I 'm not sure how it could be changed to induce the error I 'm seeing . Because the other fuzzer configurations are building , this looks like a toolchain issue to me . https : //bugs.chromium.org/p/oss-fuzz/issues/detail ? id=11437 Step # 21 : CC=clang Step # 21 : CXX=clang++ Step # 21 : CFLAGS=-O1 -fno-omit-frame-pointer -gline-tables-only -DFUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION -fsanitize=bool , array-bounds , float-divide-by-zero , function , integer-divide-by-zero , return , shift , signed-integer-overflow , unsigned-integer-overflow , vla-bound , vptr -fno-sanitize-recover=bool , array-bounds , float-divide-by-zero , function , integer-divide-by-zero , return , shift , signed-integer-overflow , vla-bound , vptr -fsanitize=fuzzer-no-link Step # 21 : CXXFLAGS=-O1 -fno-omit-frame-pointer -gline-tables-only -DFUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION -fsanitize=bool , array-bounds , float-divide-by-zero , function , integer-divide-by-zero , return , shift , signed-integer-overflow , unsigned-integer-overflow , vla-bound , vptr -fno-sanitize-recover=bool , array-bounds , float-divide-by-zero , function , integer-divide-by-zero , return , shift , signed-integer-overflow , vla-bound
Specify depth for identical testcase detection __EoT__ With the various gstreamer targets , we have setup a custom message handler to raise abort ( ) on critical messages ( which are essentially codepaths that should n't be reached ) . The problem is that we end up with stack traces like this : # 0 0x7f15a7835427 in gsignal /build/glibc-9tT8Do/glibc-2.23/sysdeps/unix/sysv/linux/raise.c:54 # 1 0x7f15a7837029 in abort /build/glibc-9tT8Do/glibc-2.23/stdlib/abort.c:89 # 2 0x42fa49 in custom_logger /src/gst-ci/fuzzing/gst-discoverer.c:79:5 # 3 0x7264cc in g_logv /work/glib-2.54.2/glib/gmessages.c:1341:11 # 4 0x726117 in g_log /work/glib-2.54.2/glib/gmessages.c:1403:3 # 5 0x5a0a50 in **What actually triggered the critical** ... It seems only the 3 functions before abort ( ) are the ones being used to detect identical issues ( which will always be the same ) , whereas we would want to add at least a function or two before that to detect similarities . Is there a way to customize the depth for similar testcase detection ?
wuff build is broken __EoT__ https : //oss-fuzz-build-logs.storage.googleapis.com/index.html `` ` Step # 4 : + for f in 'fuzz/c/std/*_fuzzer.cc' Step # 4 : ++ basename fuzz/c/std/gif_fuzzer.cc _fuzzer.cc Step # 4 : + b=gif Step # 4 : + clang++ -O1 -fno-omit-frame-pointer -gline-tables-only -DFUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION -fsanitize=address -fsanitize-address-use-after-scope -fsanitize=fuzzer-no-link -stdlib=libc++ -std=c++11 fuzz/c/std/gif_fuzzer.cc -o /workspace/out/address/gif_fuzzer -lFuzzingEngine Step # 4 : + zip -- junk-paths /workspace/out/address/gif_fuzzer_seed_corpus.zip 'test/testdata/*.gif' Step # 4 : zip warning : name not matched : test/testdata/*.gif Step # 4 : Step # 4 : zip error : Nothing to do ! ( /workspace/out/address/gif_fuzzer_seed_corpus.zip ) Finished Step # 4 ERROR `` `
resiprocate : bad build __EoT__ Has been detected after lading # 838 https : //oss-fuzz-build-logs.storage.googleapis.com/index.html `` ` Step # 5 : INFO : performing bad build checks for /workspace/out/address/aresfuzzname . Step # 5 : INFO : performing bad build checks for /workspace/out/address/fuzzUtil . Step # 5 : INFO : performing bad build checks for /workspace/out/address/aresfuzz . Step # 5 : INFO : performing bad build checks for /workspace/out/address/fuzzStack . Step # 5 : Broken fuzz targets ( 2 ) : Step # 5 : aresfuzz : Step # 5 : BAD BUILD : the target seems to have only partial coverage instrumentation . Step # 5 : aresfuzzname : Step # 5 : BAD BUILD : the target seems to have only partial coverage instrumentation . Step # 5 : ERROR : 50 % of fuzz targets seem to be broken . See the list above for a detailed information . `` ` I think we 've had a discussion previously in # 826 . The problem is that these two fuzz targets are super small and have 95 and 23 PCs respectively . Do we really need them ? Are those different from the ` c-ares ` targets (
[ bignum-fuzzer ] Build succeeds locally , but ClusterFuzz-External keeps reporting failure __EoT__ Since https : //github.com/google/oss-fuzz/pull/1810 I can build bignum-fuzzer locally . But the build on your end is still failing : https : //oss-fuzz-build-logs.storage.googleapis.com/log-c686f700-479b-4e35-8ad3-85dbe38a60d7.txt As you can see here : https : //raw.githubusercontent.com/guidovranken/bignum-fuzzer/master/modules/rust/Makefile I use static instrumentation flags because I can not use CFLAGS directly with the Rust compiler . And possibly the problem lies in the discrepancy between ClusterFuzz ' `` ` -fsanitize-coverage= `` ` flags and my static Rust flags . Can you tell me how to simulate the build configuration as used in the build log above so I can test and resolve this locally before submitting a patch ? Other alternatives include disabling the Rust fuzzer for now , or running it without code coverage instrumentation..
curl build broken __EoT__ Has been detected after lading # 838 https : //oss-fuzz-build-logs.storage.googleapis.com/index.html `` ` Step # 4 : Running : /src/curl_fuzzer/corpora/curl_fuzzer/oss-fuzz-gen-6638d89b27ceef13e985ca5a1d9f64cc9d78f579 Step # 4 : Step # 4 : ================================================================= Step # 4 : ==8599==ERROR : LeakSanitizer : detected memory leaks Step # 4 : Step # 4 : Direct leak of 168 byte ( s ) in 1 object ( s ) allocated from : Step # 4 : # 0 0x4ea4e0 in calloc /src/llvm/projects/compiler-rt/lib/asan/asan_malloc_linux.cc:97 Step # 4 : # 1 0x532680 in curl_docalloc /src/curl/lib/memdebug.c:206:9 Step # 4 : # 2 0x63d284 in Curl_ftp_parselist /src/curl/lib/ftplistparser.c:356:27 Step # 4 : # 3 0x59c267 in chop_write /src/curl/lib/sendf.c:585:22 Step # 4 : # 4 0x5c96a8 in readwrite_data /src/curl/lib/transfer.c:796:26 Step # 4 : # 5 0x5c703c in Curl_readwrite /src/curl/lib/transfer.c:1125:14 Step # 4 : # 6 0x540672 in multi_runsingle /src/curl/lib/multi.c:1893:16 Step # 4 : # 7 0x53e5c3 in curl_multi_perform /src/curl/lib/multi.c:2160:14 Step # 4 : # 8 0x5298be in fuzz_handle_transfer ( fuzz_data* ) /src/curl_fuzzer/curl_fuzzer.cc:382:5 Step # 4 : # 9 0x528acb in LLVMFuzzerTestOneInput /src/curl_fuzzer/curl_fuzzer.cc:93:3 Step # 4 : # 10 0xb0e3d1 in fuzzer : :Fuzzer : :ExecuteCallback ( unsigned char const* , unsigned long ) /src/libfuzzer/FuzzerLoop.cpp:517:13 Step # 4 : # 11 0xae547a
PackageInfoService is sometimes throwing uncaught exceptions __EoT__ On certain devices , we appear to be having the following exceptions being thrown : ` java.util.concurrent.ExecutionException : android.content.res.Resources $ NotFoundException : Unable to find resource ID # 0xXXXXXXX ` The most recent report has this exception firing at [ ` getResourcePackageName ` ] ( https : //github.com/google/gapid/blob/master/gapidapk/android/app/src/main/java/com/google/android/gapid/PackageInfoService.java # L409 ) . We should skip over any packages that have resources that can not be accessed .
Refresh device list for tracing __EoT__ I often connect a device after I 've launched GAPIC . I go to take a trace and find there 's no device in the list , and have to restart GAPIC . We should periodically poll for a new device list .
DCE bug with eglCreateImageKHR / glEGLImageTargetTexture2DOES . __EoT__ We 're getting crashes in compat for ` glEGLImageTargetTexture2DOES ` as the ` eglImage ` is nil . Just before this crash a preceding ` eglCreateImageKHR ` asserts with ` assert ( image ! = null ) ` . It appears this only happens with DCE enabled , and is likely a missing dependency between EGLImages and textures . I 'll add a work around for V1.0.0 and try to fix this in the first point release .
Stop adding TRANSFER_DST_BIT to images in MEC . __EoT__ We should do different things depending on the contents of img.Info.Usage This is for single sampled non-depth images . 1 ) If it contains VK_IMAGE_USAGE_TRANSFER_DST_BIT , then prime images how we are 2 ) elif it contains VK_IMAGE_USAGE_COLOR_ATTACHMENT_BIT , Generate a texture + quad + shader + graphics pipeline and prime it by rendering into it 3 ) elif it contains VK_IMAGE_USAGE_STORAGE_BIT , then create a `` compute '' pipeline & copy into the image 4 ) else , there are no valid contents that we can add to the image .
Stop adding TRANSFER_DST_BIT to images in MEC . __EoT__ We should do different things depending on the contents of img.Info.Usage This is for single sampled non-depth images . 1 ) If it contains VK_IMAGE_USAGE_TRANSFER_DST_BIT , then prime images how we are 2 ) elif it contains VK_IMAGE_USAGE_COLOR_ATTACHMENT_BIT , Generate a texture + quad + shader + graphics pipeline and prime it by rendering into it 3 ) elif it contains VK_IMAGE_USAGE_STORAGE_BIT , then create a `` compute '' pipeline & copy into the image 4 ) else , there are no valid contents that we can add to the image .
Add support for VK_EXT_DEBUG_MARKER __EoT__ This will let us assign readable names to Vulkan Objects , as well as group commands in command-buffers .
The gapis server has exited with an error code of : 2 __EoT__ GAPID Version : 1.1.1 : developer OS : linux amd64 App : Helix Jump Method : capture in the middle Error : can not open trace file . Please provide detailed steps that led to the error and copy-paste the stack trace . Extra details from the logs and the trace file would be extra helpful . panic : interface conversion : interface { } is [ ] gles.GLboolean , not [ ] uint8 [ recovered ] panic : Panic at command 472 : glGetBooleanv ( param : GL_STENCIL_TEST , values : 0xc7a9790c ) : interface conversion : interface { } is [ ] gles.GLboolean , not [ ] uint8 [ recovered ] panic : -- - *resolve.ContextListResolvable -- - capture : < ID : < data : '' \\\230vL\373 , \316-0\004n > \002\303 ) -\324\215G\350 '' > > config : < replay_device : < ID : < data : '' \033\236\201 & p^\025\275I\030sH , \202.\320n\027B\245 '' > > > Store ( ) : gapis/database/database.go:50 github.com/google/gapid/gapis/database.Build gapis/resolve/contexts.go:35 github.com/google/gapid/gapis/resolve.Contexts gapis/resolve/resolve.go:331 github.com/google/gapid/gapis/resolve.ResolveInternal gapis/resolve/resolve.go:293 github.com/google/gapid/gapis/resolve.ResolveService gapis/resolve/get.go:33 github.com/google/gapid/gapis/resolve . ( *GetResolvable ) .Resolve gapis/database/memory.go:130 github.com/google/gapid/gapis/database . ( *record ) .resolve gapis/database/memory.go:222
TestSignalTryWait is flakey . __EoT__ RAN : /Users/bclayton/gapid/release/test/test-core-event-task GOT : -- - FAIL : TestSignalTryWait ( 0.00s ) testing.go:37 : 16:47:04.955 E : Error : Got false Expect == true FAIL
Texture with format B10G11R11_UFLOAT_PACK32 showing up black in the UI __EoT__ The texture does have data , it ranges from very small ( 0.001 ) to fairly large ( 90+ ) , but the texture view shows uniformly black . The UI shows `` correct '' values on the text during the hover-over . It might be nice for the user to be able to specify a curve for the texture ( This one would need to be exponential in order for the data to be correctly visualized ) . I am not 100 % sure if this is a bug , or just a byproduct of how we display Float textures .
Panic in get ( ResourceData ) __EoT__ Clicking on some switchThread atoms , or any `` no contexts '' atom ( i.e . egl commands ) causes server panics when the UI requests the resource data for resources that had already been created : `` ` panic : runtime error : invalid memory address or nil pointer dereference [ signal SIGSEGV : segmentation violation code=0x1 addr=0x290 pc=0x10f1495 ] goroutine 26520 [ running ] : github.com/google/gapid/gapis/gfxapi/gles . ( *Program ) .ResourceData ( 0xc449491a80 , 0x338c8c0 , 0xc44d719440 , 0xc4456bbec0 , 0x1e6fa00 , 0xc4582c7720 , 0x0 , 0x0 ) /usr/local/google/work/gapid/src/github.com/google/gapid/gapis/gfxapi/gles/resources.go:268 +0x1f5 github.com/google/gapid/gapis/resolve.buildResources ( 0x338c8c0 , 0xc44e5b7770 , 0xc44fa2d820 , 0x338c8c0 , 0xc44e5b7770 , 0x33628c0 ) /usr/local/google/work/gapid/src/github.com/google/gapid/gapis/resolve/resource_data.go:80 +0x409 github.com/google/gapid/gapis/resolve . ( *AllResourceDataResolvable ) .Resolve ( 0xc472894820 , 0x338c800 , 0xc453e60940 , 0x33628c0 , 0xc472894820 , 0x1 , 0x3367300 ) /usr/local/google/work/gapid/src/github.com/google/gapid/gapis/resolve/resource_data.go:40 +0x6e github.com/google/gapid/gapis/database . ( *record ) .resolve ( 0xc44e5b7710 , 0x338c800 , 0xc453e60940 , 0xc45d6c3500 , 0xc44e5b7200 ) /usr/local/google/work/gapid/src/github.com/google/gapid/gapis/database/memory.go:72 +0x8a github.com/google/gapid/gapis/database . ( *memory ) .resolveLocked.func1 ( 0xc44e5b7710 , 0xc453e60980 , 0xc42000c580 ) /usr/local/google/work/gapid/src/github.com/google/gapid/gapis/database/memory.go:144 +0x45 created by github.com/google/gapid/gapis/database . ( *memory ) .resolveLocked /usr/local/google/work/gapid/src/github.com/google/gapid/gapis/database/memory.go:151 +0x4ff `` `
Depth input attachment image is considered as depth render target __EoT__ In state.go , line 53 , only the image usage bit is checked . Actually we should check the attachment type in the ` LastUsedFramebuffer ` .
Can not find -lkhronos __EoT__ When building i get this error : /usr/lib64/gcc/x86_64-suse-linux/4.8/../../../../x86_64-suse-linux/bin/ld : can not find -lkhronos
BasePipeline is set incorrectly __EoT__ Pipelines created with a base pipeline specified using ` basePipelineHandle ` create ` *PipelineObject ` s with incorrect ` BasePipeline ` . This causes a loop in the ` BasePipeline ` graph , and is the ultimate cause of the infinite loop in # 2390 .
Modified commands disappear from command tree __EoT__ Right click on a command . < img width= '' 664 '' alt= '' screen shot 2017-10-25 at 10 22 41 pm '' src= '' https : //user-images.githubusercontent.com/11505236/32024139-6c3b4952-b9d3-11e7-8a37-c14db302d799.png '' > Modify a value , click OK ... < img width= '' 712 '' alt= '' screen shot 2017-10-25 at 10 23 43 pm '' src= '' https : //user-images.githubusercontent.com/11505236/32024170-7f6598e8-b9d3-11e7-9b3a-7db1d613c510.png '' > And it disappears . Magic . The command is n't landing in another context . Not sure if this is client side or server side . Saving and reopening the capture makes the command re-appear .
Selecting a vulkan shader in GAPIC causes a crash __EoT__ As above , GAPIS crashes when this happens .
Selecting a vulkan shader in GAPIC causes a crash __EoT__ As above , GAPIS crashes when this happens .
Could n't obtain GVR library handle __EoT__ Built on commit e843f4e7 . VR libraries being missing seems to cause the device under test to not connect properly . `` ` Press enter to stop capturing ... 12:57:51.034 I : [ gapidapk.EnsureInstalled ] < gapit > Examining gapid.apk on host ... 12:57:51.064 I : [ gapidapk.EnsureInstalled ] < gapit > Looking at installed packages ... 12:57:53.472 I : [ gapidapk.EnsureInstalled ] < gapit > Found gapid package ... 12:57:55.314 I : < gapit > Adding new device 12:57:55.314 I : < gapit > Device list : 12:57:55.314 I : < gapit > 172.16.1.1:5555 12:57:55.856 I : < gapit > action : android.intent.action.MAIN : com.johnathongoss.libgdxtests/com.johnathongoss.testing.MainActivity 12:57:55.866 I : < gapit > Device is rooted 12:57:55.964 I : [ start ] < gapit > Turning device screen on 12:57:56.065 I : [ start ] < gapit > Checking for lockscreen 12:57:56.166 I : [ start ] < gapit > Checking gapid.apk is installed 12:57:56.166 I : [ gapidapk.EnsureInstalledâ‡’start ] < gapit > Examining gapid.apk on host ... 12:57:56.194 I : [ gapidapk.EnsureInstalledâ‡’start ] < gapit > Looking at installed packages ... 12:57:58.632 I : [ gapidapk.EnsureInstalledâ‡’start ] < gapit > Found gapid package
Vulkan : Depth buffer in Mid-Execution Capture is not handled correctly __EoT__ A full trace has the depth buffer handled correctly , but when mid-execution capture is applied , the depth buffer data looks corrupted .
Re-add observations to gapit commands __EoT__ Context : https : //github.com/google/gapid/pull/352 # discussion_r116492047
Re-add observations to gapit commands __EoT__ Context : https : //github.com/google/gapid/pull/352 # discussion_r116492047
Re-add observations to gapit commands __EoT__ Context : https : //github.com/google/gapid/pull/352 # discussion_r116492047
Re-add observations to gapit commands __EoT__ Context : https : //github.com/google/gapid/pull/352 # discussion_r116492047
Re-add observations to gapit commands __EoT__ Context : https : //github.com/google/gapid/pull/352 # discussion_r116492047
Map : CONIKS map hasher __EoT__ To provide l bits of security in the map we need the following : - A collision and pre-image resistant hash function H of 2l bits . - A tree specific nonce to prevent multi-tree attacks [ 1 ] [ 2 ] - An index specific value for both leafs and empty branches to prevent attacking multiple locations within the same tree [ 1 ] [ 2 ] In particular , this means that the HStar2 algorithm does not provide the full l bits of security in a multi-tree setting , and we need to adjust the tree hasher interface to support supplying location and tree specific values during the computation of leaves and empty branches . [ 1 ] : https : //eprint.iacr.org/2016/683.pdf Section 5.1 [ 2 ] : https : //eprint.iacr.org/2014/1004.pdf Section 3.1 Proposal for MapHasher : `` ` go type MapHasher interface { HashEmpty ( treeNonce int64 , index [ ] byte , height int ) [ ] byte HashLeaf ( treeNonce int64 , index [ ] byte , depth int , leaf [ ] byte ) [ ] byte HashChildren ( l , r [ ] byte ) [ ]
Map : CONIKS map hasher __EoT__ To provide l bits of security in the map we need the following : - A collision and pre-image resistant hash function H of 2l bits . - A tree specific nonce to prevent multi-tree attacks [ 1 ] [ 2 ] - An index specific value for both leafs and empty branches to prevent attacking multiple locations within the same tree [ 1 ] [ 2 ] In particular , this means that the HStar2 algorithm does not provide the full l bits of security in a multi-tree setting , and we need to adjust the tree hasher interface to support supplying location and tree specific values during the computation of leaves and empty branches . [ 1 ] : https : //eprint.iacr.org/2016/683.pdf Section 5.1 [ 2 ] : https : //eprint.iacr.org/2014/1004.pdf Section 3.1 Proposal for MapHasher : `` ` go type MapHasher interface { HashEmpty ( treeNonce int64 , index [ ] byte , height int ) [ ] byte HashLeaf ( treeNonce int64 , index [ ] byte , depth int , leaf [ ] byte ) [ ] byte HashChildren ( l , r [ ] byte ) [ ]
Map : CONIKS map hasher __EoT__ To provide l bits of security in the map we need the following : - A collision and pre-image resistant hash function H of 2l bits . - A tree specific nonce to prevent multi-tree attacks [ 1 ] [ 2 ] - An index specific value for both leafs and empty branches to prevent attacking multiple locations within the same tree [ 1 ] [ 2 ] In particular , this means that the HStar2 algorithm does not provide the full l bits of security in a multi-tree setting , and we need to adjust the tree hasher interface to support supplying location and tree specific values during the computation of leaves and empty branches . [ 1 ] : https : //eprint.iacr.org/2016/683.pdf Section 5.1 [ 2 ] : https : //eprint.iacr.org/2014/1004.pdf Section 3.1 Proposal for MapHasher : `` ` go type MapHasher interface { HashEmpty ( treeNonce int64 , index [ ] byte , height int ) [ ] byte HashLeaf ( treeNonce int64 , index [ ] byte , depth int , leaf [ ] byte ) [ ] byte HashChildren ( l , r [ ] byte ) [ ]
Map : CONIKS map hasher __EoT__ To provide l bits of security in the map we need the following : - A collision and pre-image resistant hash function H of 2l bits . - A tree specific nonce to prevent multi-tree attacks [ 1 ] [ 2 ] - An index specific value for both leafs and empty branches to prevent attacking multiple locations within the same tree [ 1 ] [ 2 ] In particular , this means that the HStar2 algorithm does not provide the full l bits of security in a multi-tree setting , and we need to adjust the tree hasher interface to support supplying location and tree specific values during the computation of leaves and empty branches . [ 1 ] : https : //eprint.iacr.org/2016/683.pdf Section 5.1 [ 2 ] : https : //eprint.iacr.org/2014/1004.pdf Section 3.1 Proposal for MapHasher : `` ` go type MapHasher interface { HashEmpty ( treeNonce int64 , index [ ] byte , height int ) [ ] byte HashLeaf ( treeNonce int64 , index [ ] byte , depth int , leaf [ ] byte ) [ ] byte HashChildren ( l , r [ ] byte ) [ ]
Map : CONIKS map hasher __EoT__ To provide l bits of security in the map we need the following : - A collision and pre-image resistant hash function H of 2l bits . - A tree specific nonce to prevent multi-tree attacks [ 1 ] [ 2 ] - An index specific value for both leafs and empty branches to prevent attacking multiple locations within the same tree [ 1 ] [ 2 ] In particular , this means that the HStar2 algorithm does not provide the full l bits of security in a multi-tree setting , and we need to adjust the tree hasher interface to support supplying location and tree specific values during the computation of leaves and empty branches . [ 1 ] : https : //eprint.iacr.org/2016/683.pdf Section 5.1 [ 2 ] : https : //eprint.iacr.org/2014/1004.pdf Section 3.1 Proposal for MapHasher : `` ` go type MapHasher interface { HashEmpty ( treeNonce int64 , index [ ] byte , height int ) [ ] byte HashLeaf ( treeNonce int64 , index [ ] byte , depth int , leaf [ ] byte ) [ ] byte HashChildren ( l , r [ ] byte ) [ ]
Logs can not be safely frozen at the moment __EoT__ Setting the frozen state blocks both queuing and signing leaving anything queued unable to be integrated . The proposed solution is : * Expose metrics on integration rate / errors to allow inspection that queue is empty for a tree * Add a new tree state that will allow signing but block queueing * Various new tests and stuff that it works correctly Will need a minor MySQL schema change to the tree states ENUM but this does not need to be done immediately .
Logs can not be safely frozen at the moment __EoT__ Setting the frozen state blocks both queuing and signing leaving anything queued unable to be integrated . The proposed solution is : * Expose metrics on integration rate / errors to allow inspection that queue is empty for a tree * Add a new tree state that will allow signing but block queueing * Various new tests and stuff that it works correctly Will need a minor MySQL schema change to the tree states ENUM but this does not need to be done immediately .
Rate limiting for logs __EoT__ The log server must implement rate limiting and prevent a large queue backlog building up . Otherwise , sequencing could fail to catch up with entries before the MMD expires . Edit by @ codingllama : s/admission control/rate limiting/ Note that this is global-per-log rate limiting , per-tenant rate-limiting is separate ( see XXX )
Support proofs at arbitrary tree sizes __EoT__ Currently we can only obtain proofs where the tree size is at an internal STH but it should be possible to return proofs at arbitrary sizes as in existing implementations . This requires dynamic rehashing of some proof nodes and is complex to implement . Research indicates it 's feasible to unroll the rehashing chain so storage can fetch all the involved nodes and the rehashing can be done by post-processing . Changes involved ( plus tests of course ) : - NodeReader interface must be able to optionally request tree revisions for sizes > = specified size - Merkle path code must handle rehashing and annotate node fetches that are part of a rehash - Proof fetching + rehashing should be broken out of the server to allow separate testing and reuse - After the storage fetch the rehashing operations must be carried out using the annotations - API should include a flag to allow rehashing so we can test it without impacting current users
Implement consistency proofs in the backend __EoT__ Fetching correct node sets and doing computations etc . Again only at tree sizes where we have an STH .
Allow servers to bind at user-specified addresses , not just ports __EoT__
Map : Sign SignedMapHead __EoT__ The trillian map server does not currently sign it 's tree heads .
Cloud Spanner support __EoT__ My understanding is that Google runs Trillian internally on top of Spanner . Would it make sense for Trillian to support Cloud Spanner for use on GCP ?
Resend original SCT if a client submits a certificate that already exists in the log __EoT__ Currently , if a client sends the same certificate twice , on the second time , it will fail with ` AddChain handler error : backend QueueLeaves request failed : rpc error : code = AlreadyExists desc = Leaf hash already exists ` due to https : //github.com/google/trillian/blob/master/examples/ct/handlers.go # L308 . To comply with the RFC , any valid submission must yield an SCT .
Resend original SCT if a client submits a certificate that already exists in the log __EoT__ Currently , if a client sends the same certificate twice , on the second time , it will fail with ` AddChain handler error : backend QueueLeaves request failed : rpc error : code = AlreadyExists desc = Leaf hash already exists ` due to https : //github.com/google/trillian/blob/master/examples/ct/handlers.go # L308 . To comply with the RFC , any valid submission must yield an SCT .
Resend original SCT if a client submits a certificate that already exists in the log __EoT__ Currently , if a client sends the same certificate twice , on the second time , it will fail with ` AddChain handler error : backend QueueLeaves request failed : rpc error : code = AlreadyExists desc = Leaf hash already exists ` due to https : //github.com/google/trillian/blob/master/examples/ct/handlers.go # L308 . To comply with the RFC , any valid submission must yield an SCT .
Resend original SCT if a client submits a certificate that already exists in the log __EoT__ Currently , if a client sends the same certificate twice , on the second time , it will fail with ` AddChain handler error : backend QueueLeaves request failed : rpc error : code = AlreadyExists desc = Leaf hash already exists ` due to https : //github.com/google/trillian/blob/master/examples/ct/handlers.go # L308 . To comply with the RFC , any valid submission must yield an SCT .
Resend original SCT if a client submits a certificate that already exists in the log __EoT__ Currently , if a client sends the same certificate twice , on the second time , it will fail with ` AddChain handler error : backend QueueLeaves request failed : rpc error : code = AlreadyExists desc = Leaf hash already exists ` due to https : //github.com/google/trillian/blob/master/examples/ct/handlers.go # L308 . To comply with the RFC , any valid submission must yield an SCT .
Resend original SCT if a client submits a certificate that already exists in the log __EoT__ Currently , if a client sends the same certificate twice , on the second time , it will fail with ` AddChain handler error : backend QueueLeaves request failed : rpc error : code = AlreadyExists desc = Leaf hash already exists ` due to https : //github.com/google/trillian/blob/master/examples/ct/handlers.go # L308 . To comply with the RFC , any valid submission must yield an SCT .
Resend original SCT if a client submits a certificate that already exists in the log __EoT__ Currently , if a client sends the same certificate twice , on the second time , it will fail with ` AddChain handler error : backend QueueLeaves request failed : rpc error : code = AlreadyExists desc = Leaf hash already exists ` due to https : //github.com/google/trillian/blob/master/examples/ct/handlers.go # L308 . To comply with the RFC , any valid submission must yield an SCT .
Resend original SCT if a client submits a certificate that already exists in the log __EoT__ Currently , if a client sends the same certificate twice , on the second time , it will fail with ` AddChain handler error : backend QueueLeaves request failed : rpc error : code = AlreadyExists desc = Leaf hash already exists ` due to https : //github.com/google/trillian/blob/master/examples/ct/handlers.go # L308 . To comply with the RFC , any valid submission must yield an SCT .
flatc JSON serialisation of sorted tables __EoT__ Hi , I think that the current implementation of Parser : :ParseVector serializes tables with key in the order of the JSON file and do n't sorts the element by their keys . I clearly do n't know enough of the internals to know if I 'm right , but it looks like it just PushElement offset to tables in the order of parsing , and do n't arrange the entries before serializing the offsets . Bye , JB .
VS2015 warning `` conditional expression is constant '' __EoT__ Visual Studio 2015 Revision : 640b525e830f84060f0d43923e4fffdb40d5676a Message : optimization of FlatBufferBuilder : :CreateVector ( ) ( # 4198 ) After this commit an annoying warning appears with the code fragment `` if ( sizeof ( T ) == 1 ) { `` flatbuffers\include\flatbuffers/flatbuffers.h ( 1060 ) : warning C4127 : conditional expression is constant
FLATBUFFERS_COPTS does n't work for MSVC __EoT__ The FLATBUFFERS_COPTS in BUILD file does n't work in Windows MSVC environment .
BufferRef is quite broken [ C++ , grpc ] __EoT__ I have been playing around a bit with gRPC and Flatbuffers lately . In doing so , I have found some issues with the ` BufferRef < T > ` type , varying from nitpicky to severe fundamental flaws . 1 . ` BufferRef : :Verify ( ) ` is not a const method . Fixing is easy : Just add the ` const ` keyword . 2 . The docs for ` BufferRef ` claims that it does n't own its buffer , but it still sometimes does ( it calls ` free ` from the destructor sometimes ) . A fix could be to correct the documentation . 3 . ` BufferRef ` sometimes calls ` free ` from its destructor , but it does n't delete or override the copy constructor . This means that if you have an owning ( ` must_free == true ` ) ` BufferRef ` and pass it by value to a function or in some other way copy it , you have a double free memory corruption/crash . This makes that type very difficult to use correctly . A fix for
Flexbuffer or bytearray support for python [ python 3.x , flatc 1.7.1 , Ubuntu 16.04 LTS , flatbuffers 2015.5.14 ] __EoT__ - python 3.X - flatc version 1.7.1 ( Sep 27 2017 ) - Ubuntu 16.04.2 LTS - flatbuffers serialization format for Python : INSTALLED : 2015.5.14.0 I would like to store a python bytearray in a flatbuffer vector of type byte _without_ having to iterate over the incoming bytearray invoking ` builder.PrependByte ` , e.g . : for j in reversed ( range ( 0 , len ( buf ) ) ) : builder.PrependByte ( buf [ j ] ) Is this possible via Flexbuffers _or other_ given the current implementation ( s ) ? Otherwise , for serialization performance reasons the above prevents usage within my use case . Many thanks in advance .
Enable verification by default for gRPC __EoT__ ( This issue is a continuation of a discussion in # 4310 ) Flatbuffers does not verify its input by default , not in gRPC and not when not using gRPC . I think Flatbuffers is a great project . It 's a great serialization format : it 's very efficient and it is useful in a wide variety of situations . In the non-RPC use case I think it makes sense to not do verification unless explicitly asked for , because in many situations a program will have just created the buffer and then it would be nonsensical to verify it . However , for gRPC and networking I think the situation is different . @ aardappel says in # 4310 that : * `` While typically you 'd want to use [ verification ] for untrusted network traffic , it should still be a conscious decision to use it , '' * `` and what you 'd want to do if it fails will differ by the use-case . '' * `` People also use GRPC between trusted servers in the same internal network / data center , and may not
Enable verification by default for gRPC __EoT__ ( This issue is a continuation of a discussion in # 4310 ) Flatbuffers does not verify its input by default , not in gRPC and not when not using gRPC . I think Flatbuffers is a great project . It 's a great serialization format : it 's very efficient and it is useful in a wide variety of situations . In the non-RPC use case I think it makes sense to not do verification unless explicitly asked for , because in many situations a program will have just created the buffer and then it would be nonsensical to verify it . However , for gRPC and networking I think the situation is different . @ aardappel says in # 4310 that : * `` While typically you 'd want to use [ verification ] for untrusted network traffic , it should still be a conscious decision to use it , '' * `` and what you 'd want to do if it fails will differ by the use-case . '' * `` People also use GRPC between trusted servers in the same internal network / data center , and may not
java : flatc-generated code has compilation errors in multi-module/multi-namespace case __EoT__ I have two fbs files having different namespaces . One of the fbs files includes the other and they are compiled with a single flatc pass . One of the table types contains a child from another namespace . This results in javac compilation errors because return type of several generated methods is not namespace-qualified and there is no corresponding import statement . Ex : `` ` public Key keyByKey ( int key ) { int o = __offset ( 6 ) ; return o ! = 0 ? Key.__lookup_by_key ( __vector ( o ) , key , bb ) : null ; } `` ` Key is a type defined in this other namespace but it 's not qualified . Interestingly , some other generated methods in the same class do have this Key type fully qualified . Another problematic generated method is `` ` public NVPair metadataByKey ( String key ) { int o = __offset ( 8 ) ; return o ! = 0 ? NVPair.__lookup_by_key ( __vector ( o ) , key , bb ) : null ; } `` ` It looks like both
Allocator Improvements __EoT__ The flatbuffers allocator currently has a few limitations/bugs : - The allocator must be copy-constructible , because the deleter in ` vector_downward : :release ( ) ` copies the allocator into the bind . This means we ca n't have a stateful allocator where the allocator manages its own memory regions . I hit this in # 4310 , and while I was debugging I found some other related issues so filing here separately . - The deleter currently does not support subclasses ( ! ) , because it will call ` simple_allocator : :deallocate ` regardless of the type of the allocator . - If we switch the bind to use a pointer to the allocator , then it will use the correct ( derived ) ` deallocate ` function , fixing the above point . However , the bind then refers to an allocator whose lifetime is not managed by the ` unique_ptr_t ` . If the ` unique_ptr_t ` outlives the allocator , it will point to an invalid allocator and could segfault . My proposal is this : - Templatize ` vector_downward < Allocator > ` , making it look more like the
Allocator Improvements __EoT__ The flatbuffers allocator currently has a few limitations/bugs : - The allocator must be copy-constructible , because the deleter in ` vector_downward : :release ( ) ` copies the allocator into the bind . This means we ca n't have a stateful allocator where the allocator manages its own memory regions . I hit this in # 4310 , and while I was debugging I found some other related issues so filing here separately . - The deleter currently does not support subclasses ( ! ) , because it will call ` simple_allocator : :deallocate ` regardless of the type of the allocator . - If we switch the bind to use a pointer to the allocator , then it will use the correct ( derived ) ` deallocate ` function , fixing the above point . However , the bind then refers to an allocator whose lifetime is not managed by the ` unique_ptr_t ` . If the ` unique_ptr_t ` outlives the allocator , it will point to an invalid allocator and could segfault . My proposal is this : - Templatize ` vector_downward < Allocator > ` , making it look more like the
Allocator Improvements __EoT__ The flatbuffers allocator currently has a few limitations/bugs : - The allocator must be copy-constructible , because the deleter in ` vector_downward : :release ( ) ` copies the allocator into the bind . This means we ca n't have a stateful allocator where the allocator manages its own memory regions . I hit this in # 4310 , and while I was debugging I found some other related issues so filing here separately . - The deleter currently does not support subclasses ( ! ) , because it will call ` simple_allocator : :deallocate ` regardless of the type of the allocator . - If we switch the bind to use a pointer to the allocator , then it will use the correct ( derived ) ` deallocate ` function , fixing the above point . However , the bind then refers to an allocator whose lifetime is not managed by the ` unique_ptr_t ` . If the ` unique_ptr_t ` outlives the allocator , it will point to an invalid allocator and could segfault . My proposal is this : - Templatize ` vector_downward < Allocator > ` , making it look more like the
Allocator Improvements __EoT__ The flatbuffers allocator currently has a few limitations/bugs : - The allocator must be copy-constructible , because the deleter in ` vector_downward : :release ( ) ` copies the allocator into the bind . This means we ca n't have a stateful allocator where the allocator manages its own memory regions . I hit this in # 4310 , and while I was debugging I found some other related issues so filing here separately . - The deleter currently does not support subclasses ( ! ) , because it will call ` simple_allocator : :deallocate ` regardless of the type of the allocator . - If we switch the bind to use a pointer to the allocator , then it will use the correct ( derived ) ` deallocate ` function , fixing the above point . However , the bind then refers to an allocator whose lifetime is not managed by the ` unique_ptr_t ` . If the ` unique_ptr_t ` outlives the allocator , it will point to an invalid allocator and could segfault . My proposal is this : - Templatize ` vector_downward < Allocator > ` , making it look more like the
Dart support __EoT__ I 'm wondering if there are plans for supporting Dart too ( especially after the JS serialization is done , I imagine it would n't be that much different to do the codegen ) .
Dart support __EoT__ I 'm wondering if there are plans for supporting Dart too ( especially after the JS serialization is done , I imagine it would n't be that much different to do the codegen ) .
Dart support __EoT__ I 'm wondering if there are plans for supporting Dart too ( especially after the JS serialization is done , I imagine it would n't be that much different to do the codegen ) .
DetachedBuffer can not be rewrapped [ C++ , clang 6.0 , OSX 10.11.6 , master ] __EoT__ *affected language* : cpp *compiler version* : clang 6.0 *operating system version* : macOs 10.11.6 *flatBuffers version* : current ` DetachedBuffer ` automatically manages the memory , which is good . However , it is kind of hard ( rather impossible ) to permanently release the memory from the ` DetachedBuffer ` . I would like a functionality to get the pointers ` _cur ` and the size ` _reserved ` out from the detached buffer such that I can **rewrap** this memory somewhere else ( without having to use type from the library ) . I have given FlatBufferBuilder a proxy object which forwards to my special allocator . Something along the line : `` ` cpp AllocatorProxyFlatBuffer allocator ( mySpecialAllocator ) ; flatbuffers : :FlatBufferBuilder builder ( 1024 , & allocator ) ; ... . auto detachedBuffer = builder.Release ( ) ; uint8_t data = detachedBuffer.data ( ) // pointer to the start of the data . std : size_t size = detachedBuffer.size ( ) // size of the data . std : :pair < uint8_t* , std : :size_t >
-- ts generate invalid code if namespace is n't declared __EoT__ Given JS / TS modules provide name-spacing using namespaces is kind of redundant . But then if you generate typescript code for file without namespace generate looks like : `` ` ts export namespace { export class ChangeProperty { // ... } } `` ` which is invalid typescript code given that namespace name is omitted .
-- ts generate invalid code if namespace is n't declared __EoT__ Given JS / TS modules provide name-spacing using namespaces is kind of redundant . But then if you generate typescript code for file without namespace generate looks like : `` ` ts export namespace { export class ChangeProperty { // ... } } `` ` which is invalid typescript code given that namespace name is omitted .
-- ts generate invalid code if namespace is n't declared __EoT__ Given JS / TS modules provide name-spacing using namespaces is kind of redundant . But then if you generate typescript code for file without namespace generate looks like : `` ` ts export namespace { export class ChangeProperty { // ... } } `` ` which is invalid typescript code given that namespace name is omitted .
-- ts generate invalid code if namespace is n't declared __EoT__ Given JS / TS modules provide name-spacing using namespaces is kind of redundant . But then if you generate typescript code for file without namespace generate looks like : `` ` ts export namespace { export class ChangeProperty { // ... } } `` ` which is invalid typescript code given that namespace name is omitted .
å…³äºŽflatbuffers typescript __EoT__ é¦–å…ˆæ„Ÿè°¢å®˜æ–¹æ”¯æŒäº†tpyescript ï¼ŒçŽ°åœ¨å¤§åž‹çš„javascrit é¡¹ç›®åŸºæœ¬éƒ½æ˜¯ç”¨typescript å†™çš„ï¼Œä½†æ˜¯å®˜æ–¹åªæä¾›äº†flatbuffers.js çš„ç‰ˆæœ¬ æ²¡æœ‰æä¾› ts çš„ç‰ˆæœ¬ã€‚å¸Œæœ›èƒ½å¤Ÿæä¾›ä¸€ä¸‹ï¼Œè¿™æ ·åœ¨TP å†™çš„é¡¹ç›® å°±ä¸ä¼š çœ‹åˆ°çº¢è‰²çš„é”™è¯¯äº†ã€‚
å…³äºŽflatbuffers typescript __EoT__ é¦–å…ˆæ„Ÿè°¢å®˜æ–¹æ”¯æŒäº†tpyescript ï¼ŒçŽ°åœ¨å¤§åž‹çš„javascrit é¡¹ç›®åŸºæœ¬éƒ½æ˜¯ç”¨typescript å†™çš„ï¼Œä½†æ˜¯å®˜æ–¹åªæä¾›äº†flatbuffers.js çš„ç‰ˆæœ¬ æ²¡æœ‰æä¾› ts çš„ç‰ˆæœ¬ã€‚å¸Œæœ›èƒ½å¤Ÿæä¾›ä¸€ä¸‹ï¼Œè¿™æ ·åœ¨TP å†™çš„é¡¹ç›® å°±ä¸ä¼š çœ‹åˆ°çº¢è‰²çš„é”™è¯¯äº†ã€‚
Not Safe for Multiple Readers __EoT__ It seems flat buffers are not safe to read from multiple reader threads at the same time . This is because of methods line this one : `` ` protected String __string ( int offset ) { offset += bb.getInt ( offset ) ; if ( bb.hasArray ( ) ) { return new String ( bb.array ( ) , offset + SIZEOF_INT , bb.getInt ( offset ) , FlatBufferBuilder.utf8charset ) ; } else { // We ca n't access .array ( ) , since the ByteBuffer is read-only . // We 're forced to make an extra copy : byte [ ] copy = new byte [ bb.getInt ( offset ) ] ; int old_pos = bb.position ( ) ; bb.position ( offset + SIZEOF_INT ) ; bb.get ( copy ) ; bb.position ( old_pos ) ; return new String ( copy , 0 , copy.length , FlatBufferBuilder.utf8charset ) ; } } `` ` The problem is that in-between `` ` int old_pos = bb.position ( ) ; `` ` and `` ` bb.position ( old_pos ) ; `` ` The reader has modified the buffer that is globally visible to all other threads
Custom Allocator not used for ReleaseBufferPointer __EoT__ Opening this issue since # 351 was prematurely closed . An active PR addressing this issue is # 4117 .
Compilation with VS 2013 fails __EoT__ flatbuffers.h is missing ` # include < algorithm > `
[ help needed ] JSON 2D arrays and JSON lists schema __EoT__ Hi , I wonder if it 's possible to represent the below JSON as a scheme in Flatbuffers : ( It 's output from InfluxDB ; I was using boost : :property_tree ( spirit ) to parse it , but it was too slow at about 15k rows/core/sec ) - The `` result '' array can contain 0+ `` results '' - A `` result '' can contain 0+ `` series '' - A `` series '' can contain a `` name '' string and a `` columns '' string-array So far I was able to create a scheme . But how to handle : - `` tags '' list with variable key-strings ? ( host/type/type_instance , ... ) - `` values '' array of arrays of unions ( strings/floats/integers/bools ) `` ` { `` results '' : [ { `` series '' : [ { `` name '' : `` chrony_value '' , `` tags '' : { `` host '' : `` apar.example.com '' , `` type '' : `` clock_skew_ppm '' , `` type_instance '' : `` chrony '' } , `` columns '' :
No FlatbuffersConfigVersion.cmake file __EoT__ When I build flatbuffers ( version 1.8.0 ) with Debian generator , the output ` .deb ` package does not contain ` FlatbuffersConfigVersion.cmake ` file . This makes it impossible to check against library version via ` find_package ` command like so : ` find_package ( Flatbuffers 1.8 REQUIRED ) ` .
Failed build with Arduino STL [ Arduino 1.6+ , C++ , avr/arm-none-eabi-gcc 4.8 , master ] __EoT__ Hello , First of all thank you for the great job done with flatbuffers . I currently started using flatbuffers on the Arduino platform in order to serialize sensor data logging for a more efficient communication with a Host CPU . I started using it with [ StandardCplusplus ] ( https : //github.com/maniacbug/StandardCplusplus ) library which worked out of the box . However this library is deprecated and its development seems to be dead , code being a bit more up-to-date in isolated forks . The current Arduino STL support is implemented by the library [ ArduinoSTL ] ( https : //github.com/mike-matera/ArduinoSTL ) , which is maintained and included in the Arduino library tree accessible from their IDE . This latter lib includes ` cstdint ` and ` utility ` together with a more up-to-date port of [ uClib++ ] ( http : //git.uclibc.org/uClibc++ ) . It so makes unnecessary the conditional fixes brought in by # 4197 and now available in the latest version 1.7.0 under [ include/base.h ] ( https : //github.com/google/flatbuffers/blob/master/include/flatbuffers/base.h ) . I fixed this issue and use flatbuffers
Java : public default constructor is error-prone __EoT__ Hi ! I am currently using flatbuffers to both hold default values and allow to sync user themes ( for an Android Wear Watch Face ) between the handset and the wearables . I always need a ` Theme ` object ( which is a flatbuffers ` Table ` ) available to get the default values if the user has n't set any theme . How to create an empty object is not clear . From the code ( and I tried to confirm ) , creating a new object initially ( ` new Theme ( ) ` ) is possible , and creates one with a ` null `` ByteBuffer ` , leading to a ` NullPointerException ` as soon as you call an accessor . `` ` java private final Theme mTheme = new Theme ( ) ; // Should work but leads to NPEs on accesses . `` ` I tried calling this : `` ` java private final Theme mTheme = Theme.getRootAsTheme ( ByteBuffer.allocate ( 0 ) ) ; // Does n't work `` ` But got an ` IndexOutOfBoundsException ` when accessing a property . Finally ,
Java : public default constructor is error-prone __EoT__ Hi ! I am currently using flatbuffers to both hold default values and allow to sync user themes ( for an Android Wear Watch Face ) between the handset and the wearables . I always need a ` Theme ` object ( which is a flatbuffers ` Table ` ) available to get the default values if the user has n't set any theme . How to create an empty object is not clear . From the code ( and I tried to confirm ) , creating a new object initially ( ` new Theme ( ) ` ) is possible , and creates one with a ` null `` ByteBuffer ` , leading to a ` NullPointerException ` as soon as you call an accessor . `` ` java private final Theme mTheme = new Theme ( ) ; // Should work but leads to NPEs on accesses . `` ` I tried calling this : `` ` java private final Theme mTheme = Theme.getRootAsTheme ( ByteBuffer.allocate ( 0 ) ) ; // Does n't work `` ` But got an ` IndexOutOfBoundsException ` when accessing a property . Finally ,
Call flatc with -b should ignore $ schema in input json __EoT__ I call flatc with option -b ` flatc -b schema.fbs config.json ` I edit the json file using Visual studio using the json schema compiled with -- jsonschema . Visual studio is only able to do code completion when there is ` `` $ schema '' : `` schema.schema.json '' ` at the top of the json file . When I try to convert the json file into the binary form I get an error : ` error : unknown field : $ schema ` The field $ schema should be ignored .
make fails osx 10.9.5 __EoT__ ran ` cmake -G `` Unix Makefiles '' ` then ` make ` `` ` ... /tests/test.cpp:40:14 : error : comparison of integers of different signs : 'unsigned long ' and 'int ' [ -Werror , -Wsign-compare ] if ( expval ! = val ) { ... /tests/test.cpp:140:3 : note : in instantiation of function template specialization 'TestEq < unsigned long , int > ' requested here TEST_EQ ( VectorLength ( inventory ) , 10 ) ; // Works even if inventory is null . `` `
Name conflicts for `` vector length '' and `` union type '' generated functions __EoT__ Given a schema like this : `` ` namespace Foo ; table A { BLength : int ; B : [ int ] ; } root_type A ; `` ` Generated function names in Java include : `` ` public int BLength ( ) { int o = __offset ( 4 ) ; return o ! = 0 ? bb.getInt ( o + bb_pos ) : 0 ; } public int BLength ( ) { int o = __offset ( 6 ) ; return o ! = 0 ? __vector_len ( o ) : 0 ; } `` ` The first function is for fetching the value of the key `` BLength '' , and the second is for fetching the length of the vector `` B '' . These names conflict and thus are invalid Java code . Given a schema like this : `` ` namespace Foo ; table T { } union U { T } table A { BType : int ; B : U ; } root_type A ; `` ` Generated function names in Java include : `` `
Incorrect rust generated code for schemas with underscore ( rust , gcc 7.3 , ubuntu 18.04 , master ) __EoT__ Hi , As of commit 615885e889c92306f16b963fc4f88d1a447debf4 , ` flatc -- rust ` generates invalid module names for schema namespaces containing an underscore character . Here is the minimal code to reproduce this bug : `` ` // foobar.fbs namespace foo_bar ; enum FooBar : byte { Foo = 0 , Bar } `` ` `` ` rust // foo_bar_generated.rs ... pub mod foo__bar { ... } // pub mod foo_bar `` ` The generated module name should be ` foo_bar ` as opposed to ` foo__bar ` .
compare_bench fails if Complexity was computed __EoT__ When the benchmarks are are run with ` Complexity ( ) ` enabled , the output JSON file contains additional members that cause the ` compare_bench.py ` script to fail : > Invalid input file : 'out.json ' does not name a valid benchmark executable or JSON file The offending lines in the JSON look like : { `` name '' : `` ComputationBenchmarks_BigO '' , `` cpu_coefficient '' : 672 , `` real_coefficient '' : 672 , `` big_o '' : `` N '' , `` time_unit '' : `` ns '' } , { `` name '' : `` ComputationBenchmarks_RMS '' , `` rms '' : 2 % } Removing them manually works fine , so presumably the script should just ignore complexity-related entries in the JSON .
compare_bench fails if Complexity was computed __EoT__ When the benchmarks are are run with ` Complexity ( ) ` enabled , the output JSON file contains additional members that cause the ` compare_bench.py ` script to fail : > Invalid input file : 'out.json ' does not name a valid benchmark executable or JSON file The offending lines in the JSON look like : { `` name '' : `` ComputationBenchmarks_BigO '' , `` cpu_coefficient '' : 672 , `` real_coefficient '' : 672 , `` big_o '' : `` N '' , `` time_unit '' : `` ns '' } , { `` name '' : `` ComputationBenchmarks_RMS '' , `` rms '' : 2 % } Removing them manually works fine , so presumably the script should just ignore complexity-related entries in the JSON .
compare_bench fails if Complexity was computed __EoT__ When the benchmarks are are run with ` Complexity ( ) ` enabled , the output JSON file contains additional members that cause the ` compare_bench.py ` script to fail : > Invalid input file : 'out.json ' does not name a valid benchmark executable or JSON file The offending lines in the JSON look like : { `` name '' : `` ComputationBenchmarks_BigO '' , `` cpu_coefficient '' : 672 , `` real_coefficient '' : 672 , `` big_o '' : `` N '' , `` time_unit '' : `` ns '' } , { `` name '' : `` ComputationBenchmarks_RMS '' , `` rms '' : 2 % } Removing them manually works fine , so presumably the script should just ignore complexity-related entries in the JSON .
compare_bench fails if Complexity was computed __EoT__ When the benchmarks are are run with ` Complexity ( ) ` enabled , the output JSON file contains additional members that cause the ` compare_bench.py ` script to fail : > Invalid input file : 'out.json ' does not name a valid benchmark executable or JSON file The offending lines in the JSON look like : { `` name '' : `` ComputationBenchmarks_BigO '' , `` cpu_coefficient '' : 672 , `` real_coefficient '' : 672 , `` big_o '' : `` N '' , `` time_unit '' : `` ns '' } , { `` name '' : `` ComputationBenchmarks_RMS '' , `` rms '' : 2 % } Removing them manually works fine , so presumably the script should just ignore complexity-related entries in the JSON .
compare_bench fails if Complexity was computed __EoT__ When the benchmarks are are run with ` Complexity ( ) ` enabled , the output JSON file contains additional members that cause the ` compare_bench.py ` script to fail : > Invalid input file : 'out.json ' does not name a valid benchmark executable or JSON file The offending lines in the JSON look like : { `` name '' : `` ComputationBenchmarks_BigO '' , `` cpu_coefficient '' : 672 , `` real_coefficient '' : 672 , `` big_o '' : `` N '' , `` time_unit '' : `` ns '' } , { `` name '' : `` ComputationBenchmarks_RMS '' , `` rms '' : 2 % } Removing them manually works fine , so presumably the script should just ignore complexity-related entries in the JSON .
compare_bench fails if Complexity was computed __EoT__ When the benchmarks are are run with ` Complexity ( ) ` enabled , the output JSON file contains additional members that cause the ` compare_bench.py ` script to fail : > Invalid input file : 'out.json ' does not name a valid benchmark executable or JSON file The offending lines in the JSON look like : { `` name '' : `` ComputationBenchmarks_BigO '' , `` cpu_coefficient '' : 672 , `` real_coefficient '' : 672 , `` big_o '' : `` N '' , `` time_unit '' : `` ns '' } , { `` name '' : `` ComputationBenchmarks_RMS '' , `` rms '' : 2 % } Removing them manually works fine , so presumably the script should just ignore complexity-related entries in the JSON .
compare_bench fails if Complexity was computed __EoT__ When the benchmarks are are run with ` Complexity ( ) ` enabled , the output JSON file contains additional members that cause the ` compare_bench.py ` script to fail : > Invalid input file : 'out.json ' does not name a valid benchmark executable or JSON file The offending lines in the JSON look like : { `` name '' : `` ComputationBenchmarks_BigO '' , `` cpu_coefficient '' : 672 , `` real_coefficient '' : 672 , `` big_o '' : `` N '' , `` time_unit '' : `` ns '' } , { `` name '' : `` ComputationBenchmarks_RMS '' , `` rms '' : 2 % } Removing them manually works fine , so presumably the script should just ignore complexity-related entries in the JSON .
compare_bench fails if Complexity was computed __EoT__ When the benchmarks are are run with ` Complexity ( ) ` enabled , the output JSON file contains additional members that cause the ` compare_bench.py ` script to fail : > Invalid input file : 'out.json ' does not name a valid benchmark executable or JSON file The offending lines in the JSON look like : { `` name '' : `` ComputationBenchmarks_BigO '' , `` cpu_coefficient '' : 672 , `` real_coefficient '' : 672 , `` big_o '' : `` N '' , `` time_unit '' : `` ns '' } , { `` name '' : `` ComputationBenchmarks_RMS '' , `` rms '' : 2 % } Removing them manually works fine , so presumably the script should just ignore complexity-related entries in the JSON .
compare_bench fails if Complexity was computed __EoT__ When the benchmarks are are run with ` Complexity ( ) ` enabled , the output JSON file contains additional members that cause the ` compare_bench.py ` script to fail : > Invalid input file : 'out.json ' does not name a valid benchmark executable or JSON file The offending lines in the JSON look like : { `` name '' : `` ComputationBenchmarks_BigO '' , `` cpu_coefficient '' : 672 , `` real_coefficient '' : 672 , `` big_o '' : `` N '' , `` time_unit '' : `` ns '' } , { `` name '' : `` ComputationBenchmarks_RMS '' , `` rms '' : 2 % } Removing them manually works fine , so presumably the script should just ignore complexity-related entries in the JSON .
compare_bench fails if Complexity was computed __EoT__ When the benchmarks are are run with ` Complexity ( ) ` enabled , the output JSON file contains additional members that cause the ` compare_bench.py ` script to fail : > Invalid input file : 'out.json ' does not name a valid benchmark executable or JSON file The offending lines in the JSON look like : { `` name '' : `` ComputationBenchmarks_BigO '' , `` cpu_coefficient '' : 672 , `` real_coefficient '' : 672 , `` big_o '' : `` N '' , `` time_unit '' : `` ns '' } , { `` name '' : `` ComputationBenchmarks_RMS '' , `` rms '' : 2 % } Removing them manually works fine , so presumably the script should just ignore complexity-related entries in the JSON .
compare_bench fails if Complexity was computed __EoT__ When the benchmarks are are run with ` Complexity ( ) ` enabled , the output JSON file contains additional members that cause the ` compare_bench.py ` script to fail : > Invalid input file : 'out.json ' does not name a valid benchmark executable or JSON file The offending lines in the JSON look like : { `` name '' : `` ComputationBenchmarks_BigO '' , `` cpu_coefficient '' : 672 , `` real_coefficient '' : 672 , `` big_o '' : `` N '' , `` time_unit '' : `` ns '' } , { `` name '' : `` ComputationBenchmarks_RMS '' , `` rms '' : 2 % } Removing them manually works fine , so presumably the script should just ignore complexity-related entries in the JSON .
compare_bench fails if Complexity was computed __EoT__ When the benchmarks are are run with ` Complexity ( ) ` enabled , the output JSON file contains additional members that cause the ` compare_bench.py ` script to fail : > Invalid input file : 'out.json ' does not name a valid benchmark executable or JSON file The offending lines in the JSON look like : { `` name '' : `` ComputationBenchmarks_BigO '' , `` cpu_coefficient '' : 672 , `` real_coefficient '' : 672 , `` big_o '' : `` N '' , `` time_unit '' : `` ns '' } , { `` name '' : `` ComputationBenchmarks_RMS '' , `` rms '' : 2 % } Removing them manually works fine , so presumably the script should just ignore complexity-related entries in the JSON .
compare_bench fails if Complexity was computed __EoT__ When the benchmarks are are run with ` Complexity ( ) ` enabled , the output JSON file contains additional members that cause the ` compare_bench.py ` script to fail : > Invalid input file : 'out.json ' does not name a valid benchmark executable or JSON file The offending lines in the JSON look like : { `` name '' : `` ComputationBenchmarks_BigO '' , `` cpu_coefficient '' : 672 , `` real_coefficient '' : 672 , `` big_o '' : `` N '' , `` time_unit '' : `` ns '' } , { `` name '' : `` ComputationBenchmarks_RMS '' , `` rms '' : 2 % } Removing them manually works fine , so presumably the script should just ignore complexity-related entries in the JSON .
compare_bench fails if Complexity was computed __EoT__ When the benchmarks are are run with ` Complexity ( ) ` enabled , the output JSON file contains additional members that cause the ` compare_bench.py ` script to fail : > Invalid input file : 'out.json ' does not name a valid benchmark executable or JSON file The offending lines in the JSON look like : { `` name '' : `` ComputationBenchmarks_BigO '' , `` cpu_coefficient '' : 672 , `` real_coefficient '' : 672 , `` big_o '' : `` N '' , `` time_unit '' : `` ns '' } , { `` name '' : `` ComputationBenchmarks_RMS '' , `` rms '' : 2 % } Removing them manually works fine , so presumably the script should just ignore complexity-related entries in the JSON .
Repetition is too large on MAC OSX and does not seem controllable __EoT__ Hi I am running some very simple benchmark ( similar to the first example given in the README ) . And I have encountered these errors : `` ` The number of inputs is very large . BM_UFTree will be repeated at least 18446744072586632936 times . benchmark ( 15824,0x7fffaed953c0 ) malloc : *** mach_vm_map ( size=18446743903025913856 ) failed ( error code=3 ) *** error : ca n't allocate region *** set a breakpoint in malloc_error_break to debug libc++abi.dylib : terminating with uncaught exception of type std : :bad_alloc : std : :bad_alloc [ 1 ] 15824 abort ./benchmark -- benchmark_repetitions=100 `` ` As you can see the command line argument does not seems to have any effect here . Am I missing something ? Thanks ! I am testing with clang++ on Mac .
Add tests to user counters __EoT__ Now that user counters were integrated , tests should be added . During review of # 262 , # 276 was suggested which added some tests . @ EricWF Could you add your tests so that I pick up from there ?
Add tests to user counters __EoT__ Now that user counters were integrated , tests should be added . During review of # 262 , # 276 was suggested which added some tests . @ EricWF Could you add your tests so that I pick up from there ?
Add tests to user counters __EoT__ Now that user counters were integrated , tests should be added . During review of # 262 , # 276 was suggested which added some tests . @ EricWF Could you add your tests so that I pick up from there ?
Add tests to user counters __EoT__ Now that user counters were integrated , tests should be added . During review of # 262 , # 276 was suggested which added some tests . @ EricWF Could you add your tests so that I pick up from there ?
Add tests to user counters __EoT__ Now that user counters were integrated , tests should be added . During review of # 262 , # 276 was suggested which added some tests . @ EricWF Could you add your tests so that I pick up from there ?
Add tests to user counters __EoT__ Now that user counters were integrated , tests should be added . During review of # 262 , # 276 was suggested which added some tests . @ EricWF Could you add your tests so that I pick up from there ?
Add tests to user counters __EoT__ Now that user counters were integrated , tests should be added . During review of # 262 , # 276 was suggested which added some tests . @ EricWF Could you add your tests so that I pick up from there ?
Add tests to user counters __EoT__ Now that user counters were integrated , tests should be added . During review of # 262 , # 276 was suggested which added some tests . @ EricWF Could you add your tests so that I pick up from there ?
Add tests to user counters __EoT__ Now that user counters were integrated , tests should be added . During review of # 262 , # 276 was suggested which added some tests . @ EricWF Could you add your tests so that I pick up from there ?
Add tests to user counters __EoT__ Now that user counters were integrated , tests should be added . During review of # 262 , # 276 was suggested which added some tests . @ EricWF Could you add your tests so that I pick up from there ?
Add tests to user counters __EoT__ Now that user counters were integrated , tests should be added . During review of # 262 , # 276 was suggested which added some tests . @ EricWF Could you add your tests so that I pick up from there ?
Add tests to user counters __EoT__ Now that user counters were integrated , tests should be added . During review of # 262 , # 276 was suggested which added some tests . @ EricWF Could you add your tests so that I pick up from there ?
Add tests to user counters __EoT__ Now that user counters were integrated , tests should be added . During review of # 262 , # 276 was suggested which added some tests . @ EricWF Could you add your tests so that I pick up from there ?
Add tests to user counters __EoT__ Now that user counters were integrated , tests should be added . During review of # 262 , # 276 was suggested which added some tests . @ EricWF Could you add your tests so that I pick up from there ?
Add tests to user counters __EoT__ Now that user counters were integrated , tests should be added . During review of # 262 , # 276 was suggested which added some tests . @ EricWF Could you add your tests so that I pick up from there ?
Add tests to user counters __EoT__ Now that user counters were integrated , tests should be added . During review of # 262 , # 276 was suggested which added some tests . @ EricWF Could you add your tests so that I pick up from there ?
Add tests to user counters __EoT__ Now that user counters were integrated , tests should be added . During review of # 262 , # 276 was suggested which added some tests . @ EricWF Could you add your tests so that I pick up from there ?
Re-add support for per-thread timing and run results . __EoT__ Issue # 86 is going to remove this functionality . In # 86 tests that are run on more than one thread report as a single result . Dominic would like to see this functionality readded .
Re-add support for per-thread timing and run results . __EoT__ Issue # 86 is going to remove this functionality . In # 86 tests that are run on more than one thread report as a single result . Dominic would like to see this functionality readded .
Warning C4800 on MSVC 2015 __EoT__ Hey , On the release of benchmark 1.3.0 there is a Warning generated for forcing std : :size_t to bool in Visual Studio 2015 . Warning C4800 'std : :size_t ' : forcing value to bool 'true ' or 'false ' ( performance warning ) \benchmark.1.3.0\include\benchmark\benchmark.h line 436 Corresponding line of code : ` bool const res = -- total_iterations_ ; ` Possible solution would be to change this line to : ` bool const res = -- total_iterations_ == 0 ; `
Impossible to specify number of iterations and number of repetitions at the same time . __EoT__ I have created a simple test : `` ` c++ # include < benchmark/benchmark.h > static void BM_Test ( benchmark : :State & state ) { std : :string x ( 1024 , 'x ' ) ; while ( state.KeepRunning ( ) ) { std : :string copy ( x ) ; } } BENCHMARK ( BM_Test ) ; int main ( int argc , const char ** argv ) { benchmark : :Initialize ( & argc , argv ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; return 0 ; } `` ` And now I try to launch it with specified number of iterations and repetitions : - when I specify only number of iterations it works good : `` ` $ ./test -- benchmark_iterations=10000 Reading /proc/self/cputime_ns failed . Using getrusage ( ) . Benchmarking on 8 X 2401 MHz CPUs 2014/10/04-11:12:01 CPU scaling is enabled : Benchmark timings may be noisy . DEBUG : Benchmark Time ( ns ) CPU ( ns ) Iterations -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
[ RFC ] *Display* aggregates only . __EoT__ There is a flag https : //github.com/google/benchmark/blob/d9cab612e40017af10bddaa5b60c7067032a9e1c/src/benchmark.cc # L75-L78 and a call https : //github.com/google/benchmark/blob/d9cab612e40017af10bddaa5b60c7067032a9e1c/include/benchmark/benchmark.h # L837-L840 But that affects everything , every reporter , destination : https : //github.com/google/benchmark/blob/d9cab612e40017af10bddaa5b60c7067032a9e1c/src/benchmark.cc # L316 It would be quite useful to have an ability to be more picky . More specifically , i would like to be able to only see the aggregates in the on-screen output , but for the file output to still contain everything . The former is useful in case of a lot of repetition ( or even more so if every iteration is reported separately ) , while the former is **great** for tooling . Now the problem . I 'm not sure how best to do it . The most straight-forward solution i can think of is to filter the results here : https : //github.com/google/benchmark/blob/d9cab612e40017af10bddaa5b60c7067032a9e1c/src/benchmark.cc # L466-L471 So the file reporter will still receive the full ` std : :vector < BenchmarkReporter : :Run > reports ` , while ` display_reporter ` may ( depending on the configuration ) only receive the aggregates . Alternative solutions could include modifying the API of ` BenchmarkReporter ` , and every
Build fails with ICC17u2 __EoT__ The build fails with ICC17 because of warnings and Werror . What is the correct solution to fix it ? Should a patch 1 ) disable Werror for ICC ( or maybe all non known compilers ) 2 ) disable the false postive warnings for all files . This could be done using : add_cxx_compiler_flag ( -wd2102 ) # ICC17u2 : Many false positives for Wstrict-aliasing add_cxx_compiler_flag ( -wd2259 ) # ICC17u2 : non-pointer conversion from `` long '' to `` int '' may lose significant bits ( even for explicit static cast , sleep.cc ( 44 ) ) add_cxx_compiler_flag ( -wd654 ) # ICC17u2 : overloaded virtual function `` benchmark : :Fixture : :SetUp '' is only partially overridden ( because of deprecated overload ) 3 ) disable warnings at file level or some other granularity Another warning which is n't clearly a false positive is ../test/output_test_helper.cc ( 34 ) : error # 3280 : declaration hides variable `` < unnamed > : :dec_re '' ( declared at line 67 of `` ../test/output_test.h '' ) Should the local dec_re be renamed or should that warning be suppressed too for 2/3 ? If I know
Build fails with ICC17u2 __EoT__ The build fails with ICC17 because of warnings and Werror . What is the correct solution to fix it ? Should a patch 1 ) disable Werror for ICC ( or maybe all non known compilers ) 2 ) disable the false postive warnings for all files . This could be done using : add_cxx_compiler_flag ( -wd2102 ) # ICC17u2 : Many false positives for Wstrict-aliasing add_cxx_compiler_flag ( -wd2259 ) # ICC17u2 : non-pointer conversion from `` long '' to `` int '' may lose significant bits ( even for explicit static cast , sleep.cc ( 44 ) ) add_cxx_compiler_flag ( -wd654 ) # ICC17u2 : overloaded virtual function `` benchmark : :Fixture : :SetUp '' is only partially overridden ( because of deprecated overload ) 3 ) disable warnings at file level or some other granularity Another warning which is n't clearly a false positive is ../test/output_test_helper.cc ( 34 ) : error # 3280 : declaration hides variable `` < unnamed > : :dec_re '' ( declared at line 67 of `` ../test/output_test.h '' ) Should the local dec_re be renamed or should that warning be suppressed too for 2/3 ? If I know
Build fails with ICC17u2 __EoT__ The build fails with ICC17 because of warnings and Werror . What is the correct solution to fix it ? Should a patch 1 ) disable Werror for ICC ( or maybe all non known compilers ) 2 ) disable the false postive warnings for all files . This could be done using : add_cxx_compiler_flag ( -wd2102 ) # ICC17u2 : Many false positives for Wstrict-aliasing add_cxx_compiler_flag ( -wd2259 ) # ICC17u2 : non-pointer conversion from `` long '' to `` int '' may lose significant bits ( even for explicit static cast , sleep.cc ( 44 ) ) add_cxx_compiler_flag ( -wd654 ) # ICC17u2 : overloaded virtual function `` benchmark : :Fixture : :SetUp '' is only partially overridden ( because of deprecated overload ) 3 ) disable warnings at file level or some other granularity Another warning which is n't clearly a false positive is ../test/output_test_helper.cc ( 34 ) : error # 3280 : declaration hides variable `` < unnamed > : :dec_re '' ( declared at line 67 of `` ../test/output_test.h '' ) Should the local dec_re be renamed or should that warning be suppressed too for 2/3 ? If I know
Build fails with ICC17u2 __EoT__ The build fails with ICC17 because of warnings and Werror . What is the correct solution to fix it ? Should a patch 1 ) disable Werror for ICC ( or maybe all non known compilers ) 2 ) disable the false postive warnings for all files . This could be done using : add_cxx_compiler_flag ( -wd2102 ) # ICC17u2 : Many false positives for Wstrict-aliasing add_cxx_compiler_flag ( -wd2259 ) # ICC17u2 : non-pointer conversion from `` long '' to `` int '' may lose significant bits ( even for explicit static cast , sleep.cc ( 44 ) ) add_cxx_compiler_flag ( -wd654 ) # ICC17u2 : overloaded virtual function `` benchmark : :Fixture : :SetUp '' is only partially overridden ( because of deprecated overload ) 3 ) disable warnings at file level or some other granularity Another warning which is n't clearly a false positive is ../test/output_test_helper.cc ( 34 ) : error # 3280 : declaration hides variable `` < unnamed > : :dec_re '' ( declared at line 67 of `` ../test/output_test.h '' ) Should the local dec_re be renamed or should that warning be suppressed too for 2/3 ? If I know
Build fails with ICC17u2 __EoT__ The build fails with ICC17 because of warnings and Werror . What is the correct solution to fix it ? Should a patch 1 ) disable Werror for ICC ( or maybe all non known compilers ) 2 ) disable the false postive warnings for all files . This could be done using : add_cxx_compiler_flag ( -wd2102 ) # ICC17u2 : Many false positives for Wstrict-aliasing add_cxx_compiler_flag ( -wd2259 ) # ICC17u2 : non-pointer conversion from `` long '' to `` int '' may lose significant bits ( even for explicit static cast , sleep.cc ( 44 ) ) add_cxx_compiler_flag ( -wd654 ) # ICC17u2 : overloaded virtual function `` benchmark : :Fixture : :SetUp '' is only partially overridden ( because of deprecated overload ) 3 ) disable warnings at file level or some other granularity Another warning which is n't clearly a false positive is ../test/output_test_helper.cc ( 34 ) : error # 3280 : declaration hides variable `` < unnamed > : :dec_re '' ( declared at line 67 of `` ../test/output_test.h '' ) Should the local dec_re be renamed or should that warning be suppressed too for 2/3 ? If I know
Build fails with ICC17u2 __EoT__ The build fails with ICC17 because of warnings and Werror . What is the correct solution to fix it ? Should a patch 1 ) disable Werror for ICC ( or maybe all non known compilers ) 2 ) disable the false postive warnings for all files . This could be done using : add_cxx_compiler_flag ( -wd2102 ) # ICC17u2 : Many false positives for Wstrict-aliasing add_cxx_compiler_flag ( -wd2259 ) # ICC17u2 : non-pointer conversion from `` long '' to `` int '' may lose significant bits ( even for explicit static cast , sleep.cc ( 44 ) ) add_cxx_compiler_flag ( -wd654 ) # ICC17u2 : overloaded virtual function `` benchmark : :Fixture : :SetUp '' is only partially overridden ( because of deprecated overload ) 3 ) disable warnings at file level or some other granularity Another warning which is n't clearly a false positive is ../test/output_test_helper.cc ( 34 ) : error # 3280 : declaration hides variable `` < unnamed > : :dec_re '' ( declared at line 67 of `` ../test/output_test.h '' ) Should the local dec_re be renamed or should that warning be suppressed too for 2/3 ? If I know
Build fails with ICC17u2 __EoT__ The build fails with ICC17 because of warnings and Werror . What is the correct solution to fix it ? Should a patch 1 ) disable Werror for ICC ( or maybe all non known compilers ) 2 ) disable the false postive warnings for all files . This could be done using : add_cxx_compiler_flag ( -wd2102 ) # ICC17u2 : Many false positives for Wstrict-aliasing add_cxx_compiler_flag ( -wd2259 ) # ICC17u2 : non-pointer conversion from `` long '' to `` int '' may lose significant bits ( even for explicit static cast , sleep.cc ( 44 ) ) add_cxx_compiler_flag ( -wd654 ) # ICC17u2 : overloaded virtual function `` benchmark : :Fixture : :SetUp '' is only partially overridden ( because of deprecated overload ) 3 ) disable warnings at file level or some other granularity Another warning which is n't clearly a false positive is ../test/output_test_helper.cc ( 34 ) : error # 3280 : declaration hides variable `` < unnamed > : :dec_re '' ( declared at line 67 of `` ../test/output_test.h '' ) Should the local dec_re be renamed or should that warning be suppressed too for 2/3 ? If I know
Build fails with ICC17u2 __EoT__ The build fails with ICC17 because of warnings and Werror . What is the correct solution to fix it ? Should a patch 1 ) disable Werror for ICC ( or maybe all non known compilers ) 2 ) disable the false postive warnings for all files . This could be done using : add_cxx_compiler_flag ( -wd2102 ) # ICC17u2 : Many false positives for Wstrict-aliasing add_cxx_compiler_flag ( -wd2259 ) # ICC17u2 : non-pointer conversion from `` long '' to `` int '' may lose significant bits ( even for explicit static cast , sleep.cc ( 44 ) ) add_cxx_compiler_flag ( -wd654 ) # ICC17u2 : overloaded virtual function `` benchmark : :Fixture : :SetUp '' is only partially overridden ( because of deprecated overload ) 3 ) disable warnings at file level or some other granularity Another warning which is n't clearly a false positive is ../test/output_test_helper.cc ( 34 ) : error # 3280 : declaration hides variable `` < unnamed > : :dec_re '' ( declared at line 67 of `` ../test/output_test.h '' ) Should the local dec_re be renamed or should that warning be suppressed too for 2/3 ? If I know
Build fails with ICC17u2 __EoT__ The build fails with ICC17 because of warnings and Werror . What is the correct solution to fix it ? Should a patch 1 ) disable Werror for ICC ( or maybe all non known compilers ) 2 ) disable the false postive warnings for all files . This could be done using : add_cxx_compiler_flag ( -wd2102 ) # ICC17u2 : Many false positives for Wstrict-aliasing add_cxx_compiler_flag ( -wd2259 ) # ICC17u2 : non-pointer conversion from `` long '' to `` int '' may lose significant bits ( even for explicit static cast , sleep.cc ( 44 ) ) add_cxx_compiler_flag ( -wd654 ) # ICC17u2 : overloaded virtual function `` benchmark : :Fixture : :SetUp '' is only partially overridden ( because of deprecated overload ) 3 ) disable warnings at file level or some other granularity Another warning which is n't clearly a false positive is ../test/output_test_helper.cc ( 34 ) : error # 3280 : declaration hides variable `` < unnamed > : :dec_re '' ( declared at line 67 of `` ../test/output_test.h '' ) Should the local dec_re be renamed or should that warning be suppressed too for 2/3 ? If I know
Build fails with ICC17u2 __EoT__ The build fails with ICC17 because of warnings and Werror . What is the correct solution to fix it ? Should a patch 1 ) disable Werror for ICC ( or maybe all non known compilers ) 2 ) disable the false postive warnings for all files . This could be done using : add_cxx_compiler_flag ( -wd2102 ) # ICC17u2 : Many false positives for Wstrict-aliasing add_cxx_compiler_flag ( -wd2259 ) # ICC17u2 : non-pointer conversion from `` long '' to `` int '' may lose significant bits ( even for explicit static cast , sleep.cc ( 44 ) ) add_cxx_compiler_flag ( -wd654 ) # ICC17u2 : overloaded virtual function `` benchmark : :Fixture : :SetUp '' is only partially overridden ( because of deprecated overload ) 3 ) disable warnings at file level or some other granularity Another warning which is n't clearly a false positive is ../test/output_test_helper.cc ( 34 ) : error # 3280 : declaration hides variable `` < unnamed > : :dec_re '' ( declared at line 67 of `` ../test/output_test.h '' ) Should the local dec_re be renamed or should that warning be suppressed too for 2/3 ? If I know
Build fails with ICC17u2 __EoT__ The build fails with ICC17 because of warnings and Werror . What is the correct solution to fix it ? Should a patch 1 ) disable Werror for ICC ( or maybe all non known compilers ) 2 ) disable the false postive warnings for all files . This could be done using : add_cxx_compiler_flag ( -wd2102 ) # ICC17u2 : Many false positives for Wstrict-aliasing add_cxx_compiler_flag ( -wd2259 ) # ICC17u2 : non-pointer conversion from `` long '' to `` int '' may lose significant bits ( even for explicit static cast , sleep.cc ( 44 ) ) add_cxx_compiler_flag ( -wd654 ) # ICC17u2 : overloaded virtual function `` benchmark : :Fixture : :SetUp '' is only partially overridden ( because of deprecated overload ) 3 ) disable warnings at file level or some other granularity Another warning which is n't clearly a false positive is ../test/output_test_helper.cc ( 34 ) : error # 3280 : declaration hides variable `` < unnamed > : :dec_re '' ( declared at line 67 of `` ../test/output_test.h '' ) Should the local dec_re be renamed or should that warning be suppressed too for 2/3 ? If I know
Build fails with ICC17u2 __EoT__ The build fails with ICC17 because of warnings and Werror . What is the correct solution to fix it ? Should a patch 1 ) disable Werror for ICC ( or maybe all non known compilers ) 2 ) disable the false postive warnings for all files . This could be done using : add_cxx_compiler_flag ( -wd2102 ) # ICC17u2 : Many false positives for Wstrict-aliasing add_cxx_compiler_flag ( -wd2259 ) # ICC17u2 : non-pointer conversion from `` long '' to `` int '' may lose significant bits ( even for explicit static cast , sleep.cc ( 44 ) ) add_cxx_compiler_flag ( -wd654 ) # ICC17u2 : overloaded virtual function `` benchmark : :Fixture : :SetUp '' is only partially overridden ( because of deprecated overload ) 3 ) disable warnings at file level or some other granularity Another warning which is n't clearly a false positive is ../test/output_test_helper.cc ( 34 ) : error # 3280 : declaration hides variable `` < unnamed > : :dec_re '' ( declared at line 67 of `` ../test/output_test.h '' ) Should the local dec_re be renamed or should that warning be suppressed too for 2/3 ? If I know
Build fails with ICC17u2 __EoT__ The build fails with ICC17 because of warnings and Werror . What is the correct solution to fix it ? Should a patch 1 ) disable Werror for ICC ( or maybe all non known compilers ) 2 ) disable the false postive warnings for all files . This could be done using : add_cxx_compiler_flag ( -wd2102 ) # ICC17u2 : Many false positives for Wstrict-aliasing add_cxx_compiler_flag ( -wd2259 ) # ICC17u2 : non-pointer conversion from `` long '' to `` int '' may lose significant bits ( even for explicit static cast , sleep.cc ( 44 ) ) add_cxx_compiler_flag ( -wd654 ) # ICC17u2 : overloaded virtual function `` benchmark : :Fixture : :SetUp '' is only partially overridden ( because of deprecated overload ) 3 ) disable warnings at file level or some other granularity Another warning which is n't clearly a false positive is ../test/output_test_helper.cc ( 34 ) : error # 3280 : declaration hides variable `` < unnamed > : :dec_re '' ( declared at line 67 of `` ../test/output_test.h '' ) Should the local dec_re be renamed or should that warning be suppressed too for 2/3 ? If I know
Build fails with ICC17u2 __EoT__ The build fails with ICC17 because of warnings and Werror . What is the correct solution to fix it ? Should a patch 1 ) disable Werror for ICC ( or maybe all non known compilers ) 2 ) disable the false postive warnings for all files . This could be done using : add_cxx_compiler_flag ( -wd2102 ) # ICC17u2 : Many false positives for Wstrict-aliasing add_cxx_compiler_flag ( -wd2259 ) # ICC17u2 : non-pointer conversion from `` long '' to `` int '' may lose significant bits ( even for explicit static cast , sleep.cc ( 44 ) ) add_cxx_compiler_flag ( -wd654 ) # ICC17u2 : overloaded virtual function `` benchmark : :Fixture : :SetUp '' is only partially overridden ( because of deprecated overload ) 3 ) disable warnings at file level or some other granularity Another warning which is n't clearly a false positive is ../test/output_test_helper.cc ( 34 ) : error # 3280 : declaration hides variable `` < unnamed > : :dec_re '' ( declared at line 67 of `` ../test/output_test.h '' ) Should the local dec_re be renamed or should that warning be suppressed too for 2/3 ? If I know
Build fails with ICC17u2 __EoT__ The build fails with ICC17 because of warnings and Werror . What is the correct solution to fix it ? Should a patch 1 ) disable Werror for ICC ( or maybe all non known compilers ) 2 ) disable the false postive warnings for all files . This could be done using : add_cxx_compiler_flag ( -wd2102 ) # ICC17u2 : Many false positives for Wstrict-aliasing add_cxx_compiler_flag ( -wd2259 ) # ICC17u2 : non-pointer conversion from `` long '' to `` int '' may lose significant bits ( even for explicit static cast , sleep.cc ( 44 ) ) add_cxx_compiler_flag ( -wd654 ) # ICC17u2 : overloaded virtual function `` benchmark : :Fixture : :SetUp '' is only partially overridden ( because of deprecated overload ) 3 ) disable warnings at file level or some other granularity Another warning which is n't clearly a false positive is ../test/output_test_helper.cc ( 34 ) : error # 3280 : declaration hides variable `` < unnamed > : :dec_re '' ( declared at line 67 of `` ../test/output_test.h '' ) Should the local dec_re be renamed or should that warning be suppressed too for 2/3 ? If I know
Fine-grained iteration count control __EoT__ Is there a way to control the iteration count directly instead of -- benchmark_min_time ?
Fine-grained iteration count control __EoT__ Is there a way to control the iteration count directly instead of -- benchmark_min_time ?
Fine-grained iteration count control __EoT__ Is there a way to control the iteration count directly instead of -- benchmark_min_time ?
Fine-grained iteration count control __EoT__ Is there a way to control the iteration count directly instead of -- benchmark_min_time ?
Fine-grained iteration count control __EoT__ Is there a way to control the iteration count directly instead of -- benchmark_min_time ?
Fine-grained iteration count control __EoT__ Is there a way to control the iteration count directly instead of -- benchmark_min_time ?
Fine-grained iteration count control __EoT__ Is there a way to control the iteration count directly instead of -- benchmark_min_time ?
Fine-grained iteration count control __EoT__ Is there a way to control the iteration count directly instead of -- benchmark_min_time ?
Fine-grained iteration count control __EoT__ Is there a way to control the iteration count directly instead of -- benchmark_min_time ?
Fine-grained iteration count control __EoT__ Is there a way to control the iteration count directly instead of -- benchmark_min_time ?
Fine-grained iteration count control __EoT__ Is there a way to control the iteration count directly instead of -- benchmark_min_time ?
Fine-grained iteration count control __EoT__ Is there a way to control the iteration count directly instead of -- benchmark_min_time ?
Fine-grained iteration count control __EoT__ Is there a way to control the iteration count directly instead of -- benchmark_min_time ?
Fine-grained iteration count control __EoT__ Is there a way to control the iteration count directly instead of -- benchmark_min_time ?
Fine-grained iteration count control __EoT__ Is there a way to control the iteration count directly instead of -- benchmark_min_time ?
Fine-grained iteration count control __EoT__ Is there a way to control the iteration count directly instead of -- benchmark_min_time ?
Fine-grained iteration count control __EoT__ Is there a way to control the iteration count directly instead of -- benchmark_min_time ?
User Counters ignore Flag parameter __EoT__ When creating a user counter it display the count as if ` kDefaults ` flag was used even if explictly setting a different flag ` st.counters [ `` Foo '' ] = benchmark : :Counter ( FooCount , benchmark : :Counter : :kIsRate ) ; `
User Counters ignore Flag parameter __EoT__ When creating a user counter it display the count as if ` kDefaults ` flag was used even if explictly setting a different flag ` st.counters [ `` Foo '' ] = benchmark : :Counter ( FooCount , benchmark : :Counter : :kIsRate ) ; `
User Counters ignore Flag parameter __EoT__ When creating a user counter it display the count as if ` kDefaults ` flag was used even if explictly setting a different flag ` st.counters [ `` Foo '' ] = benchmark : :Counter ( FooCount , benchmark : :Counter : :kIsRate ) ; `
User Counters ignore Flag parameter __EoT__ When creating a user counter it display the count as if ` kDefaults ` flag was used even if explictly setting a different flag ` st.counters [ `` Foo '' ] = benchmark : :Counter ( FooCount , benchmark : :Counter : :kIsRate ) ; `
User Counters ignore Flag parameter __EoT__ When creating a user counter it display the count as if ` kDefaults ` flag was used even if explictly setting a different flag ` st.counters [ `` Foo '' ] = benchmark : :Counter ( FooCount , benchmark : :Counter : :kIsRate ) ; `
User Counters ignore Flag parameter __EoT__ When creating a user counter it display the count as if ` kDefaults ` flag was used even if explictly setting a different flag ` st.counters [ `` Foo '' ] = benchmark : :Counter ( FooCount , benchmark : :Counter : :kIsRate ) ; `
User Counters ignore Flag parameter __EoT__ When creating a user counter it display the count as if ` kDefaults ` flag was used even if explictly setting a different flag ` st.counters [ `` Foo '' ] = benchmark : :Counter ( FooCount , benchmark : :Counter : :kIsRate ) ; `
User Counters ignore Flag parameter __EoT__ When creating a user counter it display the count as if ` kDefaults ` flag was used even if explictly setting a different flag ` st.counters [ `` Foo '' ] = benchmark : :Counter ( FooCount , benchmark : :Counter : :kIsRate ) ; `
User Counters ignore Flag parameter __EoT__ When creating a user counter it display the count as if ` kDefaults ` flag was used even if explictly setting a different flag ` st.counters [ `` Foo '' ] = benchmark : :Counter ( FooCount , benchmark : :Counter : :kIsRate ) ; `
User Counters ignore Flag parameter __EoT__ When creating a user counter it display the count as if ` kDefaults ` flag was used even if explictly setting a different flag ` st.counters [ `` Foo '' ] = benchmark : :Counter ( FooCount , benchmark : :Counter : :kIsRate ) ; `
User Counters ignore Flag parameter __EoT__ When creating a user counter it display the count as if ` kDefaults ` flag was used even if explictly setting a different flag ` st.counters [ `` Foo '' ] = benchmark : :Counter ( FooCount , benchmark : :Counter : :kIsRate ) ; `
User Counters ignore Flag parameter __EoT__ When creating a user counter it display the count as if ` kDefaults ` flag was used even if explictly setting a different flag ` st.counters [ `` Foo '' ] = benchmark : :Counter ( FooCount , benchmark : :Counter : :kIsRate ) ; `
User Counters ignore Flag parameter __EoT__ When creating a user counter it display the count as if ` kDefaults ` flag was used even if explictly setting a different flag ` st.counters [ `` Foo '' ] = benchmark : :Counter ( FooCount , benchmark : :Counter : :kIsRate ) ; `
User Counters ignore Flag parameter __EoT__ When creating a user counter it display the count as if ` kDefaults ` flag was used even if explictly setting a different flag ` st.counters [ `` Foo '' ] = benchmark : :Counter ( FooCount , benchmark : :Counter : :kIsRate ) ; `
User Counters ignore Flag parameter __EoT__ When creating a user counter it display the count as if ` kDefaults ` flag was used even if explictly setting a different flag ` st.counters [ `` Foo '' ] = benchmark : :Counter ( FooCount , benchmark : :Counter : :kIsRate ) ; `
User Counters ignore Flag parameter __EoT__ When creating a user counter it display the count as if ` kDefaults ` flag was used even if explictly setting a different flag ` st.counters [ `` Foo '' ] = benchmark : :Counter ( FooCount , benchmark : :Counter : :kIsRate ) ; `
User Counters ignore Flag parameter __EoT__ When creating a user counter it display the count as if ` kDefaults ` flag was used even if explictly setting a different flag ` st.counters [ `` Foo '' ] = benchmark : :Counter ( FooCount , benchmark : :Counter : :kIsRate ) ; `
User Counters ignore Flag parameter __EoT__ When creating a user counter it display the count as if ` kDefaults ` flag was used even if explictly setting a different flag ` st.counters [ `` Foo '' ] = benchmark : :Counter ( FooCount , benchmark : :Counter : :kIsRate ) ; `
User Counters ignore Flag parameter __EoT__ When creating a user counter it display the count as if ` kDefaults ` flag was used even if explictly setting a different flag ` st.counters [ `` Foo '' ] = benchmark : :Counter ( FooCount , benchmark : :Counter : :kIsRate ) ; `
User Counters ignore Flag parameter __EoT__ When creating a user counter it display the count as if ` kDefaults ` flag was used even if explictly setting a different flag ` st.counters [ `` Foo '' ] = benchmark : :Counter ( FooCount , benchmark : :Counter : :kIsRate ) ; `
User Counters ignore Flag parameter __EoT__ When creating a user counter it display the count as if ` kDefaults ` flag was used even if explictly setting a different flag ` st.counters [ `` Foo '' ] = benchmark : :Counter ( FooCount , benchmark : :Counter : :kIsRate ) ; `
User Counters ignore Flag parameter __EoT__ When creating a user counter it display the count as if ` kDefaults ` flag was used even if explictly setting a different flag ` st.counters [ `` Foo '' ] = benchmark : :Counter ( FooCount , benchmark : :Counter : :kIsRate ) ; `
User Counters ignore Flag parameter __EoT__ When creating a user counter it display the count as if ` kDefaults ` flag was used even if explictly setting a different flag ` st.counters [ `` Foo '' ] = benchmark : :Counter ( FooCount , benchmark : :Counter : :kIsRate ) ; `
CXXFeatureCheck for regex for cross compiling failing __EoT__ CXXFeature testing is not possible for cross compiling to another platform . Specifying -DRUN_HAVE_POSIX_REGEX=flase -DHAVE_POSIX_REGEX=true does not pass these definitions to the makefiles .
CXXFeatureCheck for regex for cross compiling failing __EoT__ CXXFeature testing is not possible for cross compiling to another platform . Specifying -DRUN_HAVE_POSIX_REGEX=flase -DHAVE_POSIX_REGEX=true does not pass these definitions to the makefiles .
CXXFeatureCheck for regex for cross compiling failing __EoT__ CXXFeature testing is not possible for cross compiling to another platform . Specifying -DRUN_HAVE_POSIX_REGEX=flase -DHAVE_POSIX_REGEX=true does not pass these definitions to the makefiles .
CXXFeatureCheck for regex for cross compiling failing __EoT__ CXXFeature testing is not possible for cross compiling to another platform . Specifying -DRUN_HAVE_POSIX_REGEX=flase -DHAVE_POSIX_REGEX=true does not pass these definitions to the makefiles .
CXXFeatureCheck for regex for cross compiling failing __EoT__ CXXFeature testing is not possible for cross compiling to another platform . Specifying -DRUN_HAVE_POSIX_REGEX=flase -DHAVE_POSIX_REGEX=true does not pass these definitions to the makefiles .
CXXFeatureCheck for regex for cross compiling failing __EoT__ CXXFeature testing is not possible for cross compiling to another platform . Specifying -DRUN_HAVE_POSIX_REGEX=flase -DHAVE_POSIX_REGEX=true does not pass these definitions to the makefiles .
CXXFeatureCheck for regex for cross compiling failing __EoT__ CXXFeature testing is not possible for cross compiling to another platform . Specifying -DRUN_HAVE_POSIX_REGEX=flase -DHAVE_POSIX_REGEX=true does not pass these definitions to the makefiles .
Custom Stats __EoT__ From the README and the API header , it looks like the supported metrics are bytes processed and items processed . Is it possible to extend these with custom metrics ? For example , one might be interested in GFLOPS , or other derived metrics such as raw bandwidth v/s effective bandwidth .
Custom Stats __EoT__ From the README and the API header , it looks like the supported metrics are bytes processed and items processed . Is it possible to extend these with custom metrics ? For example , one might be interested in GFLOPS , or other derived metrics such as raw bandwidth v/s effective bandwidth .
Custom Stats __EoT__ From the README and the API header , it looks like the supported metrics are bytes processed and items processed . Is it possible to extend these with custom metrics ? For example , one might be interested in GFLOPS , or other derived metrics such as raw bandwidth v/s effective bandwidth .
Custom Stats __EoT__ From the README and the API header , it looks like the supported metrics are bytes processed and items processed . Is it possible to extend these with custom metrics ? For example , one might be interested in GFLOPS , or other derived metrics such as raw bandwidth v/s effective bandwidth .
Custom Stats __EoT__ From the README and the API header , it looks like the supported metrics are bytes processed and items processed . Is it possible to extend these with custom metrics ? For example , one might be interested in GFLOPS , or other derived metrics such as raw bandwidth v/s effective bandwidth .
Custom Stats __EoT__ From the README and the API header , it looks like the supported metrics are bytes processed and items processed . Is it possible to extend these with custom metrics ? For example , one might be interested in GFLOPS , or other derived metrics such as raw bandwidth v/s effective bandwidth .
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RegisterBenchmark and Remove Benchmarks __EoT__ I 've been trying to create a class wrapper that can evoke benchmark when the member function is called . For now the function works on only one instance . `` ` c++ void foo ( const A & event ) { this- > CBM_Timer = [ this ] ( benchmark : :State & st , A event ) { while ( st.KeepRunning ( ) ) this- > BM_Routine_E1 ( event ) ; } ; benchmark : :RegisterBenchmark ( `` on_timer '' , this- > CBM_Timer , event ) ; int fake_argc = 1 ; cout < < fake_argc < < endl ; benchmark : :Initialize ( & fake_argc , nullptr ) ; benchmark : :RunSpecifiedBenchmarks ( ) ; } `` ` One of the problem is that invoking the function twice results in broken pipe ( tho this particular error could be caused on my end.. ) `` ` c++ gzip : gzip : stdout : Broken pipe stdout : Broken pipe Segmentation fault ( core dumped ) `` ` Another problem is when evoked multiple times , the results are `` ` 2017-06-13 14:31:46 Benchmark Time CPU Iterations -- -- -- -- --
RMS calculation seems broken , takes time Unit into account ? __EoT__ Hi . Consider the [ ` BM_Complexity_O_N ` ] ( https : //github.com/google/benchmark/blob/36a251ab3ee4e6dc668129e0ef5fa62b1b8d5d49/test/complexity_test.cc # L80-L113 ) Vanilla version outputs : `` ` Benchmark Time CPU Iterations -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - BM_Complexity_O_N/1024 441 ns 441 ns 1583329 BM_Complexity_O_N/2k 881 ns 881 ns 797104 BM_Complexity_O_N/4k 1767 ns 1767 ns 395988 BM_Complexity_O_N/8k 3691 ns 3690 ns 189567 BM_Complexity_O_N/16k 7377 ns 7376 ns 95031 BM_Complexity_O_N/32k 14813 ns 14801 ns 47507 BM_Complexity_O_N/64k 29605 ns 29602 ns 23608 BM_Complexity_O_N_BigO 0.45 N 0.45 N BM_Complexity_O_N_RMS 0 % 0 % BM_Complexity_O_N/1024 446 ns 446 ns 1587672 BM_Complexity_O_N/2k 884 ns 884 ns 792614 BM_Complexity_O_N/4k 1787 ns 1786 ns 393225 BM_Complexity_O_N/8k 3684 ns 3683 ns 189791 BM_Complexity_O_N/16k 7373 ns 7369 ns 95181 BM_Complexity_O_N/32k 14716 ns 14711 ns 47732 BM_Complexity_O_N/64k 29714 ns 29707 ns 23656 BM_Complexity_O_N_BigO 0.45 f ( N ) 0.45 f ( N ) BM_Complexity_O_N_RMS 1 % 1 % BM_Complexity_O_N/1024 442 ns 442 ns 1589204 BM_Complexity_O_N/2k 885 ns 884 ns 788984 BM_Complexity_O_N/4k 1781 ns 1781 ns 390535 BM_Complexity_O_N/8k 3691 ns
RMS calculation seems broken , takes time Unit into account ? __EoT__ Hi . Consider the [ ` BM_Complexity_O_N ` ] ( https : //github.com/google/benchmark/blob/36a251ab3ee4e6dc668129e0ef5fa62b1b8d5d49/test/complexity_test.cc # L80-L113 ) Vanilla version outputs : `` ` Benchmark Time CPU Iterations -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - BM_Complexity_O_N/1024 441 ns 441 ns 1583329 BM_Complexity_O_N/2k 881 ns 881 ns 797104 BM_Complexity_O_N/4k 1767 ns 1767 ns 395988 BM_Complexity_O_N/8k 3691 ns 3690 ns 189567 BM_Complexity_O_N/16k 7377 ns 7376 ns 95031 BM_Complexity_O_N/32k 14813 ns 14801 ns 47507 BM_Complexity_O_N/64k 29605 ns 29602 ns 23608 BM_Complexity_O_N_BigO 0.45 N 0.45 N BM_Complexity_O_N_RMS 0 % 0 % BM_Complexity_O_N/1024 446 ns 446 ns 1587672 BM_Complexity_O_N/2k 884 ns 884 ns 792614 BM_Complexity_O_N/4k 1787 ns 1786 ns 393225 BM_Complexity_O_N/8k 3684 ns 3683 ns 189791 BM_Complexity_O_N/16k 7373 ns 7369 ns 95181 BM_Complexity_O_N/32k 14716 ns 14711 ns 47732 BM_Complexity_O_N/64k 29714 ns 29707 ns 23656 BM_Complexity_O_N_BigO 0.45 f ( N ) 0.45 f ( N ) BM_Complexity_O_N_RMS 1 % 1 % BM_Complexity_O_N/1024 442 ns 442 ns 1589204 BM_Complexity_O_N/2k 885 ns 884 ns 788984 BM_Complexity_O_N/4k 1781 ns 1781 ns 390535 BM_Complexity_O_N/8k 3691 ns
RMS calculation seems broken , takes time Unit into account ? __EoT__ Hi . Consider the [ ` BM_Complexity_O_N ` ] ( https : //github.com/google/benchmark/blob/36a251ab3ee4e6dc668129e0ef5fa62b1b8d5d49/test/complexity_test.cc # L80-L113 ) Vanilla version outputs : `` ` Benchmark Time CPU Iterations -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - BM_Complexity_O_N/1024 441 ns 441 ns 1583329 BM_Complexity_O_N/2k 881 ns 881 ns 797104 BM_Complexity_O_N/4k 1767 ns 1767 ns 395988 BM_Complexity_O_N/8k 3691 ns 3690 ns 189567 BM_Complexity_O_N/16k 7377 ns 7376 ns 95031 BM_Complexity_O_N/32k 14813 ns 14801 ns 47507 BM_Complexity_O_N/64k 29605 ns 29602 ns 23608 BM_Complexity_O_N_BigO 0.45 N 0.45 N BM_Complexity_O_N_RMS 0 % 0 % BM_Complexity_O_N/1024 446 ns 446 ns 1587672 BM_Complexity_O_N/2k 884 ns 884 ns 792614 BM_Complexity_O_N/4k 1787 ns 1786 ns 393225 BM_Complexity_O_N/8k 3684 ns 3683 ns 189791 BM_Complexity_O_N/16k 7373 ns 7369 ns 95181 BM_Complexity_O_N/32k 14716 ns 14711 ns 47732 BM_Complexity_O_N/64k 29714 ns 29707 ns 23656 BM_Complexity_O_N_BigO 0.45 f ( N ) 0.45 f ( N ) BM_Complexity_O_N_RMS 1 % 1 % BM_Complexity_O_N/1024 442 ns 442 ns 1589204 BM_Complexity_O_N/2k 885 ns 884 ns 788984 BM_Complexity_O_N/4k 1781 ns 1781 ns 390535 BM_Complexity_O_N/8k 3691 ns
RMS calculation seems broken , takes time Unit into account ? __EoT__ Hi . Consider the [ ` BM_Complexity_O_N ` ] ( https : //github.com/google/benchmark/blob/36a251ab3ee4e6dc668129e0ef5fa62b1b8d5d49/test/complexity_test.cc # L80-L113 ) Vanilla version outputs : `` ` Benchmark Time CPU Iterations -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - BM_Complexity_O_N/1024 441 ns 441 ns 1583329 BM_Complexity_O_N/2k 881 ns 881 ns 797104 BM_Complexity_O_N/4k 1767 ns 1767 ns 395988 BM_Complexity_O_N/8k 3691 ns 3690 ns 189567 BM_Complexity_O_N/16k 7377 ns 7376 ns 95031 BM_Complexity_O_N/32k 14813 ns 14801 ns 47507 BM_Complexity_O_N/64k 29605 ns 29602 ns 23608 BM_Complexity_O_N_BigO 0.45 N 0.45 N BM_Complexity_O_N_RMS 0 % 0 % BM_Complexity_O_N/1024 446 ns 446 ns 1587672 BM_Complexity_O_N/2k 884 ns 884 ns 792614 BM_Complexity_O_N/4k 1787 ns 1786 ns 393225 BM_Complexity_O_N/8k 3684 ns 3683 ns 189791 BM_Complexity_O_N/16k 7373 ns 7369 ns 95181 BM_Complexity_O_N/32k 14716 ns 14711 ns 47732 BM_Complexity_O_N/64k 29714 ns 29707 ns 23656 BM_Complexity_O_N_BigO 0.45 f ( N ) 0.45 f ( N ) BM_Complexity_O_N_RMS 1 % 1 % BM_Complexity_O_N/1024 442 ns 442 ns 1589204 BM_Complexity_O_N/2k 885 ns 884 ns 788984 BM_Complexity_O_N/4k 1781 ns 1781 ns 390535 BM_Complexity_O_N/8k 3691 ns
RMS calculation seems broken , takes time Unit into account ? __EoT__ Hi . Consider the [ ` BM_Complexity_O_N ` ] ( https : //github.com/google/benchmark/blob/36a251ab3ee4e6dc668129e0ef5fa62b1b8d5d49/test/complexity_test.cc # L80-L113 ) Vanilla version outputs : `` ` Benchmark Time CPU Iterations -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - BM_Complexity_O_N/1024 441 ns 441 ns 1583329 BM_Complexity_O_N/2k 881 ns 881 ns 797104 BM_Complexity_O_N/4k 1767 ns 1767 ns 395988 BM_Complexity_O_N/8k 3691 ns 3690 ns 189567 BM_Complexity_O_N/16k 7377 ns 7376 ns 95031 BM_Complexity_O_N/32k 14813 ns 14801 ns 47507 BM_Complexity_O_N/64k 29605 ns 29602 ns 23608 BM_Complexity_O_N_BigO 0.45 N 0.45 N BM_Complexity_O_N_RMS 0 % 0 % BM_Complexity_O_N/1024 446 ns 446 ns 1587672 BM_Complexity_O_N/2k 884 ns 884 ns 792614 BM_Complexity_O_N/4k 1787 ns 1786 ns 393225 BM_Complexity_O_N/8k 3684 ns 3683 ns 189791 BM_Complexity_O_N/16k 7373 ns 7369 ns 95181 BM_Complexity_O_N/32k 14716 ns 14711 ns 47732 BM_Complexity_O_N/64k 29714 ns 29707 ns 23656 BM_Complexity_O_N_BigO 0.45 f ( N ) 0.45 f ( N ) BM_Complexity_O_N_RMS 1 % 1 % BM_Complexity_O_N/1024 442 ns 442 ns 1589204 BM_Complexity_O_N/2k 885 ns 884 ns 788984 BM_Complexity_O_N/4k 1781 ns 1781 ns 390535 BM_Complexity_O_N/8k 3691 ns
RMS calculation seems broken , takes time Unit into account ? __EoT__ Hi . Consider the [ ` BM_Complexity_O_N ` ] ( https : //github.com/google/benchmark/blob/36a251ab3ee4e6dc668129e0ef5fa62b1b8d5d49/test/complexity_test.cc # L80-L113 ) Vanilla version outputs : `` ` Benchmark Time CPU Iterations -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - BM_Complexity_O_N/1024 441 ns 441 ns 1583329 BM_Complexity_O_N/2k 881 ns 881 ns 797104 BM_Complexity_O_N/4k 1767 ns 1767 ns 395988 BM_Complexity_O_N/8k 3691 ns 3690 ns 189567 BM_Complexity_O_N/16k 7377 ns 7376 ns 95031 BM_Complexity_O_N/32k 14813 ns 14801 ns 47507 BM_Complexity_O_N/64k 29605 ns 29602 ns 23608 BM_Complexity_O_N_BigO 0.45 N 0.45 N BM_Complexity_O_N_RMS 0 % 0 % BM_Complexity_O_N/1024 446 ns 446 ns 1587672 BM_Complexity_O_N/2k 884 ns 884 ns 792614 BM_Complexity_O_N/4k 1787 ns 1786 ns 393225 BM_Complexity_O_N/8k 3684 ns 3683 ns 189791 BM_Complexity_O_N/16k 7373 ns 7369 ns 95181 BM_Complexity_O_N/32k 14716 ns 14711 ns 47732 BM_Complexity_O_N/64k 29714 ns 29707 ns 23656 BM_Complexity_O_N_BigO 0.45 f ( N ) 0.45 f ( N ) BM_Complexity_O_N_RMS 1 % 1 % BM_Complexity_O_N/1024 442 ns 442 ns 1589204 BM_Complexity_O_N/2k 885 ns 884 ns 788984 BM_Complexity_O_N/4k 1781 ns 1781 ns 390535 BM_Complexity_O_N/8k 3691 ns
RMS calculation seems broken , takes time Unit into account ? __EoT__ Hi . Consider the [ ` BM_Complexity_O_N ` ] ( https : //github.com/google/benchmark/blob/36a251ab3ee4e6dc668129e0ef5fa62b1b8d5d49/test/complexity_test.cc # L80-L113 ) Vanilla version outputs : `` ` Benchmark Time CPU Iterations -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - BM_Complexity_O_N/1024 441 ns 441 ns 1583329 BM_Complexity_O_N/2k 881 ns 881 ns 797104 BM_Complexity_O_N/4k 1767 ns 1767 ns 395988 BM_Complexity_O_N/8k 3691 ns 3690 ns 189567 BM_Complexity_O_N/16k 7377 ns 7376 ns 95031 BM_Complexity_O_N/32k 14813 ns 14801 ns 47507 BM_Complexity_O_N/64k 29605 ns 29602 ns 23608 BM_Complexity_O_N_BigO 0.45 N 0.45 N BM_Complexity_O_N_RMS 0 % 0 % BM_Complexity_O_N/1024 446 ns 446 ns 1587672 BM_Complexity_O_N/2k 884 ns 884 ns 792614 BM_Complexity_O_N/4k 1787 ns 1786 ns 393225 BM_Complexity_O_N/8k 3684 ns 3683 ns 189791 BM_Complexity_O_N/16k 7373 ns 7369 ns 95181 BM_Complexity_O_N/32k 14716 ns 14711 ns 47732 BM_Complexity_O_N/64k 29714 ns 29707 ns 23656 BM_Complexity_O_N_BigO 0.45 f ( N ) 0.45 f ( N ) BM_Complexity_O_N_RMS 1 % 1 % BM_Complexity_O_N/1024 442 ns 442 ns 1589204 BM_Complexity_O_N/2k 885 ns 884 ns 788984 BM_Complexity_O_N/4k 1781 ns 1781 ns 390535 BM_Complexity_O_N/8k 3691 ns
RMS calculation seems broken , takes time Unit into account ? __EoT__ Hi . Consider the [ ` BM_Complexity_O_N ` ] ( https : //github.com/google/benchmark/blob/36a251ab3ee4e6dc668129e0ef5fa62b1b8d5d49/test/complexity_test.cc # L80-L113 ) Vanilla version outputs : `` ` Benchmark Time CPU Iterations -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - BM_Complexity_O_N/1024 441 ns 441 ns 1583329 BM_Complexity_O_N/2k 881 ns 881 ns 797104 BM_Complexity_O_N/4k 1767 ns 1767 ns 395988 BM_Complexity_O_N/8k 3691 ns 3690 ns 189567 BM_Complexity_O_N/16k 7377 ns 7376 ns 95031 BM_Complexity_O_N/32k 14813 ns 14801 ns 47507 BM_Complexity_O_N/64k 29605 ns 29602 ns 23608 BM_Complexity_O_N_BigO 0.45 N 0.45 N BM_Complexity_O_N_RMS 0 % 0 % BM_Complexity_O_N/1024 446 ns 446 ns 1587672 BM_Complexity_O_N/2k 884 ns 884 ns 792614 BM_Complexity_O_N/4k 1787 ns 1786 ns 393225 BM_Complexity_O_N/8k 3684 ns 3683 ns 189791 BM_Complexity_O_N/16k 7373 ns 7369 ns 95181 BM_Complexity_O_N/32k 14716 ns 14711 ns 47732 BM_Complexity_O_N/64k 29714 ns 29707 ns 23656 BM_Complexity_O_N_BigO 0.45 f ( N ) 0.45 f ( N ) BM_Complexity_O_N_RMS 1 % 1 % BM_Complexity_O_N/1024 442 ns 442 ns 1589204 BM_Complexity_O_N/2k 885 ns 884 ns 788984 BM_Complexity_O_N/4k 1781 ns 1781 ns 390535 BM_Complexity_O_N/8k 3691 ns
RMS calculation seems broken , takes time Unit into account ? __EoT__ Hi . Consider the [ ` BM_Complexity_O_N ` ] ( https : //github.com/google/benchmark/blob/36a251ab3ee4e6dc668129e0ef5fa62b1b8d5d49/test/complexity_test.cc # L80-L113 ) Vanilla version outputs : `` ` Benchmark Time CPU Iterations -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - BM_Complexity_O_N/1024 441 ns 441 ns 1583329 BM_Complexity_O_N/2k 881 ns 881 ns 797104 BM_Complexity_O_N/4k 1767 ns 1767 ns 395988 BM_Complexity_O_N/8k 3691 ns 3690 ns 189567 BM_Complexity_O_N/16k 7377 ns 7376 ns 95031 BM_Complexity_O_N/32k 14813 ns 14801 ns 47507 BM_Complexity_O_N/64k 29605 ns 29602 ns 23608 BM_Complexity_O_N_BigO 0.45 N 0.45 N BM_Complexity_O_N_RMS 0 % 0 % BM_Complexity_O_N/1024 446 ns 446 ns 1587672 BM_Complexity_O_N/2k 884 ns 884 ns 792614 BM_Complexity_O_N/4k 1787 ns 1786 ns 393225 BM_Complexity_O_N/8k 3684 ns 3683 ns 189791 BM_Complexity_O_N/16k 7373 ns 7369 ns 95181 BM_Complexity_O_N/32k 14716 ns 14711 ns 47732 BM_Complexity_O_N/64k 29714 ns 29707 ns 23656 BM_Complexity_O_N_BigO 0.45 f ( N ) 0.45 f ( N ) BM_Complexity_O_N_RMS 1 % 1 % BM_Complexity_O_N/1024 442 ns 442 ns 1589204 BM_Complexity_O_N/2k 885 ns 884 ns 788984 BM_Complexity_O_N/4k 1781 ns 1781 ns 390535 BM_Complexity_O_N/8k 3691 ns
RMS calculation seems broken , takes time Unit into account ? __EoT__ Hi . Consider the [ ` BM_Complexity_O_N ` ] ( https : //github.com/google/benchmark/blob/36a251ab3ee4e6dc668129e0ef5fa62b1b8d5d49/test/complexity_test.cc # L80-L113 ) Vanilla version outputs : `` ` Benchmark Time CPU Iterations -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - BM_Complexity_O_N/1024 441 ns 441 ns 1583329 BM_Complexity_O_N/2k 881 ns 881 ns 797104 BM_Complexity_O_N/4k 1767 ns 1767 ns 395988 BM_Complexity_O_N/8k 3691 ns 3690 ns 189567 BM_Complexity_O_N/16k 7377 ns 7376 ns 95031 BM_Complexity_O_N/32k 14813 ns 14801 ns 47507 BM_Complexity_O_N/64k 29605 ns 29602 ns 23608 BM_Complexity_O_N_BigO 0.45 N 0.45 N BM_Complexity_O_N_RMS 0 % 0 % BM_Complexity_O_N/1024 446 ns 446 ns 1587672 BM_Complexity_O_N/2k 884 ns 884 ns 792614 BM_Complexity_O_N/4k 1787 ns 1786 ns 393225 BM_Complexity_O_N/8k 3684 ns 3683 ns 189791 BM_Complexity_O_N/16k 7373 ns 7369 ns 95181 BM_Complexity_O_N/32k 14716 ns 14711 ns 47732 BM_Complexity_O_N/64k 29714 ns 29707 ns 23656 BM_Complexity_O_N_BigO 0.45 f ( N ) 0.45 f ( N ) BM_Complexity_O_N_RMS 1 % 1 % BM_Complexity_O_N/1024 442 ns 442 ns 1589204 BM_Complexity_O_N/2k 885 ns 884 ns 788984 BM_Complexity_O_N/4k 1781 ns 1781 ns 390535 BM_Complexity_O_N/8k 3691 ns
RMS calculation seems broken , takes time Unit into account ? __EoT__ Hi . Consider the [ ` BM_Complexity_O_N ` ] ( https : //github.com/google/benchmark/blob/36a251ab3ee4e6dc668129e0ef5fa62b1b8d5d49/test/complexity_test.cc # L80-L113 ) Vanilla version outputs : `` ` Benchmark Time CPU Iterations -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - BM_Complexity_O_N/1024 441 ns 441 ns 1583329 BM_Complexity_O_N/2k 881 ns 881 ns 797104 BM_Complexity_O_N/4k 1767 ns 1767 ns 395988 BM_Complexity_O_N/8k 3691 ns 3690 ns 189567 BM_Complexity_O_N/16k 7377 ns 7376 ns 95031 BM_Complexity_O_N/32k 14813 ns 14801 ns 47507 BM_Complexity_O_N/64k 29605 ns 29602 ns 23608 BM_Complexity_O_N_BigO 0.45 N 0.45 N BM_Complexity_O_N_RMS 0 % 0 % BM_Complexity_O_N/1024 446 ns 446 ns 1587672 BM_Complexity_O_N/2k 884 ns 884 ns 792614 BM_Complexity_O_N/4k 1787 ns 1786 ns 393225 BM_Complexity_O_N/8k 3684 ns 3683 ns 189791 BM_Complexity_O_N/16k 7373 ns 7369 ns 95181 BM_Complexity_O_N/32k 14716 ns 14711 ns 47732 BM_Complexity_O_N/64k 29714 ns 29707 ns 23656 BM_Complexity_O_N_BigO 0.45 f ( N ) 0.45 f ( N ) BM_Complexity_O_N_RMS 1 % 1 % BM_Complexity_O_N/1024 442 ns 442 ns 1589204 BM_Complexity_O_N/2k 885 ns 884 ns 788984 BM_Complexity_O_N/4k 1781 ns 1781 ns 390535 BM_Complexity_O_N/8k 3691 ns
RMS calculation seems broken , takes time Unit into account ? __EoT__ Hi . Consider the [ ` BM_Complexity_O_N ` ] ( https : //github.com/google/benchmark/blob/36a251ab3ee4e6dc668129e0ef5fa62b1b8d5d49/test/complexity_test.cc # L80-L113 ) Vanilla version outputs : `` ` Benchmark Time CPU Iterations -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - BM_Complexity_O_N/1024 441 ns 441 ns 1583329 BM_Complexity_O_N/2k 881 ns 881 ns 797104 BM_Complexity_O_N/4k 1767 ns 1767 ns 395988 BM_Complexity_O_N/8k 3691 ns 3690 ns 189567 BM_Complexity_O_N/16k 7377 ns 7376 ns 95031 BM_Complexity_O_N/32k 14813 ns 14801 ns 47507 BM_Complexity_O_N/64k 29605 ns 29602 ns 23608 BM_Complexity_O_N_BigO 0.45 N 0.45 N BM_Complexity_O_N_RMS 0 % 0 % BM_Complexity_O_N/1024 446 ns 446 ns 1587672 BM_Complexity_O_N/2k 884 ns 884 ns 792614 BM_Complexity_O_N/4k 1787 ns 1786 ns 393225 BM_Complexity_O_N/8k 3684 ns 3683 ns 189791 BM_Complexity_O_N/16k 7373 ns 7369 ns 95181 BM_Complexity_O_N/32k 14716 ns 14711 ns 47732 BM_Complexity_O_N/64k 29714 ns 29707 ns 23656 BM_Complexity_O_N_BigO 0.45 f ( N ) 0.45 f ( N ) BM_Complexity_O_N_RMS 1 % 1 % BM_Complexity_O_N/1024 442 ns 442 ns 1589204 BM_Complexity_O_N/2k 885 ns 884 ns 788984 BM_Complexity_O_N/4k 1781 ns 1781 ns 390535 BM_Complexity_O_N/8k 3691 ns
RMS calculation seems broken , takes time Unit into account ? __EoT__ Hi . Consider the [ ` BM_Complexity_O_N ` ] ( https : //github.com/google/benchmark/blob/36a251ab3ee4e6dc668129e0ef5fa62b1b8d5d49/test/complexity_test.cc # L80-L113 ) Vanilla version outputs : `` ` Benchmark Time CPU Iterations -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - BM_Complexity_O_N/1024 441 ns 441 ns 1583329 BM_Complexity_O_N/2k 881 ns 881 ns 797104 BM_Complexity_O_N/4k 1767 ns 1767 ns 395988 BM_Complexity_O_N/8k 3691 ns 3690 ns 189567 BM_Complexity_O_N/16k 7377 ns 7376 ns 95031 BM_Complexity_O_N/32k 14813 ns 14801 ns 47507 BM_Complexity_O_N/64k 29605 ns 29602 ns 23608 BM_Complexity_O_N_BigO 0.45 N 0.45 N BM_Complexity_O_N_RMS 0 % 0 % BM_Complexity_O_N/1024 446 ns 446 ns 1587672 BM_Complexity_O_N/2k 884 ns 884 ns 792614 BM_Complexity_O_N/4k 1787 ns 1786 ns 393225 BM_Complexity_O_N/8k 3684 ns 3683 ns 189791 BM_Complexity_O_N/16k 7373 ns 7369 ns 95181 BM_Complexity_O_N/32k 14716 ns 14711 ns 47732 BM_Complexity_O_N/64k 29714 ns 29707 ns 23656 BM_Complexity_O_N_BigO 0.45 f ( N ) 0.45 f ( N ) BM_Complexity_O_N_RMS 1 % 1 % BM_Complexity_O_N/1024 442 ns 442 ns 1589204 BM_Complexity_O_N/2k 885 ns 884 ns 788984 BM_Complexity_O_N/4k 1781 ns 1781 ns 390535 BM_Complexity_O_N/8k 3691 ns
RMS calculation seems broken , takes time Unit into account ? __EoT__ Hi . Consider the [ ` BM_Complexity_O_N ` ] ( https : //github.com/google/benchmark/blob/36a251ab3ee4e6dc668129e0ef5fa62b1b8d5d49/test/complexity_test.cc # L80-L113 ) Vanilla version outputs : `` ` Benchmark Time CPU Iterations -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - BM_Complexity_O_N/1024 441 ns 441 ns 1583329 BM_Complexity_O_N/2k 881 ns 881 ns 797104 BM_Complexity_O_N/4k 1767 ns 1767 ns 395988 BM_Complexity_O_N/8k 3691 ns 3690 ns 189567 BM_Complexity_O_N/16k 7377 ns 7376 ns 95031 BM_Complexity_O_N/32k 14813 ns 14801 ns 47507 BM_Complexity_O_N/64k 29605 ns 29602 ns 23608 BM_Complexity_O_N_BigO 0.45 N 0.45 N BM_Complexity_O_N_RMS 0 % 0 % BM_Complexity_O_N/1024 446 ns 446 ns 1587672 BM_Complexity_O_N/2k 884 ns 884 ns 792614 BM_Complexity_O_N/4k 1787 ns 1786 ns 393225 BM_Complexity_O_N/8k 3684 ns 3683 ns 189791 BM_Complexity_O_N/16k 7373 ns 7369 ns 95181 BM_Complexity_O_N/32k 14716 ns 14711 ns 47732 BM_Complexity_O_N/64k 29714 ns 29707 ns 23656 BM_Complexity_O_N_BigO 0.45 f ( N ) 0.45 f ( N ) BM_Complexity_O_N_RMS 1 % 1 % BM_Complexity_O_N/1024 442 ns 442 ns 1589204 BM_Complexity_O_N/2k 885 ns 884 ns 788984 BM_Complexity_O_N/4k 1781 ns 1781 ns 390535 BM_Complexity_O_N/8k 3691 ns
RMS calculation seems broken , takes time Unit into account ? __EoT__ Hi . Consider the [ ` BM_Complexity_O_N ` ] ( https : //github.com/google/benchmark/blob/36a251ab3ee4e6dc668129e0ef5fa62b1b8d5d49/test/complexity_test.cc # L80-L113 ) Vanilla version outputs : `` ` Benchmark Time CPU Iterations -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - BM_Complexity_O_N/1024 441 ns 441 ns 1583329 BM_Complexity_O_N/2k 881 ns 881 ns 797104 BM_Complexity_O_N/4k 1767 ns 1767 ns 395988 BM_Complexity_O_N/8k 3691 ns 3690 ns 189567 BM_Complexity_O_N/16k 7377 ns 7376 ns 95031 BM_Complexity_O_N/32k 14813 ns 14801 ns 47507 BM_Complexity_O_N/64k 29605 ns 29602 ns 23608 BM_Complexity_O_N_BigO 0.45 N 0.45 N BM_Complexity_O_N_RMS 0 % 0 % BM_Complexity_O_N/1024 446 ns 446 ns 1587672 BM_Complexity_O_N/2k 884 ns 884 ns 792614 BM_Complexity_O_N/4k 1787 ns 1786 ns 393225 BM_Complexity_O_N/8k 3684 ns 3683 ns 189791 BM_Complexity_O_N/16k 7373 ns 7369 ns 95181 BM_Complexity_O_N/32k 14716 ns 14711 ns 47732 BM_Complexity_O_N/64k 29714 ns 29707 ns 23656 BM_Complexity_O_N_BigO 0.45 f ( N ) 0.45 f ( N ) BM_Complexity_O_N_RMS 1 % 1 % BM_Complexity_O_N/1024 442 ns 442 ns 1589204 BM_Complexity_O_N/2k 885 ns 884 ns 788984 BM_Complexity_O_N/4k 1781 ns 1781 ns 390535 BM_Complexity_O_N/8k 3691 ns
RMS calculation seems broken , takes time Unit into account ? __EoT__ Hi . Consider the [ ` BM_Complexity_O_N ` ] ( https : //github.com/google/benchmark/blob/36a251ab3ee4e6dc668129e0ef5fa62b1b8d5d49/test/complexity_test.cc # L80-L113 ) Vanilla version outputs : `` ` Benchmark Time CPU Iterations -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - BM_Complexity_O_N/1024 441 ns 441 ns 1583329 BM_Complexity_O_N/2k 881 ns 881 ns 797104 BM_Complexity_O_N/4k 1767 ns 1767 ns 395988 BM_Complexity_O_N/8k 3691 ns 3690 ns 189567 BM_Complexity_O_N/16k 7377 ns 7376 ns 95031 BM_Complexity_O_N/32k 14813 ns 14801 ns 47507 BM_Complexity_O_N/64k 29605 ns 29602 ns 23608 BM_Complexity_O_N_BigO 0.45 N 0.45 N BM_Complexity_O_N_RMS 0 % 0 % BM_Complexity_O_N/1024 446 ns 446 ns 1587672 BM_Complexity_O_N/2k 884 ns 884 ns 792614 BM_Complexity_O_N/4k 1787 ns 1786 ns 393225 BM_Complexity_O_N/8k 3684 ns 3683 ns 189791 BM_Complexity_O_N/16k 7373 ns 7369 ns 95181 BM_Complexity_O_N/32k 14716 ns 14711 ns 47732 BM_Complexity_O_N/64k 29714 ns 29707 ns 23656 BM_Complexity_O_N_BigO 0.45 f ( N ) 0.45 f ( N ) BM_Complexity_O_N_RMS 1 % 1 % BM_Complexity_O_N/1024 442 ns 442 ns 1589204 BM_Complexity_O_N/2k 885 ns 884 ns 788984 BM_Complexity_O_N/4k 1781 ns 1781 ns 390535 BM_Complexity_O_N/8k 3691 ns
RMS calculation seems broken , takes time Unit into account ? __EoT__ Hi . Consider the [ ` BM_Complexity_O_N ` ] ( https : //github.com/google/benchmark/blob/36a251ab3ee4e6dc668129e0ef5fa62b1b8d5d49/test/complexity_test.cc # L80-L113 ) Vanilla version outputs : `` ` Benchmark Time CPU Iterations -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - BM_Complexity_O_N/1024 441 ns 441 ns 1583329 BM_Complexity_O_N/2k 881 ns 881 ns 797104 BM_Complexity_O_N/4k 1767 ns 1767 ns 395988 BM_Complexity_O_N/8k 3691 ns 3690 ns 189567 BM_Complexity_O_N/16k 7377 ns 7376 ns 95031 BM_Complexity_O_N/32k 14813 ns 14801 ns 47507 BM_Complexity_O_N/64k 29605 ns 29602 ns 23608 BM_Complexity_O_N_BigO 0.45 N 0.45 N BM_Complexity_O_N_RMS 0 % 0 % BM_Complexity_O_N/1024 446 ns 446 ns 1587672 BM_Complexity_O_N/2k 884 ns 884 ns 792614 BM_Complexity_O_N/4k 1787 ns 1786 ns 393225 BM_Complexity_O_N/8k 3684 ns 3683 ns 189791 BM_Complexity_O_N/16k 7373 ns 7369 ns 95181 BM_Complexity_O_N/32k 14716 ns 14711 ns 47732 BM_Complexity_O_N/64k 29714 ns 29707 ns 23656 BM_Complexity_O_N_BigO 0.45 f ( N ) 0.45 f ( N ) BM_Complexity_O_N_RMS 1 % 1 % BM_Complexity_O_N/1024 442 ns 442 ns 1589204 BM_Complexity_O_N/2k 885 ns 884 ns 788984 BM_Complexity_O_N/4k 1781 ns 1781 ns 390535 BM_Complexity_O_N/8k 3691 ns
RMS calculation seems broken , takes time Unit into account ? __EoT__ Hi . Consider the [ ` BM_Complexity_O_N ` ] ( https : //github.com/google/benchmark/blob/36a251ab3ee4e6dc668129e0ef5fa62b1b8d5d49/test/complexity_test.cc # L80-L113 ) Vanilla version outputs : `` ` Benchmark Time CPU Iterations -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - BM_Complexity_O_N/1024 441 ns 441 ns 1583329 BM_Complexity_O_N/2k 881 ns 881 ns 797104 BM_Complexity_O_N/4k 1767 ns 1767 ns 395988 BM_Complexity_O_N/8k 3691 ns 3690 ns 189567 BM_Complexity_O_N/16k 7377 ns 7376 ns 95031 BM_Complexity_O_N/32k 14813 ns 14801 ns 47507 BM_Complexity_O_N/64k 29605 ns 29602 ns 23608 BM_Complexity_O_N_BigO 0.45 N 0.45 N BM_Complexity_O_N_RMS 0 % 0 % BM_Complexity_O_N/1024 446 ns 446 ns 1587672 BM_Complexity_O_N/2k 884 ns 884 ns 792614 BM_Complexity_O_N/4k 1787 ns 1786 ns 393225 BM_Complexity_O_N/8k 3684 ns 3683 ns 189791 BM_Complexity_O_N/16k 7373 ns 7369 ns 95181 BM_Complexity_O_N/32k 14716 ns 14711 ns 47732 BM_Complexity_O_N/64k 29714 ns 29707 ns 23656 BM_Complexity_O_N_BigO 0.45 f ( N ) 0.45 f ( N ) BM_Complexity_O_N_RMS 1 % 1 % BM_Complexity_O_N/1024 442 ns 442 ns 1589204 BM_Complexity_O_N/2k 885 ns 884 ns 788984 BM_Complexity_O_N/4k 1781 ns 1781 ns 390535 BM_Complexity_O_N/8k 3691 ns
benchmark : :DoNotOptimize does not compile on gcc4.8 + ppc64 __EoT__ I 'm trying to build one of my projects on a ppc64le system , and compilation of benchmarks fails on assembly inside ` benchmark : :DoNotOptimize ` : `` ` /home/mdukhan3/NNPACK/deps/fxdiv/deps/googlebenchmark/include/benchmark/benchmark_api.h:236:45 : error : can not reload integer constant operand in â€˜ asm â€™ asm volatile ( `` '' : : `` g '' ( value ) : `` memory '' ) ; `` ` Results for ` g++ -v ` : `` ` mdukhan3 @ power8 : ~/NNPACK/deps/fxdiv $ g++ -v Using built-in specs . COLLECT_GCC=g++ COLLECT_LTO_WRAPPER=/usr/lib/gcc/powerpc64le-linux-gnu/4.8/lto-wrapper Target : powerpc64le-linux-gnu Configured with : ../src/configure -v -- with-pkgversion='Ubuntu 4.8.4-2ubuntu1~14.04.3 ' -- with-bugurl=file : ///usr/share/doc/gcc-4.8/README.Bugs -- enable-languages=c , c++ , java , go , d , fortran , objc , obj-c++ -- prefix=/usr -- program-suffix=-4.8 -- enable-shared -- enable-linker-build-id -- libexecdir=/usr/lib -- without-included-gettext -- enable-threads=posix -- with-gxx-include-dir=/usr/include/c++/4.8 -- libdir=/usr/lib -- enable-nls -- with-sysroot=/ -- enable-clocale=gnu -- enable-libstdcxx-debug -- enable-libstdcxx-time=yes -- enable-gnu-unique-object -- disable-libmudflap -- disable-libsanitizer -- disable-libsanitizer -- disable-libquadmath -- enable-plugin -- with-system-zlib -- disable-browser-plugin -- enable-java-awt=gtk -- enable-gtk-cairo -- with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el/jre -- enable-java-home -- with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el -- with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-ppc64el -- with-arch-directory=ppc64el -- with-ecj-jar=/usr/share/java/eclipse-ecj.jar -- enable-objc-gc -- enable-secureplt -- with-cpu=power7 -- with-tune=power8 --
benchmark : :DoNotOptimize does not compile on gcc4.8 + ppc64 __EoT__ I 'm trying to build one of my projects on a ppc64le system , and compilation of benchmarks fails on assembly inside ` benchmark : :DoNotOptimize ` : `` ` /home/mdukhan3/NNPACK/deps/fxdiv/deps/googlebenchmark/include/benchmark/benchmark_api.h:236:45 : error : can not reload integer constant operand in â€˜ asm â€™ asm volatile ( `` '' : : `` g '' ( value ) : `` memory '' ) ; `` ` Results for ` g++ -v ` : `` ` mdukhan3 @ power8 : ~/NNPACK/deps/fxdiv $ g++ -v Using built-in specs . COLLECT_GCC=g++ COLLECT_LTO_WRAPPER=/usr/lib/gcc/powerpc64le-linux-gnu/4.8/lto-wrapper Target : powerpc64le-linux-gnu Configured with : ../src/configure -v -- with-pkgversion='Ubuntu 4.8.4-2ubuntu1~14.04.3 ' -- with-bugurl=file : ///usr/share/doc/gcc-4.8/README.Bugs -- enable-languages=c , c++ , java , go , d , fortran , objc , obj-c++ -- prefix=/usr -- program-suffix=-4.8 -- enable-shared -- enable-linker-build-id -- libexecdir=/usr/lib -- without-included-gettext -- enable-threads=posix -- with-gxx-include-dir=/usr/include/c++/4.8 -- libdir=/usr/lib -- enable-nls -- with-sysroot=/ -- enable-clocale=gnu -- enable-libstdcxx-debug -- enable-libstdcxx-time=yes -- enable-gnu-unique-object -- disable-libmudflap -- disable-libsanitizer -- disable-libsanitizer -- disable-libquadmath -- enable-plugin -- with-system-zlib -- disable-browser-plugin -- enable-java-awt=gtk -- enable-gtk-cairo -- with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el/jre -- enable-java-home -- with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el -- with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-ppc64el -- with-arch-directory=ppc64el -- with-ecj-jar=/usr/share/java/eclipse-ecj.jar -- enable-objc-gc -- enable-secureplt -- with-cpu=power7 -- with-tune=power8 --
benchmark : :DoNotOptimize does not compile on gcc4.8 + ppc64 __EoT__ I 'm trying to build one of my projects on a ppc64le system , and compilation of benchmarks fails on assembly inside ` benchmark : :DoNotOptimize ` : `` ` /home/mdukhan3/NNPACK/deps/fxdiv/deps/googlebenchmark/include/benchmark/benchmark_api.h:236:45 : error : can not reload integer constant operand in â€˜ asm â€™ asm volatile ( `` '' : : `` g '' ( value ) : `` memory '' ) ; `` ` Results for ` g++ -v ` : `` ` mdukhan3 @ power8 : ~/NNPACK/deps/fxdiv $ g++ -v Using built-in specs . COLLECT_GCC=g++ COLLECT_LTO_WRAPPER=/usr/lib/gcc/powerpc64le-linux-gnu/4.8/lto-wrapper Target : powerpc64le-linux-gnu Configured with : ../src/configure -v -- with-pkgversion='Ubuntu 4.8.4-2ubuntu1~14.04.3 ' -- with-bugurl=file : ///usr/share/doc/gcc-4.8/README.Bugs -- enable-languages=c , c++ , java , go , d , fortran , objc , obj-c++ -- prefix=/usr -- program-suffix=-4.8 -- enable-shared -- enable-linker-build-id -- libexecdir=/usr/lib -- without-included-gettext -- enable-threads=posix -- with-gxx-include-dir=/usr/include/c++/4.8 -- libdir=/usr/lib -- enable-nls -- with-sysroot=/ -- enable-clocale=gnu -- enable-libstdcxx-debug -- enable-libstdcxx-time=yes -- enable-gnu-unique-object -- disable-libmudflap -- disable-libsanitizer -- disable-libsanitizer -- disable-libquadmath -- enable-plugin -- with-system-zlib -- disable-browser-plugin -- enable-java-awt=gtk -- enable-gtk-cairo -- with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el/jre -- enable-java-home -- with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el -- with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-ppc64el -- with-arch-directory=ppc64el -- with-ecj-jar=/usr/share/java/eclipse-ecj.jar -- enable-objc-gc -- enable-secureplt -- with-cpu=power7 -- with-tune=power8 --
benchmark : :DoNotOptimize does not compile on gcc4.8 + ppc64 __EoT__ I 'm trying to build one of my projects on a ppc64le system , and compilation of benchmarks fails on assembly inside ` benchmark : :DoNotOptimize ` : `` ` /home/mdukhan3/NNPACK/deps/fxdiv/deps/googlebenchmark/include/benchmark/benchmark_api.h:236:45 : error : can not reload integer constant operand in â€˜ asm â€™ asm volatile ( `` '' : : `` g '' ( value ) : `` memory '' ) ; `` ` Results for ` g++ -v ` : `` ` mdukhan3 @ power8 : ~/NNPACK/deps/fxdiv $ g++ -v Using built-in specs . COLLECT_GCC=g++ COLLECT_LTO_WRAPPER=/usr/lib/gcc/powerpc64le-linux-gnu/4.8/lto-wrapper Target : powerpc64le-linux-gnu Configured with : ../src/configure -v -- with-pkgversion='Ubuntu 4.8.4-2ubuntu1~14.04.3 ' -- with-bugurl=file : ///usr/share/doc/gcc-4.8/README.Bugs -- enable-languages=c , c++ , java , go , d , fortran , objc , obj-c++ -- prefix=/usr -- program-suffix=-4.8 -- enable-shared -- enable-linker-build-id -- libexecdir=/usr/lib -- without-included-gettext -- enable-threads=posix -- with-gxx-include-dir=/usr/include/c++/4.8 -- libdir=/usr/lib -- enable-nls -- with-sysroot=/ -- enable-clocale=gnu -- enable-libstdcxx-debug -- enable-libstdcxx-time=yes -- enable-gnu-unique-object -- disable-libmudflap -- disable-libsanitizer -- disable-libsanitizer -- disable-libquadmath -- enable-plugin -- with-system-zlib -- disable-browser-plugin -- enable-java-awt=gtk -- enable-gtk-cairo -- with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el/jre -- enable-java-home -- with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el -- with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-ppc64el -- with-arch-directory=ppc64el -- with-ecj-jar=/usr/share/java/eclipse-ecj.jar -- enable-objc-gc -- enable-secureplt -- with-cpu=power7 -- with-tune=power8 --
benchmark : :DoNotOptimize does not compile on gcc4.8 + ppc64 __EoT__ I 'm trying to build one of my projects on a ppc64le system , and compilation of benchmarks fails on assembly inside ` benchmark : :DoNotOptimize ` : `` ` /home/mdukhan3/NNPACK/deps/fxdiv/deps/googlebenchmark/include/benchmark/benchmark_api.h:236:45 : error : can not reload integer constant operand in â€˜ asm â€™ asm volatile ( `` '' : : `` g '' ( value ) : `` memory '' ) ; `` ` Results for ` g++ -v ` : `` ` mdukhan3 @ power8 : ~/NNPACK/deps/fxdiv $ g++ -v Using built-in specs . COLLECT_GCC=g++ COLLECT_LTO_WRAPPER=/usr/lib/gcc/powerpc64le-linux-gnu/4.8/lto-wrapper Target : powerpc64le-linux-gnu Configured with : ../src/configure -v -- with-pkgversion='Ubuntu 4.8.4-2ubuntu1~14.04.3 ' -- with-bugurl=file : ///usr/share/doc/gcc-4.8/README.Bugs -- enable-languages=c , c++ , java , go , d , fortran , objc , obj-c++ -- prefix=/usr -- program-suffix=-4.8 -- enable-shared -- enable-linker-build-id -- libexecdir=/usr/lib -- without-included-gettext -- enable-threads=posix -- with-gxx-include-dir=/usr/include/c++/4.8 -- libdir=/usr/lib -- enable-nls -- with-sysroot=/ -- enable-clocale=gnu -- enable-libstdcxx-debug -- enable-libstdcxx-time=yes -- enable-gnu-unique-object -- disable-libmudflap -- disable-libsanitizer -- disable-libsanitizer -- disable-libquadmath -- enable-plugin -- with-system-zlib -- disable-browser-plugin -- enable-java-awt=gtk -- enable-gtk-cairo -- with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el/jre -- enable-java-home -- with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el -- with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-ppc64el -- with-arch-directory=ppc64el -- with-ecj-jar=/usr/share/java/eclipse-ecj.jar -- enable-objc-gc -- enable-secureplt -- with-cpu=power7 -- with-tune=power8 --
benchmark : :DoNotOptimize does not compile on gcc4.8 + ppc64 __EoT__ I 'm trying to build one of my projects on a ppc64le system , and compilation of benchmarks fails on assembly inside ` benchmark : :DoNotOptimize ` : `` ` /home/mdukhan3/NNPACK/deps/fxdiv/deps/googlebenchmark/include/benchmark/benchmark_api.h:236:45 : error : can not reload integer constant operand in â€˜ asm â€™ asm volatile ( `` '' : : `` g '' ( value ) : `` memory '' ) ; `` ` Results for ` g++ -v ` : `` ` mdukhan3 @ power8 : ~/NNPACK/deps/fxdiv $ g++ -v Using built-in specs . COLLECT_GCC=g++ COLLECT_LTO_WRAPPER=/usr/lib/gcc/powerpc64le-linux-gnu/4.8/lto-wrapper Target : powerpc64le-linux-gnu Configured with : ../src/configure -v -- with-pkgversion='Ubuntu 4.8.4-2ubuntu1~14.04.3 ' -- with-bugurl=file : ///usr/share/doc/gcc-4.8/README.Bugs -- enable-languages=c , c++ , java , go , d , fortran , objc , obj-c++ -- prefix=/usr -- program-suffix=-4.8 -- enable-shared -- enable-linker-build-id -- libexecdir=/usr/lib -- without-included-gettext -- enable-threads=posix -- with-gxx-include-dir=/usr/include/c++/4.8 -- libdir=/usr/lib -- enable-nls -- with-sysroot=/ -- enable-clocale=gnu -- enable-libstdcxx-debug -- enable-libstdcxx-time=yes -- enable-gnu-unique-object -- disable-libmudflap -- disable-libsanitizer -- disable-libsanitizer -- disable-libquadmath -- enable-plugin -- with-system-zlib -- disable-browser-plugin -- enable-java-awt=gtk -- enable-gtk-cairo -- with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el/jre -- enable-java-home -- with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el -- with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-ppc64el -- with-arch-directory=ppc64el -- with-ecj-jar=/usr/share/java/eclipse-ecj.jar -- enable-objc-gc -- enable-secureplt -- with-cpu=power7 -- with-tune=power8 --
benchmark : :DoNotOptimize does not compile on gcc4.8 + ppc64 __EoT__ I 'm trying to build one of my projects on a ppc64le system , and compilation of benchmarks fails on assembly inside ` benchmark : :DoNotOptimize ` : `` ` /home/mdukhan3/NNPACK/deps/fxdiv/deps/googlebenchmark/include/benchmark/benchmark_api.h:236:45 : error : can not reload integer constant operand in â€˜ asm â€™ asm volatile ( `` '' : : `` g '' ( value ) : `` memory '' ) ; `` ` Results for ` g++ -v ` : `` ` mdukhan3 @ power8 : ~/NNPACK/deps/fxdiv $ g++ -v Using built-in specs . COLLECT_GCC=g++ COLLECT_LTO_WRAPPER=/usr/lib/gcc/powerpc64le-linux-gnu/4.8/lto-wrapper Target : powerpc64le-linux-gnu Configured with : ../src/configure -v -- with-pkgversion='Ubuntu 4.8.4-2ubuntu1~14.04.3 ' -- with-bugurl=file : ///usr/share/doc/gcc-4.8/README.Bugs -- enable-languages=c , c++ , java , go , d , fortran , objc , obj-c++ -- prefix=/usr -- program-suffix=-4.8 -- enable-shared -- enable-linker-build-id -- libexecdir=/usr/lib -- without-included-gettext -- enable-threads=posix -- with-gxx-include-dir=/usr/include/c++/4.8 -- libdir=/usr/lib -- enable-nls -- with-sysroot=/ -- enable-clocale=gnu -- enable-libstdcxx-debug -- enable-libstdcxx-time=yes -- enable-gnu-unique-object -- disable-libmudflap -- disable-libsanitizer -- disable-libsanitizer -- disable-libquadmath -- enable-plugin -- with-system-zlib -- disable-browser-plugin -- enable-java-awt=gtk -- enable-gtk-cairo -- with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el/jre -- enable-java-home -- with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el -- with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-ppc64el -- with-arch-directory=ppc64el -- with-ecj-jar=/usr/share/java/eclipse-ecj.jar -- enable-objc-gc -- enable-secureplt -- with-cpu=power7 -- with-tune=power8 --
benchmark : :DoNotOptimize does not compile on gcc4.8 + ppc64 __EoT__ I 'm trying to build one of my projects on a ppc64le system , and compilation of benchmarks fails on assembly inside ` benchmark : :DoNotOptimize ` : `` ` /home/mdukhan3/NNPACK/deps/fxdiv/deps/googlebenchmark/include/benchmark/benchmark_api.h:236:45 : error : can not reload integer constant operand in â€˜ asm â€™ asm volatile ( `` '' : : `` g '' ( value ) : `` memory '' ) ; `` ` Results for ` g++ -v ` : `` ` mdukhan3 @ power8 : ~/NNPACK/deps/fxdiv $ g++ -v Using built-in specs . COLLECT_GCC=g++ COLLECT_LTO_WRAPPER=/usr/lib/gcc/powerpc64le-linux-gnu/4.8/lto-wrapper Target : powerpc64le-linux-gnu Configured with : ../src/configure -v -- with-pkgversion='Ubuntu 4.8.4-2ubuntu1~14.04.3 ' -- with-bugurl=file : ///usr/share/doc/gcc-4.8/README.Bugs -- enable-languages=c , c++ , java , go , d , fortran , objc , obj-c++ -- prefix=/usr -- program-suffix=-4.8 -- enable-shared -- enable-linker-build-id -- libexecdir=/usr/lib -- without-included-gettext -- enable-threads=posix -- with-gxx-include-dir=/usr/include/c++/4.8 -- libdir=/usr/lib -- enable-nls -- with-sysroot=/ -- enable-clocale=gnu -- enable-libstdcxx-debug -- enable-libstdcxx-time=yes -- enable-gnu-unique-object -- disable-libmudflap -- disable-libsanitizer -- disable-libsanitizer -- disable-libquadmath -- enable-plugin -- with-system-zlib -- disable-browser-plugin -- enable-java-awt=gtk -- enable-gtk-cairo -- with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el/jre -- enable-java-home -- with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el -- with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-ppc64el -- with-arch-directory=ppc64el -- with-ecj-jar=/usr/share/java/eclipse-ecj.jar -- enable-objc-gc -- enable-secureplt -- with-cpu=power7 -- with-tune=power8 --
benchmark : :DoNotOptimize does not compile on gcc4.8 + ppc64 __EoT__ I 'm trying to build one of my projects on a ppc64le system , and compilation of benchmarks fails on assembly inside ` benchmark : :DoNotOptimize ` : `` ` /home/mdukhan3/NNPACK/deps/fxdiv/deps/googlebenchmark/include/benchmark/benchmark_api.h:236:45 : error : can not reload integer constant operand in â€˜ asm â€™ asm volatile ( `` '' : : `` g '' ( value ) : `` memory '' ) ; `` ` Results for ` g++ -v ` : `` ` mdukhan3 @ power8 : ~/NNPACK/deps/fxdiv $ g++ -v Using built-in specs . COLLECT_GCC=g++ COLLECT_LTO_WRAPPER=/usr/lib/gcc/powerpc64le-linux-gnu/4.8/lto-wrapper Target : powerpc64le-linux-gnu Configured with : ../src/configure -v -- with-pkgversion='Ubuntu 4.8.4-2ubuntu1~14.04.3 ' -- with-bugurl=file : ///usr/share/doc/gcc-4.8/README.Bugs -- enable-languages=c , c++ , java , go , d , fortran , objc , obj-c++ -- prefix=/usr -- program-suffix=-4.8 -- enable-shared -- enable-linker-build-id -- libexecdir=/usr/lib -- without-included-gettext -- enable-threads=posix -- with-gxx-include-dir=/usr/include/c++/4.8 -- libdir=/usr/lib -- enable-nls -- with-sysroot=/ -- enable-clocale=gnu -- enable-libstdcxx-debug -- enable-libstdcxx-time=yes -- enable-gnu-unique-object -- disable-libmudflap -- disable-libsanitizer -- disable-libsanitizer -- disable-libquadmath -- enable-plugin -- with-system-zlib -- disable-browser-plugin -- enable-java-awt=gtk -- enable-gtk-cairo -- with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el/jre -- enable-java-home -- with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el -- with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-ppc64el -- with-arch-directory=ppc64el -- with-ecj-jar=/usr/share/java/eclipse-ecj.jar -- enable-objc-gc -- enable-secureplt -- with-cpu=power7 -- with-tune=power8 --
benchmark : :DoNotOptimize does not compile on gcc4.8 + ppc64 __EoT__ I 'm trying to build one of my projects on a ppc64le system , and compilation of benchmarks fails on assembly inside ` benchmark : :DoNotOptimize ` : `` ` /home/mdukhan3/NNPACK/deps/fxdiv/deps/googlebenchmark/include/benchmark/benchmark_api.h:236:45 : error : can not reload integer constant operand in â€˜ asm â€™ asm volatile ( `` '' : : `` g '' ( value ) : `` memory '' ) ; `` ` Results for ` g++ -v ` : `` ` mdukhan3 @ power8 : ~/NNPACK/deps/fxdiv $ g++ -v Using built-in specs . COLLECT_GCC=g++ COLLECT_LTO_WRAPPER=/usr/lib/gcc/powerpc64le-linux-gnu/4.8/lto-wrapper Target : powerpc64le-linux-gnu Configured with : ../src/configure -v -- with-pkgversion='Ubuntu 4.8.4-2ubuntu1~14.04.3 ' -- with-bugurl=file : ///usr/share/doc/gcc-4.8/README.Bugs -- enable-languages=c , c++ , java , go , d , fortran , objc , obj-c++ -- prefix=/usr -- program-suffix=-4.8 -- enable-shared -- enable-linker-build-id -- libexecdir=/usr/lib -- without-included-gettext -- enable-threads=posix -- with-gxx-include-dir=/usr/include/c++/4.8 -- libdir=/usr/lib -- enable-nls -- with-sysroot=/ -- enable-clocale=gnu -- enable-libstdcxx-debug -- enable-libstdcxx-time=yes -- enable-gnu-unique-object -- disable-libmudflap -- disable-libsanitizer -- disable-libsanitizer -- disable-libquadmath -- enable-plugin -- with-system-zlib -- disable-browser-plugin -- enable-java-awt=gtk -- enable-gtk-cairo -- with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el/jre -- enable-java-home -- with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el -- with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-ppc64el -- with-arch-directory=ppc64el -- with-ecj-jar=/usr/share/java/eclipse-ecj.jar -- enable-objc-gc -- enable-secureplt -- with-cpu=power7 -- with-tune=power8 --
benchmark : :DoNotOptimize does not compile on gcc4.8 + ppc64 __EoT__ I 'm trying to build one of my projects on a ppc64le system , and compilation of benchmarks fails on assembly inside ` benchmark : :DoNotOptimize ` : `` ` /home/mdukhan3/NNPACK/deps/fxdiv/deps/googlebenchmark/include/benchmark/benchmark_api.h:236:45 : error : can not reload integer constant operand in â€˜ asm â€™ asm volatile ( `` '' : : `` g '' ( value ) : `` memory '' ) ; `` ` Results for ` g++ -v ` : `` ` mdukhan3 @ power8 : ~/NNPACK/deps/fxdiv $ g++ -v Using built-in specs . COLLECT_GCC=g++ COLLECT_LTO_WRAPPER=/usr/lib/gcc/powerpc64le-linux-gnu/4.8/lto-wrapper Target : powerpc64le-linux-gnu Configured with : ../src/configure -v -- with-pkgversion='Ubuntu 4.8.4-2ubuntu1~14.04.3 ' -- with-bugurl=file : ///usr/share/doc/gcc-4.8/README.Bugs -- enable-languages=c , c++ , java , go , d , fortran , objc , obj-c++ -- prefix=/usr -- program-suffix=-4.8 -- enable-shared -- enable-linker-build-id -- libexecdir=/usr/lib -- without-included-gettext -- enable-threads=posix -- with-gxx-include-dir=/usr/include/c++/4.8 -- libdir=/usr/lib -- enable-nls -- with-sysroot=/ -- enable-clocale=gnu -- enable-libstdcxx-debug -- enable-libstdcxx-time=yes -- enable-gnu-unique-object -- disable-libmudflap -- disable-libsanitizer -- disable-libsanitizer -- disable-libquadmath -- enable-plugin -- with-system-zlib -- disable-browser-plugin -- enable-java-awt=gtk -- enable-gtk-cairo -- with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el/jre -- enable-java-home -- with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el -- with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-ppc64el -- with-arch-directory=ppc64el -- with-ecj-jar=/usr/share/java/eclipse-ecj.jar -- enable-objc-gc -- enable-secureplt -- with-cpu=power7 -- with-tune=power8 --
benchmark : :DoNotOptimize does not compile on gcc4.8 + ppc64 __EoT__ I 'm trying to build one of my projects on a ppc64le system , and compilation of benchmarks fails on assembly inside ` benchmark : :DoNotOptimize ` : `` ` /home/mdukhan3/NNPACK/deps/fxdiv/deps/googlebenchmark/include/benchmark/benchmark_api.h:236:45 : error : can not reload integer constant operand in â€˜ asm â€™ asm volatile ( `` '' : : `` g '' ( value ) : `` memory '' ) ; `` ` Results for ` g++ -v ` : `` ` mdukhan3 @ power8 : ~/NNPACK/deps/fxdiv $ g++ -v Using built-in specs . COLLECT_GCC=g++ COLLECT_LTO_WRAPPER=/usr/lib/gcc/powerpc64le-linux-gnu/4.8/lto-wrapper Target : powerpc64le-linux-gnu Configured with : ../src/configure -v -- with-pkgversion='Ubuntu 4.8.4-2ubuntu1~14.04.3 ' -- with-bugurl=file : ///usr/share/doc/gcc-4.8/README.Bugs -- enable-languages=c , c++ , java , go , d , fortran , objc , obj-c++ -- prefix=/usr -- program-suffix=-4.8 -- enable-shared -- enable-linker-build-id -- libexecdir=/usr/lib -- without-included-gettext -- enable-threads=posix -- with-gxx-include-dir=/usr/include/c++/4.8 -- libdir=/usr/lib -- enable-nls -- with-sysroot=/ -- enable-clocale=gnu -- enable-libstdcxx-debug -- enable-libstdcxx-time=yes -- enable-gnu-unique-object -- disable-libmudflap -- disable-libsanitizer -- disable-libsanitizer -- disable-libquadmath -- enable-plugin -- with-system-zlib -- disable-browser-plugin -- enable-java-awt=gtk -- enable-gtk-cairo -- with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el/jre -- enable-java-home -- with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el -- with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-ppc64el -- with-arch-directory=ppc64el -- with-ecj-jar=/usr/share/java/eclipse-ecj.jar -- enable-objc-gc -- enable-secureplt -- with-cpu=power7 -- with-tune=power8 --
benchmark : :DoNotOptimize does not compile on gcc4.8 + ppc64 __EoT__ I 'm trying to build one of my projects on a ppc64le system , and compilation of benchmarks fails on assembly inside ` benchmark : :DoNotOptimize ` : `` ` /home/mdukhan3/NNPACK/deps/fxdiv/deps/googlebenchmark/include/benchmark/benchmark_api.h:236:45 : error : can not reload integer constant operand in â€˜ asm â€™ asm volatile ( `` '' : : `` g '' ( value ) : `` memory '' ) ; `` ` Results for ` g++ -v ` : `` ` mdukhan3 @ power8 : ~/NNPACK/deps/fxdiv $ g++ -v Using built-in specs . COLLECT_GCC=g++ COLLECT_LTO_WRAPPER=/usr/lib/gcc/powerpc64le-linux-gnu/4.8/lto-wrapper Target : powerpc64le-linux-gnu Configured with : ../src/configure -v -- with-pkgversion='Ubuntu 4.8.4-2ubuntu1~14.04.3 ' -- with-bugurl=file : ///usr/share/doc/gcc-4.8/README.Bugs -- enable-languages=c , c++ , java , go , d , fortran , objc , obj-c++ -- prefix=/usr -- program-suffix=-4.8 -- enable-shared -- enable-linker-build-id -- libexecdir=/usr/lib -- without-included-gettext -- enable-threads=posix -- with-gxx-include-dir=/usr/include/c++/4.8 -- libdir=/usr/lib -- enable-nls -- with-sysroot=/ -- enable-clocale=gnu -- enable-libstdcxx-debug -- enable-libstdcxx-time=yes -- enable-gnu-unique-object -- disable-libmudflap -- disable-libsanitizer -- disable-libsanitizer -- disable-libquadmath -- enable-plugin -- with-system-zlib -- disable-browser-plugin -- enable-java-awt=gtk -- enable-gtk-cairo -- with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el/jre -- enable-java-home -- with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el -- with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-ppc64el -- with-arch-directory=ppc64el -- with-ecj-jar=/usr/share/java/eclipse-ecj.jar -- enable-objc-gc -- enable-secureplt -- with-cpu=power7 -- with-tune=power8 --
benchmark : :DoNotOptimize does not compile on gcc4.8 + ppc64 __EoT__ I 'm trying to build one of my projects on a ppc64le system , and compilation of benchmarks fails on assembly inside ` benchmark : :DoNotOptimize ` : `` ` /home/mdukhan3/NNPACK/deps/fxdiv/deps/googlebenchmark/include/benchmark/benchmark_api.h:236:45 : error : can not reload integer constant operand in â€˜ asm â€™ asm volatile ( `` '' : : `` g '' ( value ) : `` memory '' ) ; `` ` Results for ` g++ -v ` : `` ` mdukhan3 @ power8 : ~/NNPACK/deps/fxdiv $ g++ -v Using built-in specs . COLLECT_GCC=g++ COLLECT_LTO_WRAPPER=/usr/lib/gcc/powerpc64le-linux-gnu/4.8/lto-wrapper Target : powerpc64le-linux-gnu Configured with : ../src/configure -v -- with-pkgversion='Ubuntu 4.8.4-2ubuntu1~14.04.3 ' -- with-bugurl=file : ///usr/share/doc/gcc-4.8/README.Bugs -- enable-languages=c , c++ , java , go , d , fortran , objc , obj-c++ -- prefix=/usr -- program-suffix=-4.8 -- enable-shared -- enable-linker-build-id -- libexecdir=/usr/lib -- without-included-gettext -- enable-threads=posix -- with-gxx-include-dir=/usr/include/c++/4.8 -- libdir=/usr/lib -- enable-nls -- with-sysroot=/ -- enable-clocale=gnu -- enable-libstdcxx-debug -- enable-libstdcxx-time=yes -- enable-gnu-unique-object -- disable-libmudflap -- disable-libsanitizer -- disable-libsanitizer -- disable-libquadmath -- enable-plugin -- with-system-zlib -- disable-browser-plugin -- enable-java-awt=gtk -- enable-gtk-cairo -- with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el/jre -- enable-java-home -- with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el -- with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-ppc64el -- with-arch-directory=ppc64el -- with-ecj-jar=/usr/share/java/eclipse-ecj.jar -- enable-objc-gc -- enable-secureplt -- with-cpu=power7 -- with-tune=power8 --
benchmark : :DoNotOptimize does not compile on gcc4.8 + ppc64 __EoT__ I 'm trying to build one of my projects on a ppc64le system , and compilation of benchmarks fails on assembly inside ` benchmark : :DoNotOptimize ` : `` ` /home/mdukhan3/NNPACK/deps/fxdiv/deps/googlebenchmark/include/benchmark/benchmark_api.h:236:45 : error : can not reload integer constant operand in â€˜ asm â€™ asm volatile ( `` '' : : `` g '' ( value ) : `` memory '' ) ; `` ` Results for ` g++ -v ` : `` ` mdukhan3 @ power8 : ~/NNPACK/deps/fxdiv $ g++ -v Using built-in specs . COLLECT_GCC=g++ COLLECT_LTO_WRAPPER=/usr/lib/gcc/powerpc64le-linux-gnu/4.8/lto-wrapper Target : powerpc64le-linux-gnu Configured with : ../src/configure -v -- with-pkgversion='Ubuntu 4.8.4-2ubuntu1~14.04.3 ' -- with-bugurl=file : ///usr/share/doc/gcc-4.8/README.Bugs -- enable-languages=c , c++ , java , go , d , fortran , objc , obj-c++ -- prefix=/usr -- program-suffix=-4.8 -- enable-shared -- enable-linker-build-id -- libexecdir=/usr/lib -- without-included-gettext -- enable-threads=posix -- with-gxx-include-dir=/usr/include/c++/4.8 -- libdir=/usr/lib -- enable-nls -- with-sysroot=/ -- enable-clocale=gnu -- enable-libstdcxx-debug -- enable-libstdcxx-time=yes -- enable-gnu-unique-object -- disable-libmudflap -- disable-libsanitizer -- disable-libsanitizer -- disable-libquadmath -- enable-plugin -- with-system-zlib -- disable-browser-plugin -- enable-java-awt=gtk -- enable-gtk-cairo -- with-java-home=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el/jre -- enable-java-home -- with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-4.8-ppc64el -- with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-4.8-ppc64el -- with-arch-directory=ppc64el -- with-ecj-jar=/usr/share/java/eclipse-ecj.jar -- enable-objc-gc -- enable-secureplt -- with-cpu=power7 -- with-tune=power8 --
Google Benchmark should provide scripts to compare/process benchmark results . __EoT__ Currently the library makes it super easy to write and run benchmarks on code , but that 's only half the point of benchmarking code . Once you have the results you usually want to compare them to previous or different results . I propose that Google Benchmark should add scripts that allow the output of benchmarks to be easily compared and more . I imagine the usage to look something like : `` ` ./bench_before.out -- benchmark_format=json > /tmp/out1.json ./bench_after.out -- benchmark_format=json > /tmp/out2.json ./benchmark/benchmark_diff.py /tmp/out1.json /tmp/out2.json `` ` And the output would be something like `` ` BM_foo/1024 0.10 # bench_after is 10 % faster than bench_before BM_bar/5012 -5.0 # bench_before is 5x faster than bench_after `` ` This is just the basic idea of what I would like the library to provide . Additional utilities to process benchmark output would be great as well , including : - Graphing multiple runs over time . - Utilities for reporting performance regressions . I think it would make sense to write these utilities in python , but I 'm open to suggestions .
Google Benchmark should provide scripts to compare/process benchmark results . __EoT__ Currently the library makes it super easy to write and run benchmarks on code , but that 's only half the point of benchmarking code . Once you have the results you usually want to compare them to previous or different results . I propose that Google Benchmark should add scripts that allow the output of benchmarks to be easily compared and more . I imagine the usage to look something like : `` ` ./bench_before.out -- benchmark_format=json > /tmp/out1.json ./bench_after.out -- benchmark_format=json > /tmp/out2.json ./benchmark/benchmark_diff.py /tmp/out1.json /tmp/out2.json `` ` And the output would be something like `` ` BM_foo/1024 0.10 # bench_after is 10 % faster than bench_before BM_bar/5012 -5.0 # bench_before is 5x faster than bench_after `` ` This is just the basic idea of what I would like the library to provide . Additional utilities to process benchmark output would be great as well , including : - Graphing multiple runs over time . - Utilities for reporting performance regressions . I think it would make sense to write these utilities in python , but I 'm open to suggestions .
Google Benchmark should provide scripts to compare/process benchmark results . __EoT__ Currently the library makes it super easy to write and run benchmarks on code , but that 's only half the point of benchmarking code . Once you have the results you usually want to compare them to previous or different results . I propose that Google Benchmark should add scripts that allow the output of benchmarks to be easily compared and more . I imagine the usage to look something like : `` ` ./bench_before.out -- benchmark_format=json > /tmp/out1.json ./bench_after.out -- benchmark_format=json > /tmp/out2.json ./benchmark/benchmark_diff.py /tmp/out1.json /tmp/out2.json `` ` And the output would be something like `` ` BM_foo/1024 0.10 # bench_after is 10 % faster than bench_before BM_bar/5012 -5.0 # bench_before is 5x faster than bench_after `` ` This is just the basic idea of what I would like the library to provide . Additional utilities to process benchmark output would be great as well , including : - Graphing multiple runs over time . - Utilities for reporting performance regressions . I think it would make sense to write these utilities in python , but I 'm open to suggestions .
Google Benchmark should provide scripts to compare/process benchmark results . __EoT__ Currently the library makes it super easy to write and run benchmarks on code , but that 's only half the point of benchmarking code . Once you have the results you usually want to compare them to previous or different results . I propose that Google Benchmark should add scripts that allow the output of benchmarks to be easily compared and more . I imagine the usage to look something like : `` ` ./bench_before.out -- benchmark_format=json > /tmp/out1.json ./bench_after.out -- benchmark_format=json > /tmp/out2.json ./benchmark/benchmark_diff.py /tmp/out1.json /tmp/out2.json `` ` And the output would be something like `` ` BM_foo/1024 0.10 # bench_after is 10 % faster than bench_before BM_bar/5012 -5.0 # bench_before is 5x faster than bench_after `` ` This is just the basic idea of what I would like the library to provide . Additional utilities to process benchmark output would be great as well , including : - Graphing multiple runs over time . - Utilities for reporting performance regressions . I think it would make sense to write these utilities in python , but I 'm open to suggestions .
Google Benchmark should provide scripts to compare/process benchmark results . __EoT__ Currently the library makes it super easy to write and run benchmarks on code , but that 's only half the point of benchmarking code . Once you have the results you usually want to compare them to previous or different results . I propose that Google Benchmark should add scripts that allow the output of benchmarks to be easily compared and more . I imagine the usage to look something like : `` ` ./bench_before.out -- benchmark_format=json > /tmp/out1.json ./bench_after.out -- benchmark_format=json > /tmp/out2.json ./benchmark/benchmark_diff.py /tmp/out1.json /tmp/out2.json `` ` And the output would be something like `` ` BM_foo/1024 0.10 # bench_after is 10 % faster than bench_before BM_bar/5012 -5.0 # bench_before is 5x faster than bench_after `` ` This is just the basic idea of what I would like the library to provide . Additional utilities to process benchmark output would be great as well , including : - Graphing multiple runs over time . - Utilities for reporting performance regressions . I think it would make sense to write these utilities in python , but I 'm open to suggestions .
Report/error unrecognized command line flags __EoT__ Are there any use cases where additional flags that are not recognized by ` ParseCommandLineFlags ( ) ` are handled from another context ? I would prefer if , by default , benchmark would report unrecognized command line flags . Something like `` ` cpp // benchmark.cc : ParseCommandLineFlags ( ) if ( ... ) { // ... } else if ( IsFlag ( argv [ i ] , `` help '' ) ) { PrintUsageAndExit ( ) ; } else { fprintf ( stdout , `` unrecognized command line flag : % s\n '' , argv [ i ] ) ; exit ( 1 ) ; } `` `
Report/error unrecognized command line flags __EoT__ Are there any use cases where additional flags that are not recognized by ` ParseCommandLineFlags ( ) ` are handled from another context ? I would prefer if , by default , benchmark would report unrecognized command line flags . Something like `` ` cpp // benchmark.cc : ParseCommandLineFlags ( ) if ( ... ) { // ... } else if ( IsFlag ( argv [ i ] , `` help '' ) ) { PrintUsageAndExit ( ) ; } else { fprintf ( stdout , `` unrecognized command line flag : % s\n '' , argv [ i ] ) ; exit ( 1 ) ; } `` `
Report/error unrecognized command line flags __EoT__ Are there any use cases where additional flags that are not recognized by ` ParseCommandLineFlags ( ) ` are handled from another context ? I would prefer if , by default , benchmark would report unrecognized command line flags . Something like `` ` cpp // benchmark.cc : ParseCommandLineFlags ( ) if ( ... ) { // ... } else if ( IsFlag ( argv [ i ] , `` help '' ) ) { PrintUsageAndExit ( ) ; } else { fprintf ( stdout , `` unrecognized command line flag : % s\n '' , argv [ i ] ) ; exit ( 1 ) ; } `` `
Report/error unrecognized command line flags __EoT__ Are there any use cases where additional flags that are not recognized by ` ParseCommandLineFlags ( ) ` are handled from another context ? I would prefer if , by default , benchmark would report unrecognized command line flags . Something like `` ` cpp // benchmark.cc : ParseCommandLineFlags ( ) if ( ... ) { // ... } else if ( IsFlag ( argv [ i ] , `` help '' ) ) { PrintUsageAndExit ( ) ; } else { fprintf ( stdout , `` unrecognized command line flag : % s\n '' , argv [ i ] ) ; exit ( 1 ) ; } `` `
Report/error unrecognized command line flags __EoT__ Are there any use cases where additional flags that are not recognized by ` ParseCommandLineFlags ( ) ` are handled from another context ? I would prefer if , by default , benchmark would report unrecognized command line flags . Something like `` ` cpp // benchmark.cc : ParseCommandLineFlags ( ) if ( ... ) { // ... } else if ( IsFlag ( argv [ i ] , `` help '' ) ) { PrintUsageAndExit ( ) ; } else { fprintf ( stdout , `` unrecognized command line flag : % s\n '' , argv [ i ] ) ; exit ( 1 ) ; } `` `
Report/error unrecognized command line flags __EoT__ Are there any use cases where additional flags that are not recognized by ` ParseCommandLineFlags ( ) ` are handled from another context ? I would prefer if , by default , benchmark would report unrecognized command line flags . Something like `` ` cpp // benchmark.cc : ParseCommandLineFlags ( ) if ( ... ) { // ... } else if ( IsFlag ( argv [ i ] , `` help '' ) ) { PrintUsageAndExit ( ) ; } else { fprintf ( stdout , `` unrecognized command line flag : % s\n '' , argv [ i ] ) ; exit ( 1 ) ; } `` `
Report/error unrecognized command line flags __EoT__ Are there any use cases where additional flags that are not recognized by ` ParseCommandLineFlags ( ) ` are handled from another context ? I would prefer if , by default , benchmark would report unrecognized command line flags . Something like `` ` cpp // benchmark.cc : ParseCommandLineFlags ( ) if ( ... ) { // ... } else if ( IsFlag ( argv [ i ] , `` help '' ) ) { PrintUsageAndExit ( ) ; } else { fprintf ( stdout , `` unrecognized command line flag : % s\n '' , argv [ i ] ) ; exit ( 1 ) ; } `` `
Report/error unrecognized command line flags __EoT__ Are there any use cases where additional flags that are not recognized by ` ParseCommandLineFlags ( ) ` are handled from another context ? I would prefer if , by default , benchmark would report unrecognized command line flags . Something like `` ` cpp // benchmark.cc : ParseCommandLineFlags ( ) if ( ... ) { // ... } else if ( IsFlag ( argv [ i ] , `` help '' ) ) { PrintUsageAndExit ( ) ; } else { fprintf ( stdout , `` unrecognized command line flag : % s\n '' , argv [ i ] ) ; exit ( 1 ) ; } `` `
Report/error unrecognized command line flags __EoT__ Are there any use cases where additional flags that are not recognized by ` ParseCommandLineFlags ( ) ` are handled from another context ? I would prefer if , by default , benchmark would report unrecognized command line flags . Something like `` ` cpp // benchmark.cc : ParseCommandLineFlags ( ) if ( ... ) { // ... } else if ( IsFlag ( argv [ i ] , `` help '' ) ) { PrintUsageAndExit ( ) ; } else { fprintf ( stdout , `` unrecognized command line flag : % s\n '' , argv [ i ] ) ; exit ( 1 ) ; } `` `
Report/error unrecognized command line flags __EoT__ Are there any use cases where additional flags that are not recognized by ` ParseCommandLineFlags ( ) ` are handled from another context ? I would prefer if , by default , benchmark would report unrecognized command line flags . Something like `` ` cpp // benchmark.cc : ParseCommandLineFlags ( ) if ( ... ) { // ... } else if ( IsFlag ( argv [ i ] , `` help '' ) ) { PrintUsageAndExit ( ) ; } else { fprintf ( stdout , `` unrecognized command line flag : % s\n '' , argv [ i ] ) ; exit ( 1 ) ; } `` `
Report/error unrecognized command line flags __EoT__ Are there any use cases where additional flags that are not recognized by ` ParseCommandLineFlags ( ) ` are handled from another context ? I would prefer if , by default , benchmark would report unrecognized command line flags . Something like `` ` cpp // benchmark.cc : ParseCommandLineFlags ( ) if ( ... ) { // ... } else if ( IsFlag ( argv [ i ] , `` help '' ) ) { PrintUsageAndExit ( ) ; } else { fprintf ( stdout , `` unrecognized command line flag : % s\n '' , argv [ i ] ) ; exit ( 1 ) ; } `` `
Report/error unrecognized command line flags __EoT__ Are there any use cases where additional flags that are not recognized by ` ParseCommandLineFlags ( ) ` are handled from another context ? I would prefer if , by default , benchmark would report unrecognized command line flags . Something like `` ` cpp // benchmark.cc : ParseCommandLineFlags ( ) if ( ... ) { // ... } else if ( IsFlag ( argv [ i ] , `` help '' ) ) { PrintUsageAndExit ( ) ; } else { fprintf ( stdout , `` unrecognized command line flag : % s\n '' , argv [ i ] ) ; exit ( 1 ) ; } `` `
Report/error unrecognized command line flags __EoT__ Are there any use cases where additional flags that are not recognized by ` ParseCommandLineFlags ( ) ` are handled from another context ? I would prefer if , by default , benchmark would report unrecognized command line flags . Something like `` ` cpp // benchmark.cc : ParseCommandLineFlags ( ) if ( ... ) { // ... } else if ( IsFlag ( argv [ i ] , `` help '' ) ) { PrintUsageAndExit ( ) ; } else { fprintf ( stdout , `` unrecognized command line flag : % s\n '' , argv [ i ] ) ; exit ( 1 ) ; } `` `
Report/error unrecognized command line flags __EoT__ Are there any use cases where additional flags that are not recognized by ` ParseCommandLineFlags ( ) ` are handled from another context ? I would prefer if , by default , benchmark would report unrecognized command line flags . Something like `` ` cpp // benchmark.cc : ParseCommandLineFlags ( ) if ( ... ) { // ... } else if ( IsFlag ( argv [ i ] , `` help '' ) ) { PrintUsageAndExit ( ) ; } else { fprintf ( stdout , `` unrecognized command line flag : % s\n '' , argv [ i ] ) ; exit ( 1 ) ; } `` `
Report/error unrecognized command line flags __EoT__ Are there any use cases where additional flags that are not recognized by ` ParseCommandLineFlags ( ) ` are handled from another context ? I would prefer if , by default , benchmark would report unrecognized command line flags . Something like `` ` cpp // benchmark.cc : ParseCommandLineFlags ( ) if ( ... ) { // ... } else if ( IsFlag ( argv [ i ] , `` help '' ) ) { PrintUsageAndExit ( ) ; } else { fprintf ( stdout , `` unrecognized command line flag : % s\n '' , argv [ i ] ) ; exit ( 1 ) ; } `` `
Additional statitistics and complexity should be calculated outside of the reporter . __EoT__ Most of the replicated code in the reporters is dedicated to computing additional statistics and complexity reports . I believe this would be better done outside of the reporters . I 'm currently working on such a change . However I wanted to confirm with @ ismaelJimenez that such a change would be appropriate for his needs . Do you have a custom reporter that needs all of the complexity reports or would it just be sufficient for ` ReportComplexity ` to simply be given the two precomputed complexity reports ?
Template arguments range __EoT__ Hello , What do you think about adding a mechanism to iterate over range of template arguments passed to ` TEMPLATE_BENCHMARK ` ? Such thing could be implemented using loop template or Boost.Preprocessor ( I guess we do n't want to have any dependency on that ) . I 've added some insight on the topic on my blog post : http : //dominikczarnota.blogspot.com/2015/12/how-i-saved-myself-lot-of-writing-with.html
Template arguments range __EoT__ Hello , What do you think about adding a mechanism to iterate over range of template arguments passed to ` TEMPLATE_BENCHMARK ` ? Such thing could be implemented using loop template or Boost.Preprocessor ( I guess we do n't want to have any dependency on that ) . I 've added some insight on the topic on my blog post : http : //dominikczarnota.blogspot.com/2015/12/how-i-saved-myself-lot-of-writing-with.html
Parsing CPU info on Linux reads only cpuinfo file __EoT__ There is a bug in a file ` sysinfo.cc ` in a function ` ReadIntFromFile ` which is used to read CPU frequency from files ` /sys/devices/system/cpu/cpu0/tsc_freq_khz ` and ` /sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_max_freq ` . Here is the code of the function : `` ` C // Helper function for reading an int from a file . Returns true if successful // and the memory location pointed to by value is set to the value read . bool ReadIntFromFile ( const char* file , long* value ) { bool ret = false ; int fd = open ( file , O_RDONLY ) ; if ( fd ! = -1 ) { char line [ 1024 ] ; char* err ; memset ( line , '\0 ' , sizeof ( line ) ) ; CHECK ( read ( fd , line , sizeof ( line ) - 1 ) ) ; const long temp_value = strtol ( line , & err , 10 ) ; if ( line [ 0 ] ! = '\0 ' & & ( *err == '\n ' || *err == '\0 ' ) ) { *value = temp_value
Parsing CPU info on Linux reads only cpuinfo file __EoT__ There is a bug in a file ` sysinfo.cc ` in a function ` ReadIntFromFile ` which is used to read CPU frequency from files ` /sys/devices/system/cpu/cpu0/tsc_freq_khz ` and ` /sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_max_freq ` . Here is the code of the function : `` ` C // Helper function for reading an int from a file . Returns true if successful // and the memory location pointed to by value is set to the value read . bool ReadIntFromFile ( const char* file , long* value ) { bool ret = false ; int fd = open ( file , O_RDONLY ) ; if ( fd ! = -1 ) { char line [ 1024 ] ; char* err ; memset ( line , '\0 ' , sizeof ( line ) ) ; CHECK ( read ( fd , line , sizeof ( line ) - 1 ) ) ; const long temp_value = strtol ( line , & err , 10 ) ; if ( line [ 0 ] ! = '\0 ' & & ( *err == '\n ' || *err == '\0 ' ) ) { *value = temp_value
Parsing CPU info on Linux reads only cpuinfo file __EoT__ There is a bug in a file ` sysinfo.cc ` in a function ` ReadIntFromFile ` which is used to read CPU frequency from files ` /sys/devices/system/cpu/cpu0/tsc_freq_khz ` and ` /sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_max_freq ` . Here is the code of the function : `` ` C // Helper function for reading an int from a file . Returns true if successful // and the memory location pointed to by value is set to the value read . bool ReadIntFromFile ( const char* file , long* value ) { bool ret = false ; int fd = open ( file , O_RDONLY ) ; if ( fd ! = -1 ) { char line [ 1024 ] ; char* err ; memset ( line , '\0 ' , sizeof ( line ) ) ; CHECK ( read ( fd , line , sizeof ( line ) - 1 ) ) ; const long temp_value = strtol ( line , & err , 10 ) ; if ( line [ 0 ] ! = '\0 ' & & ( *err == '\n ' || *err == '\0 ' ) ) { *value = temp_value
Parsing CPU info on Linux reads only cpuinfo file __EoT__ There is a bug in a file ` sysinfo.cc ` in a function ` ReadIntFromFile ` which is used to read CPU frequency from files ` /sys/devices/system/cpu/cpu0/tsc_freq_khz ` and ` /sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_max_freq ` . Here is the code of the function : `` ` C // Helper function for reading an int from a file . Returns true if successful // and the memory location pointed to by value is set to the value read . bool ReadIntFromFile ( const char* file , long* value ) { bool ret = false ; int fd = open ( file , O_RDONLY ) ; if ( fd ! = -1 ) { char line [ 1024 ] ; char* err ; memset ( line , '\0 ' , sizeof ( line ) ) ; CHECK ( read ( fd , line , sizeof ( line ) - 1 ) ) ; const long temp_value = strtol ( line , & err , 10 ) ; if ( line [ 0 ] ! = '\0 ' & & ( *err == '\n ' || *err == '\0 ' ) ) { *value = temp_value
Parsing CPU info on Linux reads only cpuinfo file __EoT__ There is a bug in a file ` sysinfo.cc ` in a function ` ReadIntFromFile ` which is used to read CPU frequency from files ` /sys/devices/system/cpu/cpu0/tsc_freq_khz ` and ` /sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_max_freq ` . Here is the code of the function : `` ` C // Helper function for reading an int from a file . Returns true if successful // and the memory location pointed to by value is set to the value read . bool ReadIntFromFile ( const char* file , long* value ) { bool ret = false ; int fd = open ( file , O_RDONLY ) ; if ( fd ! = -1 ) { char line [ 1024 ] ; char* err ; memset ( line , '\0 ' , sizeof ( line ) ) ; CHECK ( read ( fd , line , sizeof ( line ) - 1 ) ) ; const long temp_value = strtol ( line , & err , 10 ) ; if ( line [ 0 ] ! = '\0 ' & & ( *err == '\n ' || *err == '\0 ' ) ) { *value = temp_value
Parsing CPU info on Linux reads only cpuinfo file __EoT__ There is a bug in a file ` sysinfo.cc ` in a function ` ReadIntFromFile ` which is used to read CPU frequency from files ` /sys/devices/system/cpu/cpu0/tsc_freq_khz ` and ` /sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_max_freq ` . Here is the code of the function : `` ` C // Helper function for reading an int from a file . Returns true if successful // and the memory location pointed to by value is set to the value read . bool ReadIntFromFile ( const char* file , long* value ) { bool ret = false ; int fd = open ( file , O_RDONLY ) ; if ( fd ! = -1 ) { char line [ 1024 ] ; char* err ; memset ( line , '\0 ' , sizeof ( line ) ) ; CHECK ( read ( fd , line , sizeof ( line ) - 1 ) ) ; const long temp_value = strtol ( line , & err , 10 ) ; if ( line [ 0 ] ! = '\0 ' & & ( *err == '\n ' || *err == '\0 ' ) ) { *value = temp_value
Parsing CPU info on Linux reads only cpuinfo file __EoT__ There is a bug in a file ` sysinfo.cc ` in a function ` ReadIntFromFile ` which is used to read CPU frequency from files ` /sys/devices/system/cpu/cpu0/tsc_freq_khz ` and ` /sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_max_freq ` . Here is the code of the function : `` ` C // Helper function for reading an int from a file . Returns true if successful // and the memory location pointed to by value is set to the value read . bool ReadIntFromFile ( const char* file , long* value ) { bool ret = false ; int fd = open ( file , O_RDONLY ) ; if ( fd ! = -1 ) { char line [ 1024 ] ; char* err ; memset ( line , '\0 ' , sizeof ( line ) ) ; CHECK ( read ( fd , line , sizeof ( line ) - 1 ) ) ; const long temp_value = strtol ( line , & err , 10 ) ; if ( line [ 0 ] ! = '\0 ' & & ( *err == '\n ' || *err == '\0 ' ) ) { *value = temp_value
Parsing CPU info on Linux reads only cpuinfo file __EoT__ There is a bug in a file ` sysinfo.cc ` in a function ` ReadIntFromFile ` which is used to read CPU frequency from files ` /sys/devices/system/cpu/cpu0/tsc_freq_khz ` and ` /sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_max_freq ` . Here is the code of the function : `` ` C // Helper function for reading an int from a file . Returns true if successful // and the memory location pointed to by value is set to the value read . bool ReadIntFromFile ( const char* file , long* value ) { bool ret = false ; int fd = open ( file , O_RDONLY ) ; if ( fd ! = -1 ) { char line [ 1024 ] ; char* err ; memset ( line , '\0 ' , sizeof ( line ) ) ; CHECK ( read ( fd , line , sizeof ( line ) - 1 ) ) ; const long temp_value = strtol ( line , & err , 10 ) ; if ( line [ 0 ] ! = '\0 ' & & ( *err == '\n ' || *err == '\0 ' ) ) { *value = temp_value
Parsing CPU info on Linux reads only cpuinfo file __EoT__ There is a bug in a file ` sysinfo.cc ` in a function ` ReadIntFromFile ` which is used to read CPU frequency from files ` /sys/devices/system/cpu/cpu0/tsc_freq_khz ` and ` /sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_max_freq ` . Here is the code of the function : `` ` C // Helper function for reading an int from a file . Returns true if successful // and the memory location pointed to by value is set to the value read . bool ReadIntFromFile ( const char* file , long* value ) { bool ret = false ; int fd = open ( file , O_RDONLY ) ; if ( fd ! = -1 ) { char line [ 1024 ] ; char* err ; memset ( line , '\0 ' , sizeof ( line ) ) ; CHECK ( read ( fd , line , sizeof ( line ) - 1 ) ) ; const long temp_value = strtol ( line , & err , 10 ) ; if ( line [ 0 ] ! = '\0 ' & & ( *err == '\n ' || *err == '\0 ' ) ) { *value = temp_value
version 1.4.1 is broken on s390x __EoT__ Almost all unit tests are failing on s390x with the new 1.4.1 version of benchmark . I executed the basic_test in gdb . And backtrace after the throwing an exception is here : ` Catchpoint 1 ( exception thrown ) , 0x000003fffdc27860 in __cxxabiv1 : :__cxa_throw ( obj=obj @ entry=0x10001a7c8 , tinfo=0x3fffdd2e920 < typeinfo for std : :invalid_argument > , dest=0x3fffdc3e8e0 < std : :invalid_argument : :~invalid_argument ( ) > ) at ../../../../src/libstdc++-v3/libsupc++/eh_throw.cc:78 78 ../../../../src/libstdc++-v3/libsupc++/eh_throw.cc : No such file or directory . # 0 0x000003fffdc27860 in __cxxabiv1 : :__cxa_throw ( obj=obj @ entry=0x10001a7c8 , tinfo=0x3fffdd2e920 < typeinfo for std : :invalid_argument > , dest=0x3fffdc3e8e0 < std : :invalid_argument : :~invalid_argument ( ) > ) at ../../../../src/libstdc++-v3/libsupc++/eh_throw.cc:78 # 1 0x000003fffdc548cc in std : :__throw_invalid_argument ( __s=__s @ entry=0x3fffdf4bf9e `` stoi '' ) at ../../../../../src/libstdc++-v3/src/c++11/functexcept.cc:74 # 2 0x000003fffdf47d34 in __gnu_cxx : :__stoa < long , int , char , int > ( __idx=0x0 , __str=0x10001a650 `` version = FF , identification = 01F467 , machine = 2964 '' , __name=0x3fffdf4bf9e `` stoi '' , __convf= < optimized out > ) at /usr/include/c++/8/bits/char_traits.h:320 # 3 std : :__cxx11 : :stoi ( __base=10 , __idx=0x0 , __str=
tests : Do less scrubbing of recorded HTTP requests __EoT__ There 's a bunch of scrubbing going on , but we 've decided not to do any scrubbing other than of the Authorization header . There will still need to be some string manipulation when matching requests during replay , since we match HTTP bodies and there 's some dynamic stuff in there . The scrub function for that should be provider-specific .
tests : Do less scrubbing of recorded HTTP requests __EoT__ There 's a bunch of scrubbing going on , but we 've decided not to do any scrubbing other than of the Authorization header . There will still need to be some string manipulation when matching requests during replay , since we match HTTP bodies and there 's some dynamic stuff in there . The scrub function for that should be provider-specific .
contributebot : no longer auto-assign reviewers __EoT__ I just made a new PR ( https : //github.com/google/go-cloud/pull/877 ) , wanting to first look at the change from a reviewers perspective . By the time I opened it up , @ jba had already put review comments on it about things I already knew I needed to change . I do n't want to waste my reviewers ' time , but I also find it valuable to see how my change will look to them before sending it off .
contributebot : no longer auto-assign reviewers __EoT__ I just made a new PR ( https : //github.com/google/go-cloud/pull/877 ) , wanting to first look at the change from a reviewers perspective . By the time I opened it up , @ jba had already put review comments on it about things I already knew I needed to change . I do n't want to waste my reviewers ' time , but I also find it valuable to see how my change will look to them before sending it off .
contributebot : no longer auto-assign reviewers __EoT__ I just made a new PR ( https : //github.com/google/go-cloud/pull/877 ) , wanting to first look at the change from a reviewers perspective . By the time I opened it up , @ jba had already put review comments on it about things I already knew I needed to change . I do n't want to waste my reviewers ' time , but I also find it valuable to see how my change will look to them before sending it off .
blob/test : improve comments and tests for List __EoT__ These should wait until # 541 is done . - Add a comment about guarantees for blob.Bucket.Next consistency ( i.e. , everything in UTF-8 lexicographical order ) ( this might already be there ) . - Add a comment to driver.Bucket.ListPaged about consistency requirements ( i.e. , everything in UTF-8 lexicographical order , including across pages ) . - Add tests with a Unicode filename , a filename with `` / '' , a filename with `` \ '' . - Add a test that verifies no `` directory '' is returned after the last file `` in '' it is deleted . - Add a test that verifies consistency across ListPaged after insert & delete . E.g. , `` a b c d e f '' , fetch Page of size 3 = `` a b c '' , then [ insert after `` b '' | delete `` b '' ] , should get `` d e f '' in next Page .
mysql/cloudsql : native password authentication not enabled __EoT__ # # # # What did you do ? I want to use the go-cloud library to connect to a database in a GCP MySQL instance so I followed the example in https : //github.com/google/go-cloud/blob/master/mysql/cloudmysql/example_test.go filling the proper values for my project in GCP and the database credentials . # # # # What did you expect to see ? I expect it to successfully connect to the selected database in Google Cloud SQL . # # # # What did you see instead ? The connection failed with the error message `` driver.go:120 : could not use requested auth plugin 'mysql_native_password ' : this user requires mysql native password authentication . '' Then I did a little research and found this issue [ https : //github.com/go-sql-driver/mysql/pull/644 ] ( url ) . It turns out the go-mysql-driver team added a new function to create database configuration named NewConfig in which it sets some default values included 'AllowNativePasswords : true ' . It would be great if you reflect that change in cloudmysql.go . For me the workaround was to add the line `` cfg.AllowNativePasswords = true '' : cfg : = &
blob/driver : provide a conformance test suite for implementations __EoT__ For each API , a cloud provider implements a Driver interface in order to support that API for their cloud . For each Driver type , we should create a testcase that takes a Driver as input , and runs it through its paces . If the Driver passes the tests , then we guarantee it will work with the go-cloud API implementation .
docs : write Getting Started __EoT__ I do n't know what this document should look like , and eng team are n't the best to write this . Assigning to @ enocom who has said he could do it .
docs : write Getting Started __EoT__ I do n't know what this document should look like , and eng team are n't the best to write this . Assigning to @ enocom who has said he could do it .
docs : write Getting Started __EoT__ I do n't know what this document should look like , and eng team are n't the best to write this . Assigning to @ enocom who has said he could do it .
server/sdserver : NewExporter should return a cleanup function for Exporter.Flush ( ) __EoT__ # # # Describe the bug Seems like with the current design of providing a StackDriver exporter , it 's the clients responsibility to inject an exporter , and then remember to call ` contrib.go.opencensus.io/exporter/stackdriver.Exporter.Flush ( ) ` . While ` wire ` supports cleanup , I thought it maybe good idea to augment ` NewExporter ` to also provide a cleanup function . Not sure how this will affect existing users though ; but , maybe , we can added a new provider that supports this . # # Expected behavior When one inject a ` stackdriver.Exporter ` , she should expect the cleanup function should be returned as well so remind of her to call to flush . # # # Version v0.6.0 # # # Additional context I 'm happy to put together a PR if this is going along the right direction . : )
runtimevar : refactor WatchVariable to pass/return a driver-specific State interface __EoT__ For context , see conversation on # 369 .
internal/contributebot : allow titles like `` README : `` __EoT__ For both issue and pull request titles .
sql/ocsql : allow configuration of ocsql instrumentation __EoT__ Currently ` ocsql ` is included with all trace options enabled . This means that sql queries and query parameters are annotated in spans , which can be undesirable from a security concern as well as a performance one ( high cardinality annotations or heavily annotated spans do not sit well with all tracing backends in high QPS systems ) . This is why default ` ocsql ` is much more conservative and has all extra trace options disabled . I think it would be best to have the conservative default mirrored in Go cloud as well as the ability for consumers to set the configuration of ` ocsql ` when using Go cloud . Happy to provide a PR if I can be guided a bit on how Go cloud wants configuration done ( would this be part of rdsmysql.Params and cloudmysql.Params ? )
Draft needed documentation __EoT__ Step 2 of # 87 . These can go in ` docs/ ` so they 're easily read on GitHub . Needs approval from @ cassandroid .
api : Document strategy for enforcing portability ( optional fields , legal names , ... ) __EoT__ For example , blob 's ModTime is optional and may not be available for all providers . There should be a way to tell if a specific provider implementation does .
api : Document strategy for enforcing portability ( optional fields , legal names , ... ) __EoT__ For example , blob 's ModTime is optional and may not be available for all providers . There should be a way to tell if a specific provider implementation does .
pubsub : add conformance tests __EoT__
pubsub : add conformance tests __EoT__
blob : add As for List functions __EoT__ PR # 510 is adding a couple of ` List ` functions , we should support the ` As ` escape hatches for the ` List ` types , including ` ListOptions ` and ` ListObject ` .
blob : add As for List functions __EoT__ PR # 510 is adding a couple of ` List ` functions , we should support the ` As ` escape hatches for the ` List ` types , including ` ListOptions ` and ` ListObject ` .
blob : add As for List functions __EoT__ PR # 510 is adding a couple of ` List ` functions , we should support the ` As ` escape hatches for the ` List ` types , including ` ListOptions ` and ` ListObject ` .
mysql : instrument connection with OpenCensus __EoT__ We should add [ OpenCensus instrumentation ] ( https : //opencensus.io/guides/integrations/sql/go_sql/ ) to any SQL dialers . This should not change the API AFAICT . ( Idea came from a [ tweet ] ( https : //twitter.com/odeke_et/status/1032491251566706688 ) by @ odeke-em . )
blob : gcsblob -- record is hard-coded to a specific bucket / project __EoT__ TestNewWriterObjectNaming refers to a specific bucket , which was created in a specific project . That project has now been disabled . -- - FAIL : TestNewWriterObjectNaming/An_ASCII_name_should_pass ( 0.08s ) gcsblob_test.go:176 : got `` googleapi : Error 400 : Project does not exist : 640193379878 , invalid '' ; want nil
blob : gcsblob -- record is hard-coded to a specific bucket / project __EoT__ TestNewWriterObjectNaming refers to a specific bucket , which was created in a specific project . That project has now been disabled . -- - FAIL : TestNewWriterObjectNaming/An_ASCII_name_should_pass ( 0.08s ) gcsblob_test.go:176 : got `` googleapi : Error 400 : Project does not exist : 640193379878 , invalid '' ; want nil
samples/guestbook/gcp : provision-db.sh depends on jq __EoT__ When I ran ` terraform apply ` from the sample README , it failed like this on my Mac : `` ` provision-db.sh : line 63 : jq : command not found `` ` Of course I could just ` brew install jq ` but it 's a bit of a rough spot . Maybe it would be better to rewrite this script in Go so it ca n't fail this way .
samples/guestbook/gcp : provision-db.sh depends on jq __EoT__ When I ran ` terraform apply ` from the sample README , it failed like this on my Mac : `` ` provision-db.sh : line 63 : jq : command not found `` ` Of course I could just ` brew install jq ` but it 's a bit of a rough spot . Maybe it would be better to rewrite this script in Go so it ca n't fail this way .
samples/guestbook/gcp : provision-db.sh depends on jq __EoT__ When I ran ` terraform apply ` from the sample README , it failed like this on my Mac : `` ` provision-db.sh : line 63 : jq : command not found `` ` Of course I could just ` brew install jq ` but it 's a bit of a rough spot . Maybe it would be better to rewrite this script in Go so it ca n't fail this way .
samples/guestbook/gcp : provision-db.sh depends on jq __EoT__ When I ran ` terraform apply ` from the sample README , it failed like this on my Mac : `` ` provision-db.sh : line 63 : jq : command not found `` ` Of course I could just ` brew install jq ` but it 's a bit of a rough spot . Maybe it would be better to rewrite this script in Go so it ca n't fail this way .
samples/guestbook/gcp : provision-db.sh depends on jq __EoT__ When I ran ` terraform apply ` from the sample README , it failed like this on my Mac : `` ` provision-db.sh : line 63 : jq : command not found `` ` Of course I could just ` brew install jq ` but it 's a bit of a rough spot . Maybe it would be better to rewrite this script in Go so it ca n't fail this way .
travis : set up a Mac OS X environment __EoT__ Is it just as easy as https : //docs.travis-ci.com/user/reference/osx/ # Using-OS-X and running it as a shard ?
travis : set up a Mac OS X environment __EoT__ Is it just as easy as https : //docs.travis-ci.com/user/reference/osx/ # Using-OS-X and running it as a shard ?
travis : set up a Mac OS X environment __EoT__ Is it just as easy as https : //docs.travis-ci.com/user/reference/osx/ # Using-OS-X and running it as a shard ?
all : support RPC retry __EoT__ We should establish a common pattern for retrying RPCs . It ca n't be done completely in the concrete implementations because different providers will have different notions of what is retryable . @ vangent suggested : - Drivers should provide an ` IsRetryable ` function or method that takes an error and returns a bool and , if true , a duration . ( The duration is to support providers whose RPCs return sleep hints . ) - There should be common code in ` internal ` that implements exponential backoff . Alternative designs : - _Have drivers be responsible for retry._ This puts too much burden on driver implementers . - _Wrap errors._ Return errors that are annotated or wrapped to indicate whether they are retryable . E.g . if an error has an ` IsRetryable ` method that returns true , it is retryable ; otherwise assume it is n't . This also makes it too hard and error-prone to write a driver , since the wrapping has to be done at every error return .
pubsub/gcppubsub : allow logging of pull errors __EoT__ Quoting @ zombiezen , in his review of https : //github.com/google/go-cloud/pull/705 , referring to the call to ` s.client.Pull ( ctx , req ) ` : > Unfortunately , this error will never get seen by the end user , but it could be helpful for debugging . We could add a hook to the constructor to the effect of : > > `` ` go > // A type that implements ErrorLogger reports errors that occur while receiving a batch . > type ErrorLogger interface { > LogError ( context.Context , error ) > } > `` ` > > ( Probably with some filtering for transient errors or such . )
all : implement escape hatches __EoT__ Design proposal has circulated , and will be added to design.md in a PR soon .
all : implement escape hatches __EoT__ Design proposal has circulated , and will be added to design.md in a PR soon .
testing : move to internal/testing __EoT__ We do n't really want to commit to the API surface , as it is being used to test the project itself , not for helping others tests their apps .
all : replay tests have diffs even if interactions have n't changed __EoT__ The HTTP/gRPC replay tests currently do n't scrub transient attributes like date or request ID , which makes diffs noisy . This makes it difficult for a reviewer to see whether an interaction has actually changed or whether it 's just normal noise .
all : create GitHub contribution linter __EoT__ It would be useful to have a GitHub bot that enforces/auto-suggests some of the non-code things in CONTRIBUTING.md . In particular : - [ ] Issue title format - [ ] PR title format - [ ] Commit message format ( one line summary , `` Fixes '' on last line of description ) - [ ] License headers in new files - [ ] Branches are in a fork , not on the main repository This can use the [ checks API ] ( https : //developer.github.com/changes/2018-05-07-new-checks-api-public-beta/ ) to present the information as well as some limited support for auto-fixing , like the license headers .
all : create GitHub contribution linter __EoT__ It would be useful to have a GitHub bot that enforces/auto-suggests some of the non-code things in CONTRIBUTING.md . In particular : - [ ] Issue title format - [ ] PR title format - [ ] Commit message format ( one line summary , `` Fixes '' on last line of description ) - [ ] License headers in new files - [ ] Branches are in a fork , not on the main repository This can use the [ checks API ] ( https : //developer.github.com/changes/2018-05-07-new-checks-api-public-beta/ ) to present the information as well as some limited support for auto-fixing , like the license headers .
all : create GitHub contribution linter __EoT__ It would be useful to have a GitHub bot that enforces/auto-suggests some of the non-code things in CONTRIBUTING.md . In particular : - [ ] Issue title format - [ ] PR title format - [ ] Commit message format ( one line summary , `` Fixes '' on last line of description ) - [ ] License headers in new files - [ ] Branches are in a fork , not on the main repository This can use the [ checks API ] ( https : //developer.github.com/changes/2018-05-07-new-checks-api-public-beta/ ) to present the information as well as some limited support for auto-fixing , like the license headers .
mempubsub : OpenTopic and OpenSubscription should return concrete types __EoT__ See https : //github.com/google/go-cloud/blob/ba6cdfb00c3d227ebaa0243978d0b9860b9f60e7/blob/fileblob/fileblob.go # L98 for a precedent . We might decide to change how we 're doing this , but if we do then it should be done consistently across all the Go Cloud packages .
mempubsub : OpenTopic and OpenSubscription should return concrete types __EoT__ See https : //github.com/google/go-cloud/blob/ba6cdfb00c3d227ebaa0243978d0b9860b9f60e7/blob/fileblob/fileblob.go # L98 for a precedent . We might decide to change how we 're doing this , but if we do then it should be done consistently across all the Go Cloud packages .
mempubsub : OpenTopic and OpenSubscription should return concrete types __EoT__ See https : //github.com/google/go-cloud/blob/ba6cdfb00c3d227ebaa0243978d0b9860b9f60e7/blob/fileblob/fileblob.go # L98 for a precedent . We might decide to change how we 're doing this , but if we do then it should be done consistently across all the Go Cloud packages .
pubsub : dynamically adjust batch sizes in Topic and Subscription bundler logic __EoT__ A simple thing to try first would be to start with a batch size of 1 and then adjust the size to be the number of messages that piled up while the previous call to SendBatch was running .
pubsub : GCP driver __EoT__
pubsub : GCP driver __EoT__
pubsub : GCP driver __EoT__
pubsub : GCP driver __EoT__
pubsub : GCP driver __EoT__
pubsub : GCP driver __EoT__
pubsub : determine when to fail on non-existent resources __EoT__ If the ` OpenXXX ` methods are asked to open non-existent resources , should they fail immediately , or should failure happen only when the resource is used ? Checking during open requires an RPC and a chunk of code , so it makes it harder to implement a driver , which we want to avoid . While users will know sooner that they made a mistake , they are already used to seeing errors only on first use , since at least on Amazon and GCP , there is no `` open '' step & mdash ; you just provide a string and start using the resource . So for simplicity , I 'd suggest failing on first use . Or more precisely , not having drivers do anything special in any of their methods : just pass through to the provider , or in the case of Open , just create client-side data structures .
mysql : OpenCensus instrumentation not used __EoT__ For example , in the ` cloudmysql ` package , the driver is wrapped here : https : //github.com/google/go-cloud/blob/11bb776c80e5cc8abaa6c9aead66d5c1129c9632/mysql/cloudmysql/cloudmysql.go # L95 but is not wrapped in the ` Connect ` method , which is what is called to create new connections : https : //github.com/google/go-cloud/blob/11bb776c80e5cc8abaa6c9aead66d5c1129c9632/mysql/cloudmysql/cloudmysql.go # L79
mysql : OpenCensus instrumentation not used __EoT__ For example , in the ` cloudmysql ` package , the driver is wrapped here : https : //github.com/google/go-cloud/blob/11bb776c80e5cc8abaa6c9aead66d5c1129c9632/mysql/cloudmysql/cloudmysql.go # L95 but is not wrapped in the ` Connect ` method , which is what is called to create new connections : https : //github.com/google/go-cloud/blob/11bb776c80e5cc8abaa6c9aead66d5c1129c9632/mysql/cloudmysql/cloudmysql.go # L79
mysql : OpenCensus instrumentation not used __EoT__ For example , in the ` cloudmysql ` package , the driver is wrapped here : https : //github.com/google/go-cloud/blob/11bb776c80e5cc8abaa6c9aead66d5c1129c9632/mysql/cloudmysql/cloudmysql.go # L95 but is not wrapped in the ` Connect ` method , which is what is called to create new connections : https : //github.com/google/go-cloud/blob/11bb776c80e5cc8abaa6c9aead66d5c1129c9632/mysql/cloudmysql/cloudmysql.go # L79
mysql : OpenCensus instrumentation not used __EoT__ For example , in the ` cloudmysql ` package , the driver is wrapped here : https : //github.com/google/go-cloud/blob/11bb776c80e5cc8abaa6c9aead66d5c1129c9632/mysql/cloudmysql/cloudmysql.go # L95 but is not wrapped in the ` Connect ` method , which is what is called to create new connections : https : //github.com/google/go-cloud/blob/11bb776c80e5cc8abaa6c9aead66d5c1129c9632/mysql/cloudmysql/cloudmysql.go # L79
mysql : OpenCensus instrumentation not used __EoT__ For example , in the ` cloudmysql ` package , the driver is wrapped here : https : //github.com/google/go-cloud/blob/11bb776c80e5cc8abaa6c9aead66d5c1129c9632/mysql/cloudmysql/cloudmysql.go # L95 but is not wrapped in the ` Connect ` method , which is what is called to create new connections : https : //github.com/google/go-cloud/blob/11bb776c80e5cc8abaa6c9aead66d5c1129c9632/mysql/cloudmysql/cloudmysql.go # L79
mysql : OpenCensus instrumentation not used __EoT__ For example , in the ` cloudmysql ` package , the driver is wrapped here : https : //github.com/google/go-cloud/blob/11bb776c80e5cc8abaa6c9aead66d5c1129c9632/mysql/cloudmysql/cloudmysql.go # L95 but is not wrapped in the ` Connect ` method , which is what is called to create new connections : https : //github.com/google/go-cloud/blob/11bb776c80e5cc8abaa6c9aead66d5c1129c9632/mysql/cloudmysql/cloudmysql.go # L79
mysql : OpenCensus instrumentation not used __EoT__ For example , in the ` cloudmysql ` package , the driver is wrapped here : https : //github.com/google/go-cloud/blob/11bb776c80e5cc8abaa6c9aead66d5c1129c9632/mysql/cloudmysql/cloudmysql.go # L95 but is not wrapped in the ` Connect ` method , which is what is called to create new connections : https : //github.com/google/go-cloud/blob/11bb776c80e5cc8abaa6c9aead66d5c1129c9632/mysql/cloudmysql/cloudmysql.go # L79
mysql : OpenCensus instrumentation not used __EoT__ For example , in the ` cloudmysql ` package , the driver is wrapped here : https : //github.com/google/go-cloud/blob/11bb776c80e5cc8abaa6c9aead66d5c1129c9632/mysql/cloudmysql/cloudmysql.go # L95 but is not wrapped in the ` Connect ` method , which is what is called to create new connections : https : //github.com/google/go-cloud/blob/11bb776c80e5cc8abaa6c9aead66d5c1129c9632/mysql/cloudmysql/cloudmysql.go # L79
mysql : OpenCensus instrumentation not used __EoT__ For example , in the ` cloudmysql ` package , the driver is wrapped here : https : //github.com/google/go-cloud/blob/11bb776c80e5cc8abaa6c9aead66d5c1129c9632/mysql/cloudmysql/cloudmysql.go # L95 but is not wrapped in the ` Connect ` method , which is what is called to create new connections : https : //github.com/google/go-cloud/blob/11bb776c80e5cc8abaa6c9aead66d5c1129c9632/mysql/cloudmysql/cloudmysql.go # L79
mysql : OpenCensus instrumentation not used __EoT__ For example , in the ` cloudmysql ` package , the driver is wrapped here : https : //github.com/google/go-cloud/blob/11bb776c80e5cc8abaa6c9aead66d5c1129c9632/mysql/cloudmysql/cloudmysql.go # L95 but is not wrapped in the ` Connect ` method , which is what is called to create new connections : https : //github.com/google/go-cloud/blob/11bb776c80e5cc8abaa6c9aead66d5c1129c9632/mysql/cloudmysql/cloudmysql.go # L79
postgresql : add PostgreSQL support __EoT__ We have the dialers for MySQL , but both AWS and GCP also have hosted PostgreSQL offerings . Go Cloud should have dialers for these things . It does raise an interesting question of whether they should be added to the ` awscloud.Services ` and ` gcpcloud.Services ` sets , since they would conflict with the MySQL providers . How this would be resolved is TBD .
postgresql : add PostgreSQL support __EoT__ We have the dialers for MySQL , but both AWS and GCP also have hosted PostgreSQL offerings . Go Cloud should have dialers for these things . It does raise an interesting question of whether they should be added to the ` awscloud.Services ` and ` gcpcloud.Services ` sets , since they would conflict with the MySQL providers . How this would be resolved is TBD .
blob/fileblob : rename to fakeblob ? __EoT__ In considering the implementation for https : //github.com/google/go-cloud/issues/546 , there is a large difference between what I 'd design for a ` fileblob ` that is really ` fakeblob ` , intended only for testing and local dev use , versus ` fileblob ` that is a first-class sibling to ` gcsblob ` or ` awsblob ` . And there seems to be considerable ambiguity right now about which use case to design for . Having a signed URL api for ` fileblob ` as a first-class/non-test provider seems like it would require us to make guarantees that are kind of intense . Or we could rename it to make it very clear what the intention for that package is , and _caveat emptor_ . This has been [ previously considered ] ( https : //github.com/google/go-cloud/pull/673 # discussion_r233605383 ) a couple of times with no changes to the naming , but I think the signed URL implementation requires that we commit to an intention for ` fileblob ` one way or the other , and it seems like the intention is ( and has been ) that it 's a dev blob ,
blob/fileblob : rename to fakeblob ? __EoT__ In considering the implementation for https : //github.com/google/go-cloud/issues/546 , there is a large difference between what I 'd design for a ` fileblob ` that is really ` fakeblob ` , intended only for testing and local dev use , versus ` fileblob ` that is a first-class sibling to ` gcsblob ` or ` awsblob ` . And there seems to be considerable ambiguity right now about which use case to design for . Having a signed URL api for ` fileblob ` as a first-class/non-test provider seems like it would require us to make guarantees that are kind of intense . Or we could rename it to make it very clear what the intention for that package is , and _caveat emptor_ . This has been [ previously considered ] ( https : //github.com/google/go-cloud/pull/673 # discussion_r233605383 ) a couple of times with no changes to the naming , but I think the signed URL implementation requires that we commit to an intention for ` fileblob ` one way or the other , and it seems like the intention is ( and has been ) that it 's a dev blob ,
tests : AWS integration test harness __EoT__ Much like tests/gcp , add an integration test that runs on EC2 and checks that traces and logs are written .
tests : AWS integration test harness __EoT__ Much like tests/gcp , add an integration test that runs on EC2 and checks that traces and logs are written .
testing : Figure out how to scrub sensitive information from RPCs __EoT__ Can we scrub the project ID from the RPCs ? Maybe the RPCs are just replayed without matching ? Got to see how the RPC replay system works . Blocking # 61 .
pubsub : split MakePair ( ) into OpenTopic ( ) and OpenSubscription ( topic ) __EoT__ @ vangent noted how MakePair does n't handle the case of making multiple subscriptions in a comment on https : //github.com/google/go-cloud/pull/712 .
pubsub : split MakePair ( ) into OpenTopic ( ) and OpenSubscription ( topic ) __EoT__ @ vangent noted how MakePair does n't handle the case of making multiple subscriptions in a comment on https : //github.com/google/go-cloud/pull/712 .
pubsub : split MakePair ( ) into OpenTopic ( ) and OpenSubscription ( topic ) __EoT__ @ vangent noted how MakePair does n't handle the case of making multiple subscriptions in a comment on https : //github.com/google/go-cloud/pull/712 .
samples : tests broken by recent commit __EoT__ https : //github.com/google/go-cloud/pull/511 caused some breakage : https : //travis-ci.com/google/go-cloud/jobs/156110439 I 'm still not sure how this got through . I thought the checks prevented us from committing broken changes .
runtimevar/paramstore and blob/s3blob : testdata not up to date ? __EoT__ # # Steps to reproduce `` ` shell $ vgo test -short ./blob/s3blob ./runtimevar/paramstore ok github.com/google/go-cloud/blob/s3blob ( cached ) ok github.com/google/go-cloud/runtimevar/paramstore 0.702s $ git status On branch master Your branch is up to date with 'origin/master ' . Changes not staged for commit : ( use `` git add < file > ... '' to update what will be committed ) ( use `` git checkout -- < file > ... '' to discard changes in working directory ) modified : blob/s3blob/testdata/blob.yaml modified : runtimevar/paramstore/testdata/decoder.yaml modified : runtimevar/paramstore/testdata/watch_change.yaml modified : runtimevar/paramstore/testdata/watch_initial.yaml modified : runtimevar/paramstore/testdata/write_read_delete.yaml no changes added to commit ( use `` git add '' and/or `` git commit -a '' ) `` ` Differences appear to be the ` mu ` key and some minor YAML string encoding . I would expect this to be clean on a new checkout . # # Extra info `` ` shell $ git rev-parse HEAD 189ea15d728cc05309379d7991d126307dda3350 $ vgo list -m MODULE VERSION github.com/google/go-cloud - cloud.google.com/go v0.21.0 contrib.go.opencensus.io/exporter/stackdriver v0.0.0-20180421005815-665cf5131b71 github.com/GoogleCloudPlatform/cloudsql-proxy v0.0.0-20180321230639-1e456b1c68cb github.com/aws/aws-sdk-go v1.13.20 github.com/beorn7/perks v0.0.0-20180321164747-3a771d992973 github.com/bgentry/speakeasy v0.0.0-20170417200703-4aabc24848ce github.com/census-ecosystem/opencensus-go-exporter-aws v0.0.0-20180411051634-41633bc1ff6b github.com/coreos/bbolt v0.0.0-20171207012708-48ea1b39c25f github.com/coreos/etcd v0.0.0-20180424190529-fdde8705f5bf github.com/coreos/go-semver v0.2.0 github.com/coreos/go-systemd v0.0.0-20170731111925-d21964639418 github.com/coreos/pkg v0.0.0-20160727233714-3ac0863d7acf github.com/cpuguy83/go-md2man v0.0.0-20170603125239-23709d084719 github.com/davecgh/go-spew
Make go test work on fresh clones __EoT__ I cloned this repo and then ran the following command , which I stopped after waiting 8 minutes for it to complete . Note , I 'm excluding wire tests which add ~3 minutes to the test runtime . `` ` $ time go test $ ( go list ./ ... | grep -v wire ) ? github.com/google/go-cloud/aws [ no test files ] ok github.com/google/go-cloud/aws/awscloud ( cached ) [ no tests to run ] ok github.com/google/go-cloud/blob ( cached ) ? github.com/google/go-cloud/blob/driver [ no test files ] ok github.com/google/go-cloud/blob/fileblob ( cached ) ok github.com/google/go-cloud/blob/gcsblob ( cached ) ^C real 8m7.669s user 0m4.329s sys 0m1.283s `` ` Compare with : `` ` $ time go test -short $ ( go list ./ ... | grep -v wire ) ? github.com/google/go-cloud/aws [ no test files ] ok github.com/google/go-cloud/aws/awscloud ( cached ) [ no tests to run ] ok github.com/google/go-cloud/blob ( cached ) ? github.com/google/go-cloud/blob/driver [ no test files ] ok github.com/google/go-cloud/blob/fileblob ( cached ) ok github.com/google/go-cloud/blob/gcsblob ( cached ) ok github.com/google/go-cloud/blob/s3blob ( cached ) ? github.com/google/go-cloud/gcp [ no test files ] ok github.com/google/go-cloud/gcp/gcpcloud ( cached ) [ no tests to run ] ok github.com/google/go-cloud/health (
Make go test work on fresh clones __EoT__ I cloned this repo and then ran the following command , which I stopped after waiting 8 minutes for it to complete . Note , I 'm excluding wire tests which add ~3 minutes to the test runtime . `` ` $ time go test $ ( go list ./ ... | grep -v wire ) ? github.com/google/go-cloud/aws [ no test files ] ok github.com/google/go-cloud/aws/awscloud ( cached ) [ no tests to run ] ok github.com/google/go-cloud/blob ( cached ) ? github.com/google/go-cloud/blob/driver [ no test files ] ok github.com/google/go-cloud/blob/fileblob ( cached ) ok github.com/google/go-cloud/blob/gcsblob ( cached ) ^C real 8m7.669s user 0m4.329s sys 0m1.283s `` ` Compare with : `` ` $ time go test -short $ ( go list ./ ... | grep -v wire ) ? github.com/google/go-cloud/aws [ no test files ] ok github.com/google/go-cloud/aws/awscloud ( cached ) [ no tests to run ] ok github.com/google/go-cloud/blob ( cached ) ? github.com/google/go-cloud/blob/driver [ no test files ] ok github.com/google/go-cloud/blob/fileblob ( cached ) ok github.com/google/go-cloud/blob/gcsblob ( cached ) ok github.com/google/go-cloud/blob/s3blob ( cached ) ? github.com/google/go-cloud/gcp [ no test files ] ok github.com/google/go-cloud/gcp/gcpcloud ( cached ) [ no tests to run ] ok github.com/google/go-cloud/health (
Make go test work on fresh clones __EoT__ I cloned this repo and then ran the following command , which I stopped after waiting 8 minutes for it to complete . Note , I 'm excluding wire tests which add ~3 minutes to the test runtime . `` ` $ time go test $ ( go list ./ ... | grep -v wire ) ? github.com/google/go-cloud/aws [ no test files ] ok github.com/google/go-cloud/aws/awscloud ( cached ) [ no tests to run ] ok github.com/google/go-cloud/blob ( cached ) ? github.com/google/go-cloud/blob/driver [ no test files ] ok github.com/google/go-cloud/blob/fileblob ( cached ) ok github.com/google/go-cloud/blob/gcsblob ( cached ) ^C real 8m7.669s user 0m4.329s sys 0m1.283s `` ` Compare with : `` ` $ time go test -short $ ( go list ./ ... | grep -v wire ) ? github.com/google/go-cloud/aws [ no test files ] ok github.com/google/go-cloud/aws/awscloud ( cached ) [ no tests to run ] ok github.com/google/go-cloud/blob ( cached ) ? github.com/google/go-cloud/blob/driver [ no test files ] ok github.com/google/go-cloud/blob/fileblob ( cached ) ok github.com/google/go-cloud/blob/gcsblob ( cached ) ok github.com/google/go-cloud/blob/s3blob ( cached ) ? github.com/google/go-cloud/gcp [ no test files ] ok github.com/google/go-cloud/gcp/gcpcloud ( cached ) [ no tests to run ] ok github.com/google/go-cloud/health (
all : more instances of panic ( wire.Build ( ... ) ) found __EoT__ `` ` issactrotts-macbookpro : go-cloud issactrotts $ ack 'panic\ ( wire\.Build' wire/internal/wire/testdata/NamingWorstCase/foo/wire.go 26 : panic ( wire.Build ( provide ) ) wire/internal/wire/testdata/MultipleMissingInputs/foo/wire.go 24 : panic ( wire.Build ( provideBaz ) ) wire/internal/wire/testdata/NamingWorstCaseAllInOne/foo/foo.go 54 : panic ( wire.Build ( Provide ) ) wire/internal/wire/testdata/InjectWithPanic/foo/wire.go 24 : panic ( wire.Build ( provideMessage ) ) wire/README.md 354 : panic ( wire.Build ( /* ... */ ) ) wire/wire.go 36 : // panic ( wire.Build ( otherpkg.FooSet , myProviderFunc ) ) tests/gcp/app/inject.go 29 : panic ( wire.Build ( gcp/gcpcloud/example_test.go 60 : panic ( wire.Build ( aws/awscloud/example_test.go 60 : panic ( wire.Build ( `` `
runtimevar : Improve test coverage of `` Watch should block '' __EoT__ The tests do n't verify that Watch blocks when there 's no change to the error or to the Value . This is a bit tricky since such checking that something blocks forever is inherently flaky . For record/replay providers , it 's also complicated by the fact that we expect the reply to be exactly consistent .
server : health check endpoints are not logged __EoT__ I configure a server like so : `` ` opts : = & server.Options { RequestLogger : requestlog.NewNCSALogger ( os.Stdout , func ( err error ) { log.Printf ( `` Whoops ! Error : % s '' , err ) } ) , } s : = server.New ( opts ) // ... listen and serve , etc . `` ` When I send a request to ` / ` , I get a nice log message . When I send a request to ` /healthz/liveness ` or ` /healthz/readiness ` , I get no message .
runtimevar : Merge all provider-specific tests into a single conformance test suite __EoT__ Similar to what was done for blob .
pubsub : move gcppubsub endPoint into a helper __EoT__ @ vangent 's comment on https : //github.com/google/go-cloud/pull/774 , on line 55 of gcppubsub_test.go , `` The endpoint should probably go into a helper in gcppubsub , similar to what 's in runtimevar . OK to do in a separate PR . ''
pubsub : move gcppubsub endPoint into a helper __EoT__ @ vangent 's comment on https : //github.com/google/go-cloud/pull/774 , on line 55 of gcppubsub_test.go , `` The endpoint should probably go into a helper in gcppubsub , similar to what 's in runtimevar . OK to do in a separate PR . ''
pubsub : move gcppubsub endPoint into a helper __EoT__ @ vangent 's comment on https : //github.com/google/go-cloud/pull/774 , on line 55 of gcppubsub_test.go , `` The endpoint should probably go into a helper in gcppubsub , similar to what 's in runtimevar . OK to do in a separate PR . ''
pubsub : move gcppubsub endPoint into a helper __EoT__ @ vangent 's comment on https : //github.com/google/go-cloud/pull/774 , on line 55 of gcppubsub_test.go , `` The endpoint should probably go into a helper in gcppubsub , similar to what 's in runtimevar . OK to do in a separate PR . ''
travis : update vgo invocations __EoT__ ` vgo mod -vendor ` no longer works as of the latest vgo sync from Go upstream . We need to update our scripts and tooling ( Blocking all pull requests , but pointed out by @ vangent in # 293 and @ enocom in # 292 . )
Make tests catch invalid absolute URLs in methods . __EoT__ When adding support for new endpoints , it 's possible to make a mistake and use an absolute URL in the method implementation , rather than a relative one . We need to use relative URLs so that the ` Client.BaseURL ` field works properly in ` NewRequest ` method : https : //github.com/google/go-github/blob/0f6d3ce15ec23c92c74d014303a167a9a374dd7e/github/github.go # L112-L115 https : //github.com/google/go-github/blob/0f6d3ce15ec23c92c74d014303a167a9a374dd7e/github/github.go # L238-L247 E.g. , this is invalid : `` ` Go u : = fmt.Sprintf ( `` /admin/users/ % v/authorizations '' , username ) `` ` This is correct : `` ` Go u : = fmt.Sprintf ( `` admin/users/ % v/authorizations '' , username ) `` ` Including that leading slash has no negative effect on GitHub API clients , but it typically breaks GitHub Enterprise clients . That makes this type of mistake easy to miss during code review . Right now , our tests do not catch this , so we rely on human reviewers to catch it ( e.g. , this recently happened in # 749 ) . I 've thought about it and I think I found a way to make it so that our tests
Make tests catch invalid absolute URLs in methods . __EoT__ When adding support for new endpoints , it 's possible to make a mistake and use an absolute URL in the method implementation , rather than a relative one . We need to use relative URLs so that the ` Client.BaseURL ` field works properly in ` NewRequest ` method : https : //github.com/google/go-github/blob/0f6d3ce15ec23c92c74d014303a167a9a374dd7e/github/github.go # L112-L115 https : //github.com/google/go-github/blob/0f6d3ce15ec23c92c74d014303a167a9a374dd7e/github/github.go # L238-L247 E.g. , this is invalid : `` ` Go u : = fmt.Sprintf ( `` /admin/users/ % v/authorizations '' , username ) `` ` This is correct : `` ` Go u : = fmt.Sprintf ( `` admin/users/ % v/authorizations '' , username ) `` ` Including that leading slash has no negative effect on GitHub API clients , but it typically breaks GitHub Enterprise clients . That makes this type of mistake easy to miss during code review . Right now , our tests do not catch this , so we rely on human reviewers to catch it ( e.g. , this recently happened in # 749 ) . I 've thought about it and I think I found a way to make it so that our tests
Make tests catch invalid absolute URLs in methods . __EoT__ When adding support for new endpoints , it 's possible to make a mistake and use an absolute URL in the method implementation , rather than a relative one . We need to use relative URLs so that the ` Client.BaseURL ` field works properly in ` NewRequest ` method : https : //github.com/google/go-github/blob/0f6d3ce15ec23c92c74d014303a167a9a374dd7e/github/github.go # L112-L115 https : //github.com/google/go-github/blob/0f6d3ce15ec23c92c74d014303a167a9a374dd7e/github/github.go # L238-L247 E.g. , this is invalid : `` ` Go u : = fmt.Sprintf ( `` /admin/users/ % v/authorizations '' , username ) `` ` This is correct : `` ` Go u : = fmt.Sprintf ( `` admin/users/ % v/authorizations '' , username ) `` ` Including that leading slash has no negative effect on GitHub API clients , but it typically breaks GitHub Enterprise clients . That makes this type of mistake easy to miss during code review . Right now , our tests do not catch this , so we rely on human reviewers to catch it ( e.g. , this recently happened in # 749 ) . I 've thought about it and I think I found a way to make it so that our tests
Make tests catch invalid absolute URLs in methods . __EoT__ When adding support for new endpoints , it 's possible to make a mistake and use an absolute URL in the method implementation , rather than a relative one . We need to use relative URLs so that the ` Client.BaseURL ` field works properly in ` NewRequest ` method : https : //github.com/google/go-github/blob/0f6d3ce15ec23c92c74d014303a167a9a374dd7e/github/github.go # L112-L115 https : //github.com/google/go-github/blob/0f6d3ce15ec23c92c74d014303a167a9a374dd7e/github/github.go # L238-L247 E.g. , this is invalid : `` ` Go u : = fmt.Sprintf ( `` /admin/users/ % v/authorizations '' , username ) `` ` This is correct : `` ` Go u : = fmt.Sprintf ( `` admin/users/ % v/authorizations '' , username ) `` ` Including that leading slash has no negative effect on GitHub API clients , but it typically breaks GitHub Enterprise clients . That makes this type of mistake easy to miss during code review . Right now , our tests do not catch this , so we rely on human reviewers to catch it ( e.g. , this recently happened in # 749 ) . I 've thought about it and I think I found a way to make it so that our tests
Make tests catch invalid absolute URLs in methods . __EoT__ When adding support for new endpoints , it 's possible to make a mistake and use an absolute URL in the method implementation , rather than a relative one . We need to use relative URLs so that the ` Client.BaseURL ` field works properly in ` NewRequest ` method : https : //github.com/google/go-github/blob/0f6d3ce15ec23c92c74d014303a167a9a374dd7e/github/github.go # L112-L115 https : //github.com/google/go-github/blob/0f6d3ce15ec23c92c74d014303a167a9a374dd7e/github/github.go # L238-L247 E.g. , this is invalid : `` ` Go u : = fmt.Sprintf ( `` /admin/users/ % v/authorizations '' , username ) `` ` This is correct : `` ` Go u : = fmt.Sprintf ( `` admin/users/ % v/authorizations '' , username ) `` ` Including that leading slash has no negative effect on GitHub API clients , but it typically breaks GitHub Enterprise clients . That makes this type of mistake easy to miss during code review . Right now , our tests do not catch this , so we rely on human reviewers to catch it ( e.g. , this recently happened in # 749 ) . I 've thought about it and I think I found a way to make it so that our tests
Support new Repository Transfer API __EoT__ GitHub Developers API announcement : https : //developer.github.com/changes/2017-11-09-repository-transfer-api-preview/ GitHub v3 API developer docs : https : //developer.github.com/v3/repos/ # transfer-a-repository This would be a great project for a new contributor to this repo ! The new endpoint should fit nicely into one of the existing source files , possibly ` repos.go ` unless a better alternative is found and we can discuss it . All contributors and contributions are greatly appreciated . Thank you for your help in keeping this GitHub API client library for Go up-to-date !
Remove custom media type for Marketplace API __EoT__ GitHub Developer API announcement : https : //developer.github.com/changes/2018-05-22-marketplace-api/ The custom media type ` application/vnd.github.valkyrie-preview+json ` can be removed from ` github.go ` as the `` Marketplace API '' is no longer in preview but has become official .
Support preview Codes of Conduct API __EoT__ GitHub API announcement : https : //developer.github.com/changes/2017-05-23-coc-api/ GitHub Developers docs : https : //developer.github.com/v3/codes_of_conduct/
DiffURL & PatchURL is not available in PullRequest struct __EoT__ I need to access the diff and patch urls that is being returned in the json structure but these attributes are currently not available in the PullRequest struct . I will submit a pull request shortly that solves this issue .
[ Feature Request ] Support Branch protection for setting ` required_approving_review_count ` __EoT__ Per the [ rest api docs ] ( https : //developer.github.com/v3/repos/branches/ # update-branch-protection ) you can send ` required_approving_review_count ` as part of the ` required_pull_request_reviews ` object with an integer between 1-6 which represents the number of people required to approve the request in larger teams it is not uncommon to want more than a single approval before merging . ! [ image ] ( https : //user-images.githubusercontent.com/3145127/39460174-282769c8-4cb6-11e8-8d83-e01f5ef4c75c.png )
Support new user blocking APIs and webhook __EoT__ Announcement : https : //developer.github.com/changes/2017-02-28-user-blocking-apis-and-webhook/ GitHub API docs : - [ x ] https : //developer.github.com/v3/users/blocking/ - [ x ] https : //developer.github.com/v3/orgs/blocking/ - [ x ] https : //developer.github.com/v3/activity/events/types/ # orgblockevent
Support new user blocking APIs and webhook __EoT__ Announcement : https : //developer.github.com/changes/2017-02-28-user-blocking-apis-and-webhook/ GitHub API docs : - [ x ] https : //developer.github.com/v3/users/blocking/ - [ x ] https : //developer.github.com/v3/orgs/blocking/ - [ x ] https : //developer.github.com/v3/activity/events/types/ # orgblockevent
Support preview Organization Invitation API __EoT__ GitHub Developer API announcement : https : //developer.github.com/changes/2018-01-25-organization-invitation-api-preview/ Supporting this change will require the following : - [ ] create two new endpoints - [ ] add two new fields to an existing endpoint - [ ] add the new custom media type - [ ] create tests for the new endpoints and update the test ( s ) for the existing endpoint - [ ] run the generator as described in [ CONTRIBUTING.md ] ( https : //github.com/google/go-github/blob/master/CONTRIBUTING.md ) This would be a great PR for any new contributor to this repo or a new Go developer . All contributions are greatly appreciated ! Feel free to volunteer for any issue , and we can send you an invite to contribute to the repo ( which you then accept ) and the issue can be assigned to you so that others do n't attempt to duplicate the work . Thank you !
Github API for GetRef can return an array , but go-github does n't handle it . __EoT__ https : //developer.github.com/v3/git/refs/ # get-a-reference If my ref string matches as a prefix to 1 or more refs , an array is returned . In this case , I get this error at line 63 of git_refs.go : json : can not unmarshal array into Go value of type github.Reference
Support preview Organizations Membership API and new filter __EoT__ Announcement : https : //developer.github.com/changes/2017-01-16-audit-outside-collaborators-for-2fa/
Consistent Go naming for JSON fields __EoT__ As discussed in https : //github.com/google/go-github/pull/520 # discussion_r98727959 and https : //github.com/google/go-github/pull/602 # issuecomment-290255621 the naming of fields such as ` Total ` ( as opposed to ` TotalCount ` ) where the JSON name is ` total_count ` seems inconsistent and it would be nice to consolidate . However , this is a breaking API change . This issue has been opened to discuss the situation and see if it is worth breaking the API to have more consistency .
Add support for Commit Search preview API . __EoT__ Announcement : https : //developer.github.com/changes/2017-01-05-commit-search-api/ . Docs : https : //developer.github.com/v3/search/ # search-commits . This was announced today .
Add support for Commit Search preview API . __EoT__ Announcement : https : //developer.github.com/changes/2017-01-05-commit-search-api/ . Docs : https : //developer.github.com/v3/search/ # search-commits . This was announced today .
Unit tests should use local client instead of package-global client __EoT__ In https : //github.com/google/go-github/pull/732 it became clear that having a package-global ` client ` for unit testing will prevent the unit tests from being run in parallel in the future . Ideally , each test will have its own client ... something like this : `` ` go client , teardown : = setup ( ) defer teardown ( ) `` ` and the global ` client ` in ` github_test.go ` will be removed . This would make for a very large PR , but could possibly be done with the assistance of [ ` gofmt -r ` ] ( https : //blog.golang.org/go-fmt-your-code ) or your favorite text editor . : smile : This would also be a great project for a new contributor to the repo . Let it be known that all help is greatly appreciated ! Thank you in advance .
Unit tests should use local client instead of package-global client __EoT__ In https : //github.com/google/go-github/pull/732 it became clear that having a package-global ` client ` for unit testing will prevent the unit tests from being run in parallel in the future . Ideally , each test will have its own client ... something like this : `` ` go client , teardown : = setup ( ) defer teardown ( ) `` ` and the global ` client ` in ` github_test.go ` will be removed . This would make for a very large PR , but could possibly be done with the assistance of [ ` gofmt -r ` ] ( https : //blog.golang.org/go-fmt-your-code ) or your favorite text editor . : smile : This would also be a great project for a new contributor to the repo . Let it be known that all help is greatly appreciated ! Thank you in advance .
Unit tests should use local client instead of package-global client __EoT__ In https : //github.com/google/go-github/pull/732 it became clear that having a package-global ` client ` for unit testing will prevent the unit tests from being run in parallel in the future . Ideally , each test will have its own client ... something like this : `` ` go client , teardown : = setup ( ) defer teardown ( ) `` ` and the global ` client ` in ` github_test.go ` will be removed . This would make for a very large PR , but could possibly be done with the assistance of [ ` gofmt -r ` ] ( https : //blog.golang.org/go-fmt-your-code ) or your favorite text editor . : smile : This would also be a great project for a new contributor to the repo . Let it be known that all help is greatly appreciated ! Thank you in advance .
Unit tests should use local client instead of package-global client __EoT__ In https : //github.com/google/go-github/pull/732 it became clear that having a package-global ` client ` for unit testing will prevent the unit tests from being run in parallel in the future . Ideally , each test will have its own client ... something like this : `` ` go client , teardown : = setup ( ) defer teardown ( ) `` ` and the global ` client ` in ` github_test.go ` will be removed . This would make for a very large PR , but could possibly be done with the assistance of [ ` gofmt -r ` ] ( https : //blog.golang.org/go-fmt-your-code ) or your favorite text editor . : smile : This would also be a great project for a new contributor to the repo . Let it be known that all help is greatly appreciated ! Thank you in advance .
Support Marketplace ( preview ) API . __EoT__ GitHub API Docs : https : //developer.github.com/v3/apps/marketplace/ . Not seeing an announcement blog post mentioning this , but there are new API endpoints available , as documented at the URL above . This issue is to track their implementation . A sub-task here is supporting the ` MarketplacePurchaseEvent ` event , which is tracked in a separate issue # 704 .
Support Marketplace ( preview ) API . __EoT__ GitHub API Docs : https : //developer.github.com/v3/apps/marketplace/ . Not seeing an announcement blog post mentioning this , but there are new API endpoints available , as documented at the URL above . This issue is to track their implementation . A sub-task here is supporting the ` MarketplacePurchaseEvent ` event , which is tracked in a separate issue # 704 .
Support Marketplace ( preview ) API . __EoT__ GitHub API Docs : https : //developer.github.com/v3/apps/marketplace/ . Not seeing an announcement blog post mentioning this , but there are new API endpoints available , as documented at the URL above . This issue is to track their implementation . A sub-task here is supporting the ` MarketplacePurchaseEvent ` event , which is tracked in a separate issue # 704 .
Support Marketplace ( preview ) API . __EoT__ GitHub API Docs : https : //developer.github.com/v3/apps/marketplace/ . Not seeing an announcement blog post mentioning this , but there are new API endpoints available , as documented at the URL above . This issue is to track their implementation . A sub-task here is supporting the ` MarketplacePurchaseEvent ` event , which is tracked in a separate issue # 704 .
Support preview Protected Branches API __EoT__ GitHub Developer API announcement : https : //developer.github.com/changes/2018-03-16-protected-branches-required-approving-reviews/ This would be a great PR for any new contributor to this repo or a new Go developer . All contributions are greatly appreciated ! Feel free to volunteer for any issue , and we can send you an invite to contribute to the repo ( which you then accept after you enable two-factor authentication ) and the issue can be assigned to you so that others do n't attempt to duplicate the work . Please check out our [ CONTRIBUTING.md ] ( https : //github.com/google/go-github/blob/master/CONTRIBUTING.md ) guide to get started . Thank you !
Make use of testHeader and testMethod consistent __EoT__ As found out while reviewing https : //github.com/google/go-github/pull/817 , we have some inconsistencies in our tests where some of the tests do n't use the ` testHeader ` and ` testMethod ` functions to test request header and method . One of the defaulter is https : //github.com/google/go-github/blob/master/github/git_trees_test.go # L22 . This is completely harmless . Though it 's important to make sure it does n't cause further confusion for the new contributors as it did in that PR initially . Via this issue , I 'd like to point it out and mention that I 'd like to fix this inconsistency .
Support the new Checks API ( beta ) __EoT__ The new Checks API was recently announced in https : //developer.github.com/changes/2018-05-07-new-checks-api-public-beta/ While it 's currently in beta and supports only GithHubApps , it might be worthwhile adding support . I 'm happy to get an initial PR going for this .
Support breaking changes to the Projects API preview __EoT__ GitHub Announcement : https : //developer.github.com/changes/2017-09-01-breaking-changes-to-projects-api-preview-and-webhooks-date-format/ To resolve this issue , it appears that unit tests will need to be updated for the time parsing functions that process webhook payloads . It is possible that the new format is already supported , but it is more likely that the parsing format needs to be changed slightly . I personally think that it might be nice to support both formats ( old and new ) in the code base with a `` TODO '' to remove support for the old format once it is no longer being sent by GitHub , but this is completely up for discussion and I do n't have my heart set on this . ( One nice serendipity of this would be that no breaking changes would be made to the API . ) Thank you in advance for contributing to this open source project ! Your assistance is greatly appreciated .
Prefer to explicitly return nil when there is no error for code readability __EoT__ A common pattern in this repo is : `` ` go if err ! = nil { return ... , err } return ... , err `` ` and we would like to change the last return to explicitly ` return ... , nil ` for Go readability .
Support preview Lock Reason via the Lock Issue API __EoT__ GitHub Developer API announcement : https : //developer.github.com/changes/2018-01-10-lock-reason-api-preview/ Supporting this change will require the following : - [ ] add a new preview media type ( ` application/vnd.github.sailor-v-preview+json ` ) to ` github.go ` , - [ ] add new ` active_lock_reason ` and ` lock_reason ` fields ( ` ActiveLockReason ` and ` LockReason ` in Go , respectively ) to ` Issue ` and ` PullRequest ` responses , - [ ] add a new ( optional ) ` lock_reason ` field ( ` LockReason ` in Go ) to the ` IssuesService.Lock ` request ( which will involve a breaking API change , so that the ` libraryVersion ` will need to be bumped ) , - [ ] add a new ` lock_reason ` ( ` LockReason ` in Go ) field to ` IssueEvent ` and ` PullRequestEvent ` , - [ ] write supporting tests , and - [ ] bump the ` libraryVersion ` . This is slightly more involved than some of the other recent enhancements , but would still be a good challenge for any new contributor to this repo or
Function to get the Delivery ID for a webhook request __EoT__ I would like to add a function to get the Delivery ID header ( ` X-GitHub-Delivery ` ) for a webhook request . This would essentially be the same as [ ` WebHookType ` ] ( https : //github.com/google/go-github/blob/master/github/messages.go # L162 ) but a different header . If there are no problems with this , I would be glad to open a PR .
struct for PullRequest under PullRequestEvent does not have `` _links '' __EoT__ Hi , When my webhook is configured for Pull Requests event , the delivered payload has `` _links '' within `` pull_request '' object whereas I do n't see it defined under https : //godoc.org/github.com/google/go-github/github # PullRequest struct . Can you please confirm if this sdk is compatible with v3 version of github APIs ?
Apps.ListUserRepos may return message `` specify a custom media type in the 'Accept ' header '' . __EoT__ When I am trying to use ` Apps.ListUserRepos ` , I got message like this : ! [ image ] ( https : //user-images.githubusercontent.com/14567045/33372247-3e8de840-d538-11e7-944a-75e1d68be1ea.png ) I know this can be fix by adding following accept header : `` ` go // in github/github.go line 85~86 // https : //developer.github.com/changes/2016-09-14-Integrations-Early-Access/ mediaTypeIntegrationPreview = `` application/vnd.github.machine-man-preview+json '' `` ` And it can be fix by add following to ` ListUserRepos ` function in ` github/apps_installation.go ` `` ` go req.Header.Set ( `` Accept '' , mediaTypeIntegrationPreview ) `` ` I can provide a PR to fix this . I noticed that there is a TODO comment in other function says that ` TODO : remove custom Accept header when this API fully launches. ` , so I will keep this in my fix .
authorizations API support __EoT__ https : //developer.github.com/v3/oauth_authorizations/ One tricky part is that it only accepts basic auth , so I think it 'd require users of the library to implement a simple RoundTripper that added this header ( and then use that as the HTTP client transport for go-github ) .
authorizations API support __EoT__ https : //developer.github.com/v3/oauth_authorizations/ One tricky part is that it only accepts basic auth , so I think it 'd require users of the library to implement a simple RoundTripper that added this header ( and then use that as the HTTP client transport for go-github ) .
Support preview Nested Teams API __EoT__ GitHub Announcement : https : //developer.github.com/changes/2017-08-30-preview-nested-teams/ This involves adding new endpoints and changing existing endpoints according to the blog post . Successful completion and closing of this issue involves addressing all the changes described in the GitHub announcement . Note that it does n't have to all be done in a single PR . It is always a good idea to carefully document what portions are being addressed in any given PR and if there is still work yet to be done . Thank you in advance for contributing to this open source project ! Your assistance is greatly appreciated .
Support preview Nested Teams API __EoT__ GitHub Announcement : https : //developer.github.com/changes/2017-08-30-preview-nested-teams/ This involves adding new endpoints and changing existing endpoints according to the blog post . Successful completion and closing of this issue involves addressing all the changes described in the GitHub announcement . Note that it does n't have to all be done in a single PR . It is always a good idea to carefully document what portions are being addressed in any given PR and if there is still work yet to be done . Thank you in advance for contributing to this open source project ! Your assistance is greatly appreciated .
Add support for new repository traffic API __EoT__ Announcement : https : //developer.github.com/changes/2016-08-15-traffic-api-preview/ Docs : https : //developer.github.com/v3/repos/traffic/ This is a tad bit more advanced , but would still be a good beginner task . It does n't require adding a new service ( just put these methods on the existing RepositoriesService ) , but likely does involve adding a few new data types .
Transfer request has incorrect json name for specifying team ids __EoT__ As pointed out in the [ recent comment on the PR ] ( https : //github.com/google/go-github/pull/788 # discussion_r201838235 ) for adding support , the json name should be ` team_ids ` , not ` team_id ` .
Add support for MarketplacePurchaseEvent __EoT__ When looking through the [ webhook events ] ( https : //developer.github.com/webhooks/ ) I noticed that we currently do not support the [ MarketplacePurchaseEvent ] ( https : //developer.github.com/v3/activity/events/types/ # marketplacepurchaseevent ) in [ ` messages.go ` ] ( https : //github.com/google/go-github/blob/master/github/messages.go # L55 ) . It would be nice to add this support and it would be a great project for a new contributor to this repo .
Add support for MarketplacePurchaseEvent __EoT__ When looking through the [ webhook events ] ( https : //developer.github.com/webhooks/ ) I noticed that we currently do not support the [ MarketplacePurchaseEvent ] ( https : //developer.github.com/v3/activity/events/types/ # marketplacepurchaseevent ) in [ ` messages.go ` ] ( https : //github.com/google/go-github/blob/master/github/messages.go # L55 ) . It would be nice to add this support and it would be a great project for a new contributor to this repo .
Add support for MarketplacePurchaseEvent __EoT__ When looking through the [ webhook events ] ( https : //developer.github.com/webhooks/ ) I noticed that we currently do not support the [ MarketplacePurchaseEvent ] ( https : //developer.github.com/v3/activity/events/types/ # marketplacepurchaseevent ) in [ ` messages.go ` ] ( https : //github.com/google/go-github/blob/master/github/messages.go # L55 ) . It would be nice to add this support and it would be a great project for a new contributor to this repo .
Add support for new Pull Request Reviews preview API . __EoT__ Announcement : https : //developer.github.com/changes/2016-12-14-reviews-api/ . Docs : https : //developer.github.com/v3/pulls/reviews/ . This was announced today .
handling custom media types __EoT__ A handful of API methods ( like [ issues ] ( http : //developer.github.com/v3/issues/ ) ) support the use of [ custom mime ] ( http : //developer.github.com/v3/media/ ) types to return additional fields in the response , using different formats . As currently designed , go-github does n't have a simple way to expose this , since most HTTP-specific details are hidden from the caller . We could probably just add it to the Options parameter for the given method , but that feels a little weird . Additionally , I do n't yet have a real use-case for needing to use these custom mime-types . Therefore , I 'm currently leaning toward punting on this until someone actually needs it . So if you need it , please comment here , and we can look at how to work that into the library .
review_requested PullRequestEvent ( webhook ) does not unmarshall properly for RequestedReviewers field __EoT__ The ` PullRequestEvent ` struct is set up incorrectly for webhooks . There is no field called RequestedReviewers in the payload for a ` PullRequestEvent ` . There is a ` RequestedReviewer ` field but the ` RequestedReviewers ` field is located inside the ` PullRequest ` object in the payload . Therefore , ` RequestedReviewers ` should actually be put inside ` PullRequestEvent.PullRequest ` , not on the highest level of ` PullRequestEvent ` . The payload accepts only one ` requested_reviewer ` at a time , so if you request two reviewers at once , it will send two webhook events with different ` requested_reviewer ` I am guessing that this was a correct implementation at some point because this was tested and implemented earlier , but it seems that Github has changed their layout and there is no way to be sure online because it does not provide an example JSON response for ` review_requested ` actions for ` PullRequestEvent ` where ` requested_reviewers ` is population in the example JSON . The doc is here : https : //developer.github.com/v3/activity/events/types/ # pullrequestevent but it
Support new preview Label API improvements __EoT__ Following GitHub announcement https : //github.com/blog/2505-label-improvements-emoji-descriptions-and-more , it would be useful to be able to define and access to label descriptions .
search API __EoT__ starting work on the new [ search API ] ( http : //developer.github.com/v3/search/ )
search API __EoT__ starting work on the new [ search API ] ( http : //developer.github.com/v3/search/ )
Update Traffic API methods to use new timestamps __EoT__ There was a lot of discussion on # 418 due to the fact that the Traffic API was using a different timestamp format than the rest of the API . That 's now been fixed : https : //developer.github.com/changes/2016-10-26-traffic-api-update/ @ succo : you wan na take a stab at updating this , since you 're most familiar with that code ?
Update Traffic API methods to use new timestamps __EoT__ There was a lot of discussion on # 418 due to the fact that the Traffic API was using a different timestamp format than the rest of the API . That 's now been fixed : https : //developer.github.com/changes/2016-10-26-traffic-api-update/ @ succo : you wan na take a stab at updating this , since you 're most familiar with that code ?
Support preview Installations API - updated routes for adding or removing a repository __EoT__ GitHub Developers announcment : https : //developer.github.com/changes/2017-06-30-installations-adding-removing-a-repository/ GitHub API docs : https : //developer.github.com/v3/apps/installations/ # add-repository-to-installation
Support preview Installations API - updated routes for adding or removing a repository __EoT__ GitHub Developers announcment : https : //developer.github.com/changes/2017-06-30-installations-adding-removing-a-repository/ GitHub API docs : https : //developer.github.com/v3/apps/installations/ # add-repository-to-installation
Support preview Installations API - updated routes for adding or removing a repository __EoT__ GitHub Developers announcment : https : //developer.github.com/changes/2017-06-30-installations-adding-removing-a-repository/ GitHub API docs : https : //developer.github.com/v3/apps/installations/ # add-repository-to-installation
What happens if you Git.CreateCommit ( ... , nil ) or PullRequests.Edit ( ... , nil ) ? __EoT__ ` GitService.CreateCommit ` is [ documented ] ( https : //godoc.org/github.com/google/go-github/github # GitService.CreateCommit ) as : > CreateCommit creates a new commit in a repository . > > The commit.Committer is optional and will be filled with the commit.Author data if omitted . If the commit.Author is omitted , it will be filled in with the authenticated user â€™ s information and the current date . > > GitHub API docs : http : //developer.github.com/v3/git/commits/ # create-a-commit If it 's legal , what happens if a user calls ` CreateCommit ( `` owner '' , `` repo '' , nil ) ` ? Should we document it , because I do n't think it 's intuitive or clear . If it 's not legal , that should be documented . -- - Same question for ` PullRequestsService.Edit ` , [ documented ] ( https : //godoc.org/github.com/google/go-github/github # PullRequestsService.Edit ) as : > Edit a pull request . > > GitHub API docs : https : //developer.github.com/v3/pulls/ # update-a-pull-request -- - In case nil values are not legal , why are pointers
AppEngine now supports Go 1.8 __EoT__ AppEngine now [ supports Go 1.8 ] ( https : //cloud.google.com/appengine/docs/standard/go/release-notes # october_25_2018 ) . This means that you no longer need to run the ` gofmt ` command [ here ] ( https : //github.com/google/go-github # google-app-engine ) .
Support new Webhook Event for Repository Vulnerability Alerts __EoT__ GitHub Developer API announcement : https : //developer.github.com/changes/2018-04-24-preview-dependency-graph-and-vulnerability-hooks/
Support new Webhook Event for Repository Vulnerability Alerts __EoT__ GitHub Developer API announcement : https : //developer.github.com/changes/2018-04-24-preview-dependency-graph-and-vulnerability-hooks/
Search Commits API Results Problem __EoT__ Issue : When calling search commits api ( go-github/github/search.go:84 ) returned results contain correct number of items but each item properties are empty . As per github search docs https : //developer.github.com/v3/search/ # search-commits Search Commits is returning results as : `` ` { `` total_count '' : 1 , `` incomplete_results '' : false , `` items '' : [ { `` url '' : `` https : //api.github.com/repos/octocat/Spoon-Knife/commits/bb4cc8d3b2e14b3af5df699876dd4ff3acd00b7f '' , `` sha '' : `` bb4cc8d3b2e14b3af5df699876dd4ff3acd00b7f '' , `` html_url '' : `` https : //github.com/octocat/Spoon-Knife/commit/bb4cc8d3b2e14b3af5df699876dd4ff3acd00b7f '' , `` comments_url '' : `` https : //api.github.com/repos/octocat/Spoon-Knife/commits/bb4cc8d3b2e14b3af5df699876dd4ff3acd00b7f/comments '' , `` commit '' : { `` url '' : `` https : //api.github.com/repos/octocat/Spoon-Knife/git/commits/bb4cc8d3b2e14b3af5df699876dd4ff3acd00b7f '' , `` author '' : { `` date '' : `` 2014-02-04T14:38:36-08:00 '' , `` name '' : `` The Octocat '' , `` email '' : `` octocat @ nowhere.com '' } , `` committer '' : { `` date '' : `` 2014-02-12T15:18:55-08:00 '' , `` name '' : `` The Octocat '' , `` email '' : `` octocat @ nowhere.com '' } , `` message '' : `` Create styles.css and updated README '' , ``
Support preview Hovercard API __EoT__ GitHub Developer API announcement : https : //developer.github.com/changes/2018-03-21-hovercard-api-preview/ This would be a great PR for any new contributor to this repo or a new Go developer . All contributions are greatly appreciated ! Feel free to volunteer for any issue , and we can send you an invite to contribute to the repo ( which you then accept after you enable two-factor authentication ) and the issue can be assigned to you so that others do n't attempt to duplicate the work . Please check out our [ CONTRIBUTING.md ] ( https : //github.com/google/go-github/blob/master/CONTRIBUTING.md ) guide to get started . Thank you !
IsTeamMember is Deprecated __EoT__ The API endpoint backing ` OrganizationsService.IsTeamMember ` is deprecated in favor of the ` GetTeamMembership ` endpoint : https : //developer.github.com/v3/orgs/teams/ # get-team-member . To discourage API users from unknowingly calling deprecated methods , the documentation for the method should reflect this . Disagree/Agree ?
Support preview Team Review Requests API __EoT__ GitHub Announcement : https : //developer.github.com/changes/2017-07-26-team-review-request-thor-preview/ GitHub API Docs : https : //developer.github.com/v3/pulls/review_requests/ Note that the announcement describes a breaking change which may involve a bump of this package 's version number .
Remove custom media type for Git Signing API __EoT__ GitHub Developer announcement : https : //developer.github.com/changes/2017-10-12-git-signing/ Theoretically , we can remove the custom media type ` application/vnd.github.cryptographer-preview ` but I have sent in a request to the GitHub Support team to clarify if removing this will cause problems for GitHub Enterprise customers . I will update this issue when I hear back .
Add new webhooks for Labels and Milestones __EoT__ Announcement : https : //developer.github.com/changes/2016-10-27-new-label-and-milestone-webhooks/ This should be very straightforward ... just defining the types for these new events .
Add support for Preview Review Requests API __EoT__ Announcement : https : //developer.github.com/changes/2016-12-16-review-requests-api/
OrganizationsService.ListPendingOrgInvitations does not work __EoT__ The argument of ` ListPendingOrgInvitations ` is to pass an integer as ` org ` , but this is a mistake and I think you should pass the string correctly .
Change Check* ( ) and other related functions to Is* ( ) __EoT__ As discussed in issue # 34 , the convention that has been previously used to name functions that check if something exists in a given state has been Check* ( ) . For example , [ CheckAssignee ( ) ] ( https : //github.com/google/go-github/blob/master/github/issues_assignees.go # L28 ) . In order to make these more consistent to the style of the go stdlib functions ( see [ math ] ( http : //golang.org/pkg/math/ ) and [ os ] ( http : //golang.org/pkg/os/ ) ) , these should be changed to Is* ( ) ( i.e . CheckAssignee ( ) - > IsAssignee ( ) ) . Some of these functions are not always in the Check* ( ) form , such as [ Starred ( ) ] ( https : //github.com/google/go-github/blob/master/github/gists.go # L212 ) . A good place to start would be to search for parseBoolResponse ( ) . Most of these functions use this to determine their result .
Proposal : Add GetByID methods to UsersService , RepositoriesService , with caveat . __EoT__ I 'd like to hear your thoughts on the following proposal . I know this project is Go client library for accessing the GitHub API . It tries to stay very current with GitHub API changes . It implements support for new APIs early , even when they 're still in preview period and may result in changes before preview period ends . As a result , this Go package tries to maintain a stable API , but when breaking GitHub API changes happen that necessitate otherwise , it follows suit . I propose considering adding two methods , but read below for details : `` ` Go // GetByID fetches a user . func ( s *UsersService ) GetByID ( id int ) ( *User , *Response , error ) { ... } // GetByID fetches a repository . func ( s *RepositoriesService ) GetByID ( id int ) ( *Repository , *Response , error ) { ... } `` ` # # # Motivation Right now , ` UsersService ` exposes two methods for getting users , one is by ` login ` ,
Add convenience helper for creating client with custom ( enterprise ) URL . __EoT__ I am trying to write a little app that connects to a Github Enterprise instance . I would like to be able to pass in the base enterprise URL , and have it override the default one . As far as I can see ( from the tests and research I have done ) the API for Github Enterprise includes all of the regular functionality , plus some more Enterprise-only features ( but perhaps someone knows different ? ) . Because of this , just adding the ability to pass in the URL would already be of great benefit . I have created this issue to discuss , but have also already made an attempt at the changes and will submit a PR , in case this issue is accepted .
Add convenience helper for creating client with custom ( enterprise ) URL . __EoT__ I am trying to write a little app that connects to a Github Enterprise instance . I would like to be able to pass in the base enterprise URL , and have it override the default one . As far as I can see ( from the tests and research I have done ) the API for Github Enterprise includes all of the regular functionality , plus some more Enterprise-only features ( but perhaps someone knows different ? ) . Because of this , just adding the ability to pass in the URL would already be of great benefit . I have created this issue to discuss , but have also already made an attempt at the changes and will submit a PR , in case this issue is accepted .
Add convenience helper for creating client with custom ( enterprise ) URL . __EoT__ I am trying to write a little app that connects to a Github Enterprise instance . I would like to be able to pass in the base enterprise URL , and have it override the default one . As far as I can see ( from the tests and research I have done ) the API for Github Enterprise includes all of the regular functionality , plus some more Enterprise-only features ( but perhaps someone knows different ? ) . Because of this , just adding the ability to pass in the URL would already be of great benefit . I have created this issue to discuss , but have also already made an attempt at the changes and will submit a PR , in case this issue is accepted .
Add convenience helper for creating client with custom ( enterprise ) URL . __EoT__ I am trying to write a little app that connects to a Github Enterprise instance . I would like to be able to pass in the base enterprise URL , and have it override the default one . As far as I can see ( from the tests and research I have done ) the API for Github Enterprise includes all of the regular functionality , plus some more Enterprise-only features ( but perhaps someone knows different ? ) . Because of this , just adding the ability to pass in the URL would already be of great benefit . I have created this issue to discuss , but have also already made an attempt at the changes and will submit a PR , in case this issue is accepted .
Support update to organization and team invitations APIs __EoT__ Announcement : https : //developer.github.com/changes/2017-01-12-organization-team-invitation-update/
Support update to organization and team invitations APIs __EoT__ Announcement : https : //developer.github.com/changes/2017-01-12-organization-team-invitation-update/
Support update to organization and team invitations APIs __EoT__ Announcement : https : //developer.github.com/changes/2017-01-12-organization-team-invitation-update/
Support update to organization and team invitations APIs __EoT__ Announcement : https : //developer.github.com/changes/2017-01-12-organization-team-invitation-update/
Create `` Increment a Decimal-Coded Number '' Question __EoT__
Create sort-scrambled-itinerary question __EoT__
Create sort-scrambled-itinerary question __EoT__
Create sort-scrambled-itinerary question __EoT__
Create sort-scrambled-itinerary question __EoT__
Create Alien_language_alphabet Question __EoT__
Create Alien_language_alphabet Question __EoT__
Feedback state lost when switching between windows or reload __EoT__ Expected behavior : When switching between questions , feedback state is maintained . Since code is saved , it would be odd to be able to get back to where you left off in terms of your written code but not be able to get back to where you were in terms of the feedback . Observed behavior : Switching between questions or reloading the page causes the app to lose state on the feedback window . Steps to reproduce : Modify and run code . Observe feedback . Reload page or go to another question and back again . Observe that feedback is reset .
[ UI issue ] Console display starts from middle of the message __EoT__ Repro Steps : 1 . Open `` app.html '' . 2 . Navigate to question 1 . 3 . Change the returned string from `` '' to a random string ( e.g. , `` abcd '' ) which will produce wrong answer . 4 . Click `` Run '' . 5 . The console now shows the input ( `` moo cow bark dog '' ) and the output ( `` abcd '' ) , and the expected output ( `` oom woc krab god '' ) if scrolling down the window . ! [ screen shot 2017-04-03 at 11 49 40 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625396/c1d557a2-1863-11e7-9713-ef834d2e5e24.png ) 6 . Change the returned string from `` abcd '' to another random string ( suppose the user is fixing the bug in the code ) . 7 . Run the code again . 8 . The console shows the expected output , but the actual output is not seen until you scroll up the window . ! [ screen shot 2017-04-03 at 11 51 00 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625434/dbe4f4a4-1863-11e7-88b7-70e1e9ff1c67.png ) Expected behavior : The console
[ UI issue ] Console display starts from middle of the message __EoT__ Repro Steps : 1 . Open `` app.html '' . 2 . Navigate to question 1 . 3 . Change the returned string from `` '' to a random string ( e.g. , `` abcd '' ) which will produce wrong answer . 4 . Click `` Run '' . 5 . The console now shows the input ( `` moo cow bark dog '' ) and the output ( `` abcd '' ) , and the expected output ( `` oom woc krab god '' ) if scrolling down the window . ! [ screen shot 2017-04-03 at 11 49 40 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625396/c1d557a2-1863-11e7-9713-ef834d2e5e24.png ) 6 . Change the returned string from `` abcd '' to another random string ( suppose the user is fixing the bug in the code ) . 7 . Run the code again . 8 . The console shows the expected output , but the actual output is not seen until you scroll up the window . ! [ screen shot 2017-04-03 at 11 51 00 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625434/dbe4f4a4-1863-11e7-88b7-70e1e9ff1c67.png ) Expected behavior : The console
[ UI issue ] Console display starts from middle of the message __EoT__ Repro Steps : 1 . Open `` app.html '' . 2 . Navigate to question 1 . 3 . Change the returned string from `` '' to a random string ( e.g. , `` abcd '' ) which will produce wrong answer . 4 . Click `` Run '' . 5 . The console now shows the input ( `` moo cow bark dog '' ) and the output ( `` abcd '' ) , and the expected output ( `` oom woc krab god '' ) if scrolling down the window . ! [ screen shot 2017-04-03 at 11 49 40 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625396/c1d557a2-1863-11e7-9713-ef834d2e5e24.png ) 6 . Change the returned string from `` abcd '' to another random string ( suppose the user is fixing the bug in the code ) . 7 . Run the code again . 8 . The console shows the expected output , but the actual output is not seen until you scroll up the window . ! [ screen shot 2017-04-03 at 11 51 00 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625434/dbe4f4a4-1863-11e7-88b7-70e1e9ff1c67.png ) Expected behavior : The console
[ UI issue ] Console display starts from middle of the message __EoT__ Repro Steps : 1 . Open `` app.html '' . 2 . Navigate to question 1 . 3 . Change the returned string from `` '' to a random string ( e.g. , `` abcd '' ) which will produce wrong answer . 4 . Click `` Run '' . 5 . The console now shows the input ( `` moo cow bark dog '' ) and the output ( `` abcd '' ) , and the expected output ( `` oom woc krab god '' ) if scrolling down the window . ! [ screen shot 2017-04-03 at 11 49 40 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625396/c1d557a2-1863-11e7-9713-ef834d2e5e24.png ) 6 . Change the returned string from `` abcd '' to another random string ( suppose the user is fixing the bug in the code ) . 7 . Run the code again . 8 . The console shows the expected output , but the actual output is not seen until you scroll up the window . ! [ screen shot 2017-04-03 at 11 51 00 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625434/dbe4f4a4-1863-11e7-88b7-70e1e9ff1c67.png ) Expected behavior : The console
[ UI issue ] Console display starts from middle of the message __EoT__ Repro Steps : 1 . Open `` app.html '' . 2 . Navigate to question 1 . 3 . Change the returned string from `` '' to a random string ( e.g. , `` abcd '' ) which will produce wrong answer . 4 . Click `` Run '' . 5 . The console now shows the input ( `` moo cow bark dog '' ) and the output ( `` abcd '' ) , and the expected output ( `` oom woc krab god '' ) if scrolling down the window . ! [ screen shot 2017-04-03 at 11 49 40 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625396/c1d557a2-1863-11e7-9713-ef834d2e5e24.png ) 6 . Change the returned string from `` abcd '' to another random string ( suppose the user is fixing the bug in the code ) . 7 . Run the code again . 8 . The console shows the expected output , but the actual output is not seen until you scroll up the window . ! [ screen shot 2017-04-03 at 11 51 00 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625434/dbe4f4a4-1863-11e7-88b7-70e1e9ff1c67.png ) Expected behavior : The console
[ UI issue ] Console display starts from middle of the message __EoT__ Repro Steps : 1 . Open `` app.html '' . 2 . Navigate to question 1 . 3 . Change the returned string from `` '' to a random string ( e.g. , `` abcd '' ) which will produce wrong answer . 4 . Click `` Run '' . 5 . The console now shows the input ( `` moo cow bark dog '' ) and the output ( `` abcd '' ) , and the expected output ( `` oom woc krab god '' ) if scrolling down the window . ! [ screen shot 2017-04-03 at 11 49 40 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625396/c1d557a2-1863-11e7-9713-ef834d2e5e24.png ) 6 . Change the returned string from `` abcd '' to another random string ( suppose the user is fixing the bug in the code ) . 7 . Run the code again . 8 . The console shows the expected output , but the actual output is not seen until you scroll up the window . ! [ screen shot 2017-04-03 at 11 51 00 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625434/dbe4f4a4-1863-11e7-88b7-70e1e9ff1c67.png ) Expected behavior : The console
[ UI issue ] Console display starts from middle of the message __EoT__ Repro Steps : 1 . Open `` app.html '' . 2 . Navigate to question 1 . 3 . Change the returned string from `` '' to a random string ( e.g. , `` abcd '' ) which will produce wrong answer . 4 . Click `` Run '' . 5 . The console now shows the input ( `` moo cow bark dog '' ) and the output ( `` abcd '' ) , and the expected output ( `` oom woc krab god '' ) if scrolling down the window . ! [ screen shot 2017-04-03 at 11 49 40 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625396/c1d557a2-1863-11e7-9713-ef834d2e5e24.png ) 6 . Change the returned string from `` abcd '' to another random string ( suppose the user is fixing the bug in the code ) . 7 . Run the code again . 8 . The console shows the expected output , but the actual output is not seen until you scroll up the window . ! [ screen shot 2017-04-03 at 11 51 00 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625434/dbe4f4a4-1863-11e7-88b7-70e1e9ff1c67.png ) Expected behavior : The console
[ UI issue ] Console display starts from middle of the message __EoT__ Repro Steps : 1 . Open `` app.html '' . 2 . Navigate to question 1 . 3 . Change the returned string from `` '' to a random string ( e.g. , `` abcd '' ) which will produce wrong answer . 4 . Click `` Run '' . 5 . The console now shows the input ( `` moo cow bark dog '' ) and the output ( `` abcd '' ) , and the expected output ( `` oom woc krab god '' ) if scrolling down the window . ! [ screen shot 2017-04-03 at 11 49 40 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625396/c1d557a2-1863-11e7-9713-ef834d2e5e24.png ) 6 . Change the returned string from `` abcd '' to another random string ( suppose the user is fixing the bug in the code ) . 7 . Run the code again . 8 . The console shows the expected output , but the actual output is not seen until you scroll up the window . ! [ screen shot 2017-04-03 at 11 51 00 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625434/dbe4f4a4-1863-11e7-88b7-70e1e9ff1c67.png ) Expected behavior : The console
[ UI issue ] Console display starts from middle of the message __EoT__ Repro Steps : 1 . Open `` app.html '' . 2 . Navigate to question 1 . 3 . Change the returned string from `` '' to a random string ( e.g. , `` abcd '' ) which will produce wrong answer . 4 . Click `` Run '' . 5 . The console now shows the input ( `` moo cow bark dog '' ) and the output ( `` abcd '' ) , and the expected output ( `` oom woc krab god '' ) if scrolling down the window . ! [ screen shot 2017-04-03 at 11 49 40 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625396/c1d557a2-1863-11e7-9713-ef834d2e5e24.png ) 6 . Change the returned string from `` abcd '' to another random string ( suppose the user is fixing the bug in the code ) . 7 . Run the code again . 8 . The console shows the expected output , but the actual output is not seen until you scroll up the window . ! [ screen shot 2017-04-03 at 11 51 00 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625434/dbe4f4a4-1863-11e7-88b7-70e1e9ff1c67.png ) Expected behavior : The console
[ UI issue ] Console display starts from middle of the message __EoT__ Repro Steps : 1 . Open `` app.html '' . 2 . Navigate to question 1 . 3 . Change the returned string from `` '' to a random string ( e.g. , `` abcd '' ) which will produce wrong answer . 4 . Click `` Run '' . 5 . The console now shows the input ( `` moo cow bark dog '' ) and the output ( `` abcd '' ) , and the expected output ( `` oom woc krab god '' ) if scrolling down the window . ! [ screen shot 2017-04-03 at 11 49 40 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625396/c1d557a2-1863-11e7-9713-ef834d2e5e24.png ) 6 . Change the returned string from `` abcd '' to another random string ( suppose the user is fixing the bug in the code ) . 7 . Run the code again . 8 . The console shows the expected output , but the actual output is not seen until you scroll up the window . ! [ screen shot 2017-04-03 at 11 51 00 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625434/dbe4f4a4-1863-11e7-88b7-70e1e9ff1c67.png ) Expected behavior : The console
[ UI issue ] Console display starts from middle of the message __EoT__ Repro Steps : 1 . Open `` app.html '' . 2 . Navigate to question 1 . 3 . Change the returned string from `` '' to a random string ( e.g. , `` abcd '' ) which will produce wrong answer . 4 . Click `` Run '' . 5 . The console now shows the input ( `` moo cow bark dog '' ) and the output ( `` abcd '' ) , and the expected output ( `` oom woc krab god '' ) if scrolling down the window . ! [ screen shot 2017-04-03 at 11 49 40 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625396/c1d557a2-1863-11e7-9713-ef834d2e5e24.png ) 6 . Change the returned string from `` abcd '' to another random string ( suppose the user is fixing the bug in the code ) . 7 . Run the code again . 8 . The console shows the expected output , but the actual output is not seen until you scroll up the window . ! [ screen shot 2017-04-03 at 11 51 00 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625434/dbe4f4a4-1863-11e7-88b7-70e1e9ff1c67.png ) Expected behavior : The console
[ UI issue ] Console display starts from middle of the message __EoT__ Repro Steps : 1 . Open `` app.html '' . 2 . Navigate to question 1 . 3 . Change the returned string from `` '' to a random string ( e.g. , `` abcd '' ) which will produce wrong answer . 4 . Click `` Run '' . 5 . The console now shows the input ( `` moo cow bark dog '' ) and the output ( `` abcd '' ) , and the expected output ( `` oom woc krab god '' ) if scrolling down the window . ! [ screen shot 2017-04-03 at 11 49 40 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625396/c1d557a2-1863-11e7-9713-ef834d2e5e24.png ) 6 . Change the returned string from `` abcd '' to another random string ( suppose the user is fixing the bug in the code ) . 7 . Run the code again . 8 . The console shows the expected output , but the actual output is not seen until you scroll up the window . ! [ screen shot 2017-04-03 at 11 51 00 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625434/dbe4f4a4-1863-11e7-88b7-70e1e9ff1c67.png ) Expected behavior : The console
[ UI issue ] Console display starts from middle of the message __EoT__ Repro Steps : 1 . Open `` app.html '' . 2 . Navigate to question 1 . 3 . Change the returned string from `` '' to a random string ( e.g. , `` abcd '' ) which will produce wrong answer . 4 . Click `` Run '' . 5 . The console now shows the input ( `` moo cow bark dog '' ) and the output ( `` abcd '' ) , and the expected output ( `` oom woc krab god '' ) if scrolling down the window . ! [ screen shot 2017-04-03 at 11 49 40 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625396/c1d557a2-1863-11e7-9713-ef834d2e5e24.png ) 6 . Change the returned string from `` abcd '' to another random string ( suppose the user is fixing the bug in the code ) . 7 . Run the code again . 8 . The console shows the expected output , but the actual output is not seen until you scroll up the window . ! [ screen shot 2017-04-03 at 11 51 00 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625434/dbe4f4a4-1863-11e7-88b7-70e1e9ff1c67.png ) Expected behavior : The console
[ UI issue ] Console display starts from middle of the message __EoT__ Repro Steps : 1 . Open `` app.html '' . 2 . Navigate to question 1 . 3 . Change the returned string from `` '' to a random string ( e.g. , `` abcd '' ) which will produce wrong answer . 4 . Click `` Run '' . 5 . The console now shows the input ( `` moo cow bark dog '' ) and the output ( `` abcd '' ) , and the expected output ( `` oom woc krab god '' ) if scrolling down the window . ! [ screen shot 2017-04-03 at 11 49 40 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625396/c1d557a2-1863-11e7-9713-ef834d2e5e24.png ) 6 . Change the returned string from `` abcd '' to another random string ( suppose the user is fixing the bug in the code ) . 7 . Run the code again . 8 . The console shows the expected output , but the actual output is not seen until you scroll up the window . ! [ screen shot 2017-04-03 at 11 51 00 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625434/dbe4f4a4-1863-11e7-88b7-70e1e9ff1c67.png ) Expected behavior : The console
[ UI issue ] Console display starts from middle of the message __EoT__ Repro Steps : 1 . Open `` app.html '' . 2 . Navigate to question 1 . 3 . Change the returned string from `` '' to a random string ( e.g. , `` abcd '' ) which will produce wrong answer . 4 . Click `` Run '' . 5 . The console now shows the input ( `` moo cow bark dog '' ) and the output ( `` abcd '' ) , and the expected output ( `` oom woc krab god '' ) if scrolling down the window . ! [ screen shot 2017-04-03 at 11 49 40 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625396/c1d557a2-1863-11e7-9713-ef834d2e5e24.png ) 6 . Change the returned string from `` abcd '' to another random string ( suppose the user is fixing the bug in the code ) . 7 . Run the code again . 8 . The console shows the expected output , but the actual output is not seen until you scroll up the window . ! [ screen shot 2017-04-03 at 11 51 00 am ] ( https : //cloud.githubusercontent.com/assets/5546251/24625434/dbe4f4a4-1863-11e7-88b7-70e1e9ff1c67.png ) Expected behavior : The console
Remove the extra characters that appear at the end of stdout . __EoT__ After running user code , the stout returned contains the stdout ( and string separators ) for all of the test cases that were executed ( all test cases up until and including those of the current task or all test cases up until an error occurred , whichever comes first ) . While it contains all the necessary information , the returned stdout also has a decent number of random extra characters at the end . This issue occurs on both the server and client version , but it seems that the extra characters are random as they will differ between runs .
Create Find first non-repeating character in a string question __EoT__
Description of question 4 : Run-Length Encoding __EoT__ In the description of question 4 : ! [ image ] ( https : //cloud.githubusercontent.com/assets/7587606/24636743/332a99e4-1891-11e7-957f-e5221352e0b9.png ) '' abcccccd '' should be encoded as `` ab5xcd '' but not `` ab5xc ''
Description of question 4 : Run-Length Encoding __EoT__ In the description of question 4 : ! [ image ] ( https : //cloud.githubusercontent.com/assets/7587606/24636743/332a99e4-1891-11e7-957f-e5221352e0b9.png ) '' abcccccd '' should be encoded as `` ab5xcd '' but not `` ab5xc ''
Description of question 4 : Run-Length Encoding __EoT__ In the description of question 4 : ! [ image ] ( https : //cloud.githubusercontent.com/assets/7587606/24636743/332a99e4-1891-11e7-957f-e5221352e0b9.png ) '' abcccccd '' should be encoded as `` ab5xcd '' but not `` ab5xc ''
Description of question 4 : Run-Length Encoding __EoT__ In the description of question 4 : ! [ image ] ( https : //cloud.githubusercontent.com/assets/7587606/24636743/332a99e4-1891-11e7-957f-e5221352e0b9.png ) '' abcccccd '' should be encoded as `` ab5xcd '' but not `` ab5xc ''
Description of question 4 : Run-Length Encoding __EoT__ In the description of question 4 : ! [ image ] ( https : //cloud.githubusercontent.com/assets/7587606/24636743/332a99e4-1891-11e7-957f-e5221352e0b9.png ) '' abcccccd '' should be encoded as `` ab5xcd '' but not `` ab5xc ''
Description of question 4 : Run-Length Encoding __EoT__ In the description of question 4 : ! [ image ] ( https : //cloud.githubusercontent.com/assets/7587606/24636743/332a99e4-1891-11e7-957f-e5221352e0b9.png ) '' abcccccd '' should be encoded as `` ab5xcd '' but not `` ab5xc ''
Description of question 4 : Run-Length Encoding __EoT__ In the description of question 4 : ! [ image ] ( https : //cloud.githubusercontent.com/assets/7587606/24636743/332a99e4-1891-11e7-957f-e5221352e0b9.png ) '' abcccccd '' should be encoded as `` ab5xcd '' but not `` ab5xc ''
Description of question 4 : Run-Length Encoding __EoT__ In the description of question 4 : ! [ image ] ( https : //cloud.githubusercontent.com/assets/7587606/24636743/332a99e4-1891-11e7-957f-e5221352e0b9.png ) '' abcccccd '' should be encoded as `` ab5xcd '' but not `` ab5xc ''
[ Question Schema ] No Checks on Previous Tasks __EoT__ The Judge does not test my code for tasks that I 've passed before . For example , on Question 2 , my code went through Task One smoothly but had bugs on Task Two . Then I made some changes so as to pass the tests for Task Two . Even through now my code failed on the tests of Task One , the Online Judge would not find out , but instead moved me forward to the third task . Such design seemed quite inappropriate to me , because the next round task , based on the previous one , encourages challengers to generalize their codes , rather than allowing them to pass the tasks individually .
[ Question Schema ] No Checks on Previous Tasks __EoT__ The Judge does not test my code for tasks that I 've passed before . For example , on Question 2 , my code went through Task One smoothly but had bugs on Task Two . Then I made some changes so as to pass the tests for Task Two . Even through now my code failed on the tests of Task One , the Online Judge would not find out , but instead moved me forward to the third task . Such design seemed quite inappropriate to me , because the next round task , based on the previous one , encourages challengers to generalize their codes , rather than allowing them to pass the tasks individually .
[ Question Schema ] No Checks on Previous Tasks __EoT__ The Judge does not test my code for tasks that I 've passed before . For example , on Question 2 , my code went through Task One smoothly but had bugs on Task Two . Then I made some changes so as to pass the tests for Task Two . Even through now my code failed on the tests of Task One , the Online Judge would not find out , but instead moved me forward to the third task . Such design seemed quite inappropriate to me , because the next round task , based on the previous one , encourages challengers to generalize their codes , rather than allowing them to pass the tasks individually .
When a question is completed , change its representation from a number to a check mark . __EoT__ In the stepper at the top of the page , it would be nice to show progress more clearly . One UI suggestion that we received is to change the number in the circle to a green check mark when a question is fully completed ; this seems like a nice thing to do . /cc @ rabidbit
When a question is completed , change its representation from a number to a check mark . __EoT__ In the stepper at the top of the page , it would be nice to show progress more clearly . One UI suggestion that we received is to change the number in the circle to a green check mark when a question is fully completed ; this seems like a nice thing to do . /cc @ rabidbit
When a question is completed , change its representation from a number to a check mark . __EoT__ In the stepper at the top of the page , it would be nice to show progress more clearly . One UI suggestion that we received is to change the number in the circle to a green check mark when a question is fully completed ; this seems like a nice thing to do . /cc @ rabidbit
When a question is completed , change its representation from a number to a check mark . __EoT__ In the stepper at the top of the page , it would be nice to show progress more clearly . One UI suggestion that we received is to change the number in the circle to a green check mark when a question is fully completed ; this seems like a nice thing to do . /cc @ rabidbit
When a question is completed , change its representation from a number to a check mark . __EoT__ In the stepper at the top of the page , it would be nice to show progress more clearly . One UI suggestion that we received is to change the number in the circle to a green check mark when a question is fully completed ; this seems like a nice thing to do . /cc @ rabidbit
When a question is completed , change its representation from a number to a check mark . __EoT__ In the stepper at the top of the page , it would be nice to show progress more clearly . One UI suggestion that we received is to change the number in the circle to a green check mark when a question is fully completed ; this seems like a nice thing to do . /cc @ rabidbit
When a question is completed , change its representation from a number to a check mark . __EoT__ In the stepper at the top of the page , it would be nice to show progress more clearly . One UI suggestion that we received is to change the number in the circle to a green check mark when a question is fully completed ; this seems like a nice thing to do . /cc @ rabidbit
When a question is completed , change its representation from a number to a check mark . __EoT__ In the stepper at the top of the page , it would be nice to show progress more clearly . One UI suggestion that we received is to change the number in the circle to a green check mark when a question is fully completed ; this seems like a nice thing to do . /cc @ rabidbit
When a question is completed , change its representation from a number to a check mark . __EoT__ In the stepper at the top of the page , it would be nice to show progress more clearly . One UI suggestion that we received is to change the number in the circle to a green check mark when a question is fully completed ; this seems like a nice thing to do . /cc @ rabidbit
When a question is completed , change its representation from a number to a check mark . __EoT__ In the stepper at the top of the page , it would be nice to show progress more clearly . One UI suggestion that we received is to change the number in the circle to a green check mark when a question is fully completed ; this seems like a nice thing to do . /cc @ rabidbit
When a question is completed , change its representation from a number to a check mark . __EoT__ In the stepper at the top of the page , it would be nice to show progress more clearly . One UI suggestion that we received is to change the number in the circle to a green check mark when a question is fully completed ; this seems like a nice thing to do . /cc @ rabidbit
When a question is completed , change its representation from a number to a check mark . __EoT__ In the stepper at the top of the page , it would be nice to show progress more clearly . One UI suggestion that we received is to change the number in the circle to a green check mark when a question is fully completed ; this seems like a nice thing to do . /cc @ rabidbit
When a question is completed , change its representation from a number to a check mark . __EoT__ In the stepper at the top of the page , it would be nice to show progress more clearly . One UI suggestion that we received is to change the number in the circle to a green check mark when a question is fully completed ; this seems like a nice thing to do . /cc @ rabidbit
When a question is completed , change its representation from a number to a check mark . __EoT__ In the stepper at the top of the page , it would be nice to show progress more clearly . One UI suggestion that we received is to change the number in the circle to a green check mark when a question is fully completed ; this seems like a nice thing to do . /cc @ rabidbit
When a question is completed , change its representation from a number to a check mark . __EoT__ In the stepper at the top of the page , it would be nice to show progress more clearly . One UI suggestion that we received is to change the number in the circle to a green check mark when a question is fully completed ; this seems like a nice thing to do . /cc @ rabidbit
Make auto-save responsive to use interaction __EoT__ # 209 implements auto-save but it simply runs auto-save every 30seconds and is not based on user interactions . Rather than saving every 30seconds ( or any other static interval ) , auto-save should be activated 3 seconds after an edit event with saves set 8 seconds apart ( that is `` Saving code ... '' displayed for 3 seconds , then 5 second pause , then auto-save again ) . That is , if the user makes a quick change , it auto-saves after 3 seconds . If the user keeps typing non-stop for while , it saves every 8 seconds while the user types until the user finishes typing ( it saves one last time 8 seconds after the user finishes typing the long code ) . If the user does not change anything , auto-save is not activated .
Make auto-save responsive to use interaction __EoT__ # 209 implements auto-save but it simply runs auto-save every 30seconds and is not based on user interactions . Rather than saving every 30seconds ( or any other static interval ) , auto-save should be activated 3 seconds after an edit event with saves set 8 seconds apart ( that is `` Saving code ... '' displayed for 3 seconds , then 5 second pause , then auto-save again ) . That is , if the user makes a quick change , it auto-saves after 3 seconds . If the user keeps typing non-stop for while , it saves every 8 seconds while the user types until the user finishes typing ( it saves one last time 8 seconds after the user finishes typing the long code ) . If the user does not change anything , auto-save is not activated .
[ Sample question issue ] RLE question gives confusing description __EoT__ In the last phase of this question , the user are required to encode the word in a comprehensible manner , in which `` 5aaaa '' can be decoded properly . However , the most straight forward way is to encode `` 5 '' as `` 1x5 '' , which conflict with the previous phase 's requirement ( encoding should be as short as possible ) .
Arrays as input are not currently supported . __EoT__ For are_all_unique , it takes in an array as an argument , but that 's not currently supported , so jsonVariableToPython crashes . Launch app.html Navigate to i18n question Complete tasks up to first instance of filling in are_all_unique App should crash .
Arrays as input are not currently supported . __EoT__ For are_all_unique , it takes in an array as an argument , but that 's not currently supported , so jsonVariableToPython crashes . Launch app.html Navigate to i18n question Complete tasks up to first instance of filling in are_all_unique App should crash .
Code can not be saved in the webpage __EoT__ I finished the question 2 ( compiled successfully and got tests passed ) and moved forward to question 3 . When I went back to question 2 ( click `` 2 '' in the process bar ) to review the code , the code has been reset and my code was not saved .
Code can not be saved in the webpage __EoT__ I finished the question 2 ( compiled successfully and got tests passed ) and moved forward to question 3 . When I went back to question 2 ( click `` 2 '' in the process bar ) to review the code , the code has been reset and my code was not saved .
Code can not be saved in the webpage __EoT__ I finished the question 2 ( compiled successfully and got tests passed ) and moved forward to question 3 . When I went back to question 2 ( click `` 2 '' in the process bar ) to review the code , the code has been reset and my code was not saved .
Code can not be saved in the webpage __EoT__ I finished the question 2 ( compiled successfully and got tests passed ) and moved forward to question 3 . When I went back to question 2 ( click `` 2 '' in the process bar ) to review the code , the code has been reset and my code was not saved .
Code can not be saved in the webpage __EoT__ I finished the question 2 ( compiled successfully and got tests passed ) and moved forward to question 3 . When I went back to question 2 ( click `` 2 '' in the process bar ) to review the code , the code has been reset and my code was not saved .
Code can not be saved in the webpage __EoT__ I finished the question 2 ( compiled successfully and got tests passed ) and moved forward to question 3 . When I went back to question 2 ( click `` 2 '' in the process bar ) to review the code , the code has been reset and my code was not saved .
Code can not be saved in the webpage __EoT__ I finished the question 2 ( compiled successfully and got tests passed ) and moved forward to question 3 . When I went back to question 2 ( click `` 2 '' in the process bar ) to review the code , the code has been reset and my code was not saved .
Code can not be saved in the webpage __EoT__ I finished the question 2 ( compiled successfully and got tests passed ) and moved forward to question 3 . When I went back to question 2 ( click `` 2 '' in the process bar ) to review the code , the code has been reset and my code was not saved .
Code can not be saved in the webpage __EoT__ I finished the question 2 ( compiled successfully and got tests passed ) and moved forward to question 3 . When I went back to question 2 ( click `` 2 '' in the process bar ) to review the code , the code has been reset and my code was not saved .
Code can not be saved in the webpage __EoT__ I finished the question 2 ( compiled successfully and got tests passed ) and moved forward to question 3 . When I went back to question 2 ( click `` 2 '' in the process bar ) to review the code , the code has been reset and my code was not saved .
Error messages are not interpolated . __EoT__ Repro instructions : Go to the reverseWords problem , and type the code seen in the screenshot below . In the screenshot below , the feedback message is not interpolated , so it is meaningless . We should either support interpolation or choose a different feedback message . ! [ screenshot from 2017-03-01 15 41 42 ] ( https : //cloud.githubusercontent.com/assets/10575562/23486620/0c5bdf2c-fe96-11e6-8409-ecfa63f240af.png )
Have the reset button only reset the code window . __EoT__ Have the reset button only reset the code window ( as opposed to both the code window and feedback window ) . User studies show users prefer to reset each independently .
Have the reset button only reset the code window . __EoT__ Have the reset button only reset the code window ( as opposed to both the code window and feedback window ) . User studies show users prefer to reset each independently .
Create Custom Modal Component __EoT__ **Steps for Reproduction : ** * Pull down ` add-privacy-notice ` branch ( if not already merged into ` master ` branch ) . * Open the ` app.html ` file in Chrome * Click on link labeled ` Privacy ` in the bottom right corner . * Notice that a pop-us is displayed with a `` This page says ... '' title at the top . The issue is that we do n't want this title to appear , but since it 's a browser-specific behavior , we need to make our own custom modals that can ideally be further extended to other use cases ( i.e . Clippy pop ups ) .
Create Custom Modal Component __EoT__ **Steps for Reproduction : ** * Pull down ` add-privacy-notice ` branch ( if not already merged into ` master ` branch ) . * Open the ` app.html ` file in Chrome * Click on link labeled ` Privacy ` in the bottom right corner . * Notice that a pop-us is displayed with a `` This page says ... '' title at the top . The issue is that we do n't want this title to appear , but since it 's a browser-specific behavior , we need to make our own custom modals that can ideally be further extended to other use cases ( i.e . Clippy pop ups ) .
Create Custom Modal Component __EoT__ **Steps for Reproduction : ** * Pull down ` add-privacy-notice ` branch ( if not already merged into ` master ` branch ) . * Open the ` app.html ` file in Chrome * Click on link labeled ` Privacy ` in the bottom right corner . * Notice that a pop-us is displayed with a `` This page says ... '' title at the top . The issue is that we do n't want this title to appear , but since it 's a browser-specific behavior , we need to make our own custom modals that can ideally be further extended to other use cases ( i.e . Clippy pop ups ) .
Create Custom Modal Component __EoT__ **Steps for Reproduction : ** * Pull down ` add-privacy-notice ` branch ( if not already merged into ` master ` branch ) . * Open the ` app.html ` file in Chrome * Click on link labeled ` Privacy ` in the bottom right corner . * Notice that a pop-us is displayed with a `` This page says ... '' title at the top . The issue is that we do n't want this title to appear , but since it 's a browser-specific behavior , we need to make our own custom modals that can ideally be further extended to other use cases ( i.e . Clippy pop ups ) .
Create Custom Modal Component __EoT__ **Steps for Reproduction : ** * Pull down ` add-privacy-notice ` branch ( if not already merged into ` master ` branch ) . * Open the ` app.html ` file in Chrome * Click on link labeled ` Privacy ` in the bottom right corner . * Notice that a pop-us is displayed with a `` This page says ... '' title at the top . The issue is that we do n't want this title to appear , but since it 's a browser-specific behavior , we need to make our own custom modals that can ideally be further extended to other use cases ( i.e . Clippy pop ups ) .
Create Custom Modal Component __EoT__ **Steps for Reproduction : ** * Pull down ` add-privacy-notice ` branch ( if not already merged into ` master ` branch ) . * Open the ` app.html ` file in Chrome * Click on link labeled ` Privacy ` in the bottom right corner . * Notice that a pop-us is displayed with a `` This page says ... '' title at the top . The issue is that we do n't want this title to appear , but since it 's a browser-specific behavior , we need to make our own custom modals that can ideally be further extended to other use cases ( i.e . Clippy pop ups ) .
`` Reset Feedback '' Button should be removed and `` Reset Code '' should be updated . __EoT__ Currently , there 's a concern that Reset Feedback is n't terribly useful and Reset Code is more of a `` Start Over '' . We should update the UI to reflect this and have Start Over do the following : - end a session - clear LocalStorage
Give users positive reinforcement about anticipating edge cases and feedback on failing tests __EoT__ When a user 's code passes a broad category of tests ( e.g . short inputs ) we would like to give positive reinforcement for their work . Also , if tests that used to pass stop working , we would like to remind them that their code needs to be fixed . We would like to do this by adding tags to the test inputs that group them into relevant categories ( e.g . `` the general case '' , `` short inputs '' , `` large inputs '' , `` sorted inputs '' , etc. ) . Then , all inputs would need to be evaluated with each run in order to determine the tags that are passing and the tags that are still failing . Design doc is here : https : //goto.google.com/tie-positive Suggested implementation : 1 . Maintain a list of tags whose tests are all passing ( passing_tags_list starts off empty ) and a list of failing tests that are displayed in the output ( displayed_failing_tests_list starts off empty ) 2 . Run all the tests compiling a list of all
Give users positive reinforcement about anticipating edge cases and feedback on failing tests __EoT__ When a user 's code passes a broad category of tests ( e.g . short inputs ) we would like to give positive reinforcement for their work . Also , if tests that used to pass stop working , we would like to remind them that their code needs to be fixed . We would like to do this by adding tags to the test inputs that group them into relevant categories ( e.g . `` the general case '' , `` short inputs '' , `` large inputs '' , `` sorted inputs '' , etc. ) . Then , all inputs would need to be evaluated with each run in order to determine the tags that are passing and the tags that are still failing . Design doc is here : https : //goto.google.com/tie-positive Suggested implementation : 1 . Maintain a list of tags whose tests are all passing ( passing_tags_list starts off empty ) and a list of failing tests that are displayed in the output ( displayed_failing_tests_list starts off empty ) 2 . Run all the tests compiling a list of all
Give users positive reinforcement about anticipating edge cases and feedback on failing tests __EoT__ When a user 's code passes a broad category of tests ( e.g . short inputs ) we would like to give positive reinforcement for their work . Also , if tests that used to pass stop working , we would like to remind them that their code needs to be fixed . We would like to do this by adding tags to the test inputs that group them into relevant categories ( e.g . `` the general case '' , `` short inputs '' , `` large inputs '' , `` sorted inputs '' , etc. ) . Then , all inputs would need to be evaluated with each run in order to determine the tags that are passing and the tags that are still failing . Design doc is here : https : //goto.google.com/tie-positive Suggested implementation : 1 . Maintain a list of tags whose tests are all passing ( passing_tags_list starts off empty ) and a list of failing tests that are displayed in the output ( displayed_failing_tests_list starts off empty ) 2 . Run all the tests compiling a list of all
User study - Include line number in language detection __EoT__ When issue # 413 occurred , it was very hard to know what part of the code was being referenced . Adding a line number to the feedback would help the user better understand where the problem is and also helps to better troubleshoot a potential bug .
User study - Include line number in language detection __EoT__ When issue # 413 occurred , it was very hard to know what part of the code was being referenced . Adding a line number to the feedback would help the user better understand where the problem is and also helps to better troubleshoot a potential bug .
Write remaining Karma unit tests for the frontend . __EoT__ Karma tests still need to be added for : - [ ] client/services/SolutionHandlerService.js - [ ] client/services/code_evaluators/PythonCodeRunnerService.js Both of these require figuring out how to use the Sk library in tests .
Add SessionInvalidEvent to Event Service . __EoT__
User study - Not operator regex buggy __EoT__ Faulty regex for `` Are you making sure to use the right NOT operator ? `` ; specifically , it catches on ` ! `` ` in this code : `` ` def check ( char ) : if char == `` ! `` : `` ` Also , awkward wording in `` Are you making sure to use the right NOT operator ? ''
No auto indentation for Python __EoT__ Hi , The current version of TIE does not do auto-indentation when there is a new line after 'def ' , 'if ' or 'where ' clauses . It just aligns the new line with the previous one . It may be more user-friendly and save some time if this feature is included , since it is not convenient for users to count spaces themselves . I captured two screenshots to compare between TIE and Leetcode . ! [ screen shot 2017-04-03 at 6 31 24 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638265/eda11898-189b-11e7-8721-0c0097d9f1f6.png ) ! [ screen shot 2017-04-03 at 6 32 25 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638264/ed9fa0b2-189b-11e7-9035-1d2ff08feafc.png )
No auto indentation for Python __EoT__ Hi , The current version of TIE does not do auto-indentation when there is a new line after 'def ' , 'if ' or 'where ' clauses . It just aligns the new line with the previous one . It may be more user-friendly and save some time if this feature is included , since it is not convenient for users to count spaces themselves . I captured two screenshots to compare between TIE and Leetcode . ! [ screen shot 2017-04-03 at 6 31 24 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638265/eda11898-189b-11e7-8721-0c0097d9f1f6.png ) ! [ screen shot 2017-04-03 at 6 32 25 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638264/ed9fa0b2-189b-11e7-9035-1d2ff08feafc.png )
No auto indentation for Python __EoT__ Hi , The current version of TIE does not do auto-indentation when there is a new line after 'def ' , 'if ' or 'where ' clauses . It just aligns the new line with the previous one . It may be more user-friendly and save some time if this feature is included , since it is not convenient for users to count spaces themselves . I captured two screenshots to compare between TIE and Leetcode . ! [ screen shot 2017-04-03 at 6 31 24 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638265/eda11898-189b-11e7-8721-0c0097d9f1f6.png ) ! [ screen shot 2017-04-03 at 6 32 25 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638264/ed9fa0b2-189b-11e7-9035-1d2ff08feafc.png )
No auto indentation for Python __EoT__ Hi , The current version of TIE does not do auto-indentation when there is a new line after 'def ' , 'if ' or 'where ' clauses . It just aligns the new line with the previous one . It may be more user-friendly and save some time if this feature is included , since it is not convenient for users to count spaces themselves . I captured two screenshots to compare between TIE and Leetcode . ! [ screen shot 2017-04-03 at 6 31 24 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638265/eda11898-189b-11e7-8721-0c0097d9f1f6.png ) ! [ screen shot 2017-04-03 at 6 32 25 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638264/ed9fa0b2-189b-11e7-9035-1d2ff08feafc.png )
No auto indentation for Python __EoT__ Hi , The current version of TIE does not do auto-indentation when there is a new line after 'def ' , 'if ' or 'where ' clauses . It just aligns the new line with the previous one . It may be more user-friendly and save some time if this feature is included , since it is not convenient for users to count spaces themselves . I captured two screenshots to compare between TIE and Leetcode . ! [ screen shot 2017-04-03 at 6 31 24 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638265/eda11898-189b-11e7-8721-0c0097d9f1f6.png ) ! [ screen shot 2017-04-03 at 6 32 25 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638264/ed9fa0b2-189b-11e7-9035-1d2ff08feafc.png )
No auto indentation for Python __EoT__ Hi , The current version of TIE does not do auto-indentation when there is a new line after 'def ' , 'if ' or 'where ' clauses . It just aligns the new line with the previous one . It may be more user-friendly and save some time if this feature is included , since it is not convenient for users to count spaces themselves . I captured two screenshots to compare between TIE and Leetcode . ! [ screen shot 2017-04-03 at 6 31 24 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638265/eda11898-189b-11e7-8721-0c0097d9f1f6.png ) ! [ screen shot 2017-04-03 at 6 32 25 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638264/ed9fa0b2-189b-11e7-9035-1d2ff08feafc.png )
No auto indentation for Python __EoT__ Hi , The current version of TIE does not do auto-indentation when there is a new line after 'def ' , 'if ' or 'where ' clauses . It just aligns the new line with the previous one . It may be more user-friendly and save some time if this feature is included , since it is not convenient for users to count spaces themselves . I captured two screenshots to compare between TIE and Leetcode . ! [ screen shot 2017-04-03 at 6 31 24 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638265/eda11898-189b-11e7-8721-0c0097d9f1f6.png ) ! [ screen shot 2017-04-03 at 6 32 25 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638264/ed9fa0b2-189b-11e7-9035-1d2ff08feafc.png )
No auto indentation for Python __EoT__ Hi , The current version of TIE does not do auto-indentation when there is a new line after 'def ' , 'if ' or 'where ' clauses . It just aligns the new line with the previous one . It may be more user-friendly and save some time if this feature is included , since it is not convenient for users to count spaces themselves . I captured two screenshots to compare between TIE and Leetcode . ! [ screen shot 2017-04-03 at 6 31 24 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638265/eda11898-189b-11e7-8721-0c0097d9f1f6.png ) ! [ screen shot 2017-04-03 at 6 32 25 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638264/ed9fa0b2-189b-11e7-9035-1d2ff08feafc.png )
No auto indentation for Python __EoT__ Hi , The current version of TIE does not do auto-indentation when there is a new line after 'def ' , 'if ' or 'where ' clauses . It just aligns the new line with the previous one . It may be more user-friendly and save some time if this feature is included , since it is not convenient for users to count spaces themselves . I captured two screenshots to compare between TIE and Leetcode . ! [ screen shot 2017-04-03 at 6 31 24 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638265/eda11898-189b-11e7-8721-0c0097d9f1f6.png ) ! [ screen shot 2017-04-03 at 6 32 25 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638264/ed9fa0b2-189b-11e7-9035-1d2ff08feafc.png )
No auto indentation for Python __EoT__ Hi , The current version of TIE does not do auto-indentation when there is a new line after 'def ' , 'if ' or 'where ' clauses . It just aligns the new line with the previous one . It may be more user-friendly and save some time if this feature is included , since it is not convenient for users to count spaces themselves . I captured two screenshots to compare between TIE and Leetcode . ! [ screen shot 2017-04-03 at 6 31 24 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638265/eda11898-189b-11e7-8721-0c0097d9f1f6.png ) ! [ screen shot 2017-04-03 at 6 32 25 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638264/ed9fa0b2-189b-11e7-9035-1d2ff08feafc.png )
No auto indentation for Python __EoT__ Hi , The current version of TIE does not do auto-indentation when there is a new line after 'def ' , 'if ' or 'where ' clauses . It just aligns the new line with the previous one . It may be more user-friendly and save some time if this feature is included , since it is not convenient for users to count spaces themselves . I captured two screenshots to compare between TIE and Leetcode . ! [ screen shot 2017-04-03 at 6 31 24 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638265/eda11898-189b-11e7-8721-0c0097d9f1f6.png ) ! [ screen shot 2017-04-03 at 6 32 25 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638264/ed9fa0b2-189b-11e7-9035-1d2ff08feafc.png )
No auto indentation for Python __EoT__ Hi , The current version of TIE does not do auto-indentation when there is a new line after 'def ' , 'if ' or 'where ' clauses . It just aligns the new line with the previous one . It may be more user-friendly and save some time if this feature is included , since it is not convenient for users to count spaces themselves . I captured two screenshots to compare between TIE and Leetcode . ! [ screen shot 2017-04-03 at 6 31 24 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638265/eda11898-189b-11e7-8721-0c0097d9f1f6.png ) ! [ screen shot 2017-04-03 at 6 32 25 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638264/ed9fa0b2-189b-11e7-9035-1d2ff08feafc.png )
No auto indentation for Python __EoT__ Hi , The current version of TIE does not do auto-indentation when there is a new line after 'def ' , 'if ' or 'where ' clauses . It just aligns the new line with the previous one . It may be more user-friendly and save some time if this feature is included , since it is not convenient for users to count spaces themselves . I captured two screenshots to compare between TIE and Leetcode . ! [ screen shot 2017-04-03 at 6 31 24 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638265/eda11898-189b-11e7-8721-0c0097d9f1f6.png ) ! [ screen shot 2017-04-03 at 6 32 25 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638264/ed9fa0b2-189b-11e7-9035-1d2ff08feafc.png )
No auto indentation for Python __EoT__ Hi , The current version of TIE does not do auto-indentation when there is a new line after 'def ' , 'if ' or 'where ' clauses . It just aligns the new line with the previous one . It may be more user-friendly and save some time if this feature is included , since it is not convenient for users to count spaces themselves . I captured two screenshots to compare between TIE and Leetcode . ! [ screen shot 2017-04-03 at 6 31 24 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638265/eda11898-189b-11e7-8721-0c0097d9f1f6.png ) ! [ screen shot 2017-04-03 at 6 32 25 pm ] ( https : //cloud.githubusercontent.com/assets/14260930/24638264/ed9fa0b2-189b-11e7-9035-1d2ff08feafc.png )
Element width not consistent and no centering in smaller window size __EoT__ # # # Behaviour On a larger window , everything is centered with the same height on the left and right sides : ! [ screenshot from 2017-04-04 13 40 46 ] ( https : //cloud.githubusercontent.com/assets/1597618/24677644/56604002-193c-11e7-9f54-8d4ef6979284.png ) On smaller windows , the elements do not have a consistent width , nor are they centered on the page . Also , the extra height on these is not required when the elements are stacked vertically . ! [ screenshot from 2017-04-04 13 43 15 ] ( https : //cloud.githubusercontent.com/assets/1597618/24677731/ad61ab98-193c-11e7-92aa-c29bde15e9a1.png ) # # # Repro steps 1 . Open app.html 2 . Decrease browser window size till the question box goes below the code editing box
Scrolling animation is consistent on Linux and Chromebooks __EoT__ On Linux and Chromebooks , the smooth scrolling for the speech balloons is inconsistent ( sometimes the balloons scroll down smoothly , but mostly it does n't ) .
Maintain feedback history when moving on to the next task / question . __EoT__ Maintain feedback history when moving on to the next task / question . Currently , feedback history is cleared when moving to the next question . User studies show users prefer to either maintain the history or give the option to clear the history if the user would like .
Maintain feedback history when moving on to the next task / question . __EoT__ Maintain feedback history when moving on to the next task / question . Currently , feedback history is cleared when moving to the next question . User studies show users prefer to either maintain the history or give the option to clear the history if the user would like .
Allow user to change size of code editing area __EoT__ From feedback received in # 112 , users would like to be able to change the size of the code editing area in a way that is similar to the other boxes on the page .
Question Title UI Modifications for better space utilization and prominence __EoT__ **Description : ** Every question title has a prefix `` Question # : '' in it . This is redundant as the question index is already being displayed in the navigation bar on the top . The highlighted index number displays the current question number . So , we can remove the `` Question # : '' prefix from the question title . Also , the CSS of the question title can be modified to make it more prominent . **Current state : ** < img width= '' 1343 '' alt= '' screen shot 2017-04-10 at 10 37 54 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24874548/e5e3dfee-1dd9-11e7-8fbd-f5ea96e4646d.png '' > **Suggested Modification : ** The following screenshot is just a mock-up of the suggested modifications.We can have a discussion about the best way to approach the same . < img width= '' 1354 '' alt= '' screen shot 2017-04-07 at 10 25 59 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24873848/4918f228-1dd7-11e7-89d5-531589e98759.png '' >
Question Title UI Modifications for better space utilization and prominence __EoT__ **Description : ** Every question title has a prefix `` Question # : '' in it . This is redundant as the question index is already being displayed in the navigation bar on the top . The highlighted index number displays the current question number . So , we can remove the `` Question # : '' prefix from the question title . Also , the CSS of the question title can be modified to make it more prominent . **Current state : ** < img width= '' 1343 '' alt= '' screen shot 2017-04-10 at 10 37 54 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24874548/e5e3dfee-1dd9-11e7-8fbd-f5ea96e4646d.png '' > **Suggested Modification : ** The following screenshot is just a mock-up of the suggested modifications.We can have a discussion about the best way to approach the same . < img width= '' 1354 '' alt= '' screen shot 2017-04-07 at 10 25 59 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24873848/4918f228-1dd7-11e7-89d5-531589e98759.png '' >
Question Title UI Modifications for better space utilization and prominence __EoT__ **Description : ** Every question title has a prefix `` Question # : '' in it . This is redundant as the question index is already being displayed in the navigation bar on the top . The highlighted index number displays the current question number . So , we can remove the `` Question # : '' prefix from the question title . Also , the CSS of the question title can be modified to make it more prominent . **Current state : ** < img width= '' 1343 '' alt= '' screen shot 2017-04-10 at 10 37 54 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24874548/e5e3dfee-1dd9-11e7-8fbd-f5ea96e4646d.png '' > **Suggested Modification : ** The following screenshot is just a mock-up of the suggested modifications.We can have a discussion about the best way to approach the same . < img width= '' 1354 '' alt= '' screen shot 2017-04-07 at 10 25 59 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24873848/4918f228-1dd7-11e7-89d5-531589e98759.png '' >
Question Title UI Modifications for better space utilization and prominence __EoT__ **Description : ** Every question title has a prefix `` Question # : '' in it . This is redundant as the question index is already being displayed in the navigation bar on the top . The highlighted index number displays the current question number . So , we can remove the `` Question # : '' prefix from the question title . Also , the CSS of the question title can be modified to make it more prominent . **Current state : ** < img width= '' 1343 '' alt= '' screen shot 2017-04-10 at 10 37 54 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24874548/e5e3dfee-1dd9-11e7-8fbd-f5ea96e4646d.png '' > **Suggested Modification : ** The following screenshot is just a mock-up of the suggested modifications.We can have a discussion about the best way to approach the same . < img width= '' 1354 '' alt= '' screen shot 2017-04-07 at 10 25 59 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24873848/4918f228-1dd7-11e7-89d5-531589e98759.png '' >
Question Title UI Modifications for better space utilization and prominence __EoT__ **Description : ** Every question title has a prefix `` Question # : '' in it . This is redundant as the question index is already being displayed in the navigation bar on the top . The highlighted index number displays the current question number . So , we can remove the `` Question # : '' prefix from the question title . Also , the CSS of the question title can be modified to make it more prominent . **Current state : ** < img width= '' 1343 '' alt= '' screen shot 2017-04-10 at 10 37 54 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24874548/e5e3dfee-1dd9-11e7-8fbd-f5ea96e4646d.png '' > **Suggested Modification : ** The following screenshot is just a mock-up of the suggested modifications.We can have a discussion about the best way to approach the same . < img width= '' 1354 '' alt= '' screen shot 2017-04-07 at 10 25 59 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24873848/4918f228-1dd7-11e7-89d5-531589e98759.png '' >
Question Title UI Modifications for better space utilization and prominence __EoT__ **Description : ** Every question title has a prefix `` Question # : '' in it . This is redundant as the question index is already being displayed in the navigation bar on the top . The highlighted index number displays the current question number . So , we can remove the `` Question # : '' prefix from the question title . Also , the CSS of the question title can be modified to make it more prominent . **Current state : ** < img width= '' 1343 '' alt= '' screen shot 2017-04-10 at 10 37 54 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24874548/e5e3dfee-1dd9-11e7-8fbd-f5ea96e4646d.png '' > **Suggested Modification : ** The following screenshot is just a mock-up of the suggested modifications.We can have a discussion about the best way to approach the same . < img width= '' 1354 '' alt= '' screen shot 2017-04-07 at 10 25 59 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24873848/4918f228-1dd7-11e7-89d5-531589e98759.png '' >
Question Title UI Modifications for better space utilization and prominence __EoT__ **Description : ** Every question title has a prefix `` Question # : '' in it . This is redundant as the question index is already being displayed in the navigation bar on the top . The highlighted index number displays the current question number . So , we can remove the `` Question # : '' prefix from the question title . Also , the CSS of the question title can be modified to make it more prominent . **Current state : ** < img width= '' 1343 '' alt= '' screen shot 2017-04-10 at 10 37 54 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24874548/e5e3dfee-1dd9-11e7-8fbd-f5ea96e4646d.png '' > **Suggested Modification : ** The following screenshot is just a mock-up of the suggested modifications.We can have a discussion about the best way to approach the same . < img width= '' 1354 '' alt= '' screen shot 2017-04-07 at 10 25 59 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24873848/4918f228-1dd7-11e7-89d5-531589e98759.png '' >
Question Title UI Modifications for better space utilization and prominence __EoT__ **Description : ** Every question title has a prefix `` Question # : '' in it . This is redundant as the question index is already being displayed in the navigation bar on the top . The highlighted index number displays the current question number . So , we can remove the `` Question # : '' prefix from the question title . Also , the CSS of the question title can be modified to make it more prominent . **Current state : ** < img width= '' 1343 '' alt= '' screen shot 2017-04-10 at 10 37 54 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24874548/e5e3dfee-1dd9-11e7-8fbd-f5ea96e4646d.png '' > **Suggested Modification : ** The following screenshot is just a mock-up of the suggested modifications.We can have a discussion about the best way to approach the same . < img width= '' 1354 '' alt= '' screen shot 2017-04-07 at 10 25 59 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24873848/4918f228-1dd7-11e7-89d5-531589e98759.png '' >
Question Title UI Modifications for better space utilization and prominence __EoT__ **Description : ** Every question title has a prefix `` Question # : '' in it . This is redundant as the question index is already being displayed in the navigation bar on the top . The highlighted index number displays the current question number . So , we can remove the `` Question # : '' prefix from the question title . Also , the CSS of the question title can be modified to make it more prominent . **Current state : ** < img width= '' 1343 '' alt= '' screen shot 2017-04-10 at 10 37 54 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24874548/e5e3dfee-1dd9-11e7-8fbd-f5ea96e4646d.png '' > **Suggested Modification : ** The following screenshot is just a mock-up of the suggested modifications.We can have a discussion about the best way to approach the same . < img width= '' 1354 '' alt= '' screen shot 2017-04-07 at 10 25 59 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24873848/4918f228-1dd7-11e7-89d5-531589e98759.png '' >
Question Title UI Modifications for better space utilization and prominence __EoT__ **Description : ** Every question title has a prefix `` Question # : '' in it . This is redundant as the question index is already being displayed in the navigation bar on the top . The highlighted index number displays the current question number . So , we can remove the `` Question # : '' prefix from the question title . Also , the CSS of the question title can be modified to make it more prominent . **Current state : ** < img width= '' 1343 '' alt= '' screen shot 2017-04-10 at 10 37 54 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24874548/e5e3dfee-1dd9-11e7-8fbd-f5ea96e4646d.png '' > **Suggested Modification : ** The following screenshot is just a mock-up of the suggested modifications.We can have a discussion about the best way to approach the same . < img width= '' 1354 '' alt= '' screen shot 2017-04-07 at 10 25 59 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24873848/4918f228-1dd7-11e7-89d5-531589e98759.png '' >
Question Title UI Modifications for better space utilization and prominence __EoT__ **Description : ** Every question title has a prefix `` Question # : '' in it . This is redundant as the question index is already being displayed in the navigation bar on the top . The highlighted index number displays the current question number . So , we can remove the `` Question # : '' prefix from the question title . Also , the CSS of the question title can be modified to make it more prominent . **Current state : ** < img width= '' 1343 '' alt= '' screen shot 2017-04-10 at 10 37 54 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24874548/e5e3dfee-1dd9-11e7-8fbd-f5ea96e4646d.png '' > **Suggested Modification : ** The following screenshot is just a mock-up of the suggested modifications.We can have a discussion about the best way to approach the same . < img width= '' 1354 '' alt= '' screen shot 2017-04-07 at 10 25 59 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24873848/4918f228-1dd7-11e7-89d5-531589e98759.png '' >
Question Title UI Modifications for better space utilization and prominence __EoT__ **Description : ** Every question title has a prefix `` Question # : '' in it . This is redundant as the question index is already being displayed in the navigation bar on the top . The highlighted index number displays the current question number . So , we can remove the `` Question # : '' prefix from the question title . Also , the CSS of the question title can be modified to make it more prominent . **Current state : ** < img width= '' 1343 '' alt= '' screen shot 2017-04-10 at 10 37 54 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24874548/e5e3dfee-1dd9-11e7-8fbd-f5ea96e4646d.png '' > **Suggested Modification : ** The following screenshot is just a mock-up of the suggested modifications.We can have a discussion about the best way to approach the same . < img width= '' 1354 '' alt= '' screen shot 2017-04-07 at 10 25 59 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24873848/4918f228-1dd7-11e7-89d5-531589e98759.png '' >
Question Title UI Modifications for better space utilization and prominence __EoT__ **Description : ** Every question title has a prefix `` Question # : '' in it . This is redundant as the question index is already being displayed in the navigation bar on the top . The highlighted index number displays the current question number . So , we can remove the `` Question # : '' prefix from the question title . Also , the CSS of the question title can be modified to make it more prominent . **Current state : ** < img width= '' 1343 '' alt= '' screen shot 2017-04-10 at 10 37 54 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24874548/e5e3dfee-1dd9-11e7-8fbd-f5ea96e4646d.png '' > **Suggested Modification : ** The following screenshot is just a mock-up of the suggested modifications.We can have a discussion about the best way to approach the same . < img width= '' 1354 '' alt= '' screen shot 2017-04-07 at 10 25 59 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24873848/4918f228-1dd7-11e7-89d5-531589e98759.png '' >
Question Title UI Modifications for better space utilization and prominence __EoT__ **Description : ** Every question title has a prefix `` Question # : '' in it . This is redundant as the question index is already being displayed in the navigation bar on the top . The highlighted index number displays the current question number . So , we can remove the `` Question # : '' prefix from the question title . Also , the CSS of the question title can be modified to make it more prominent . **Current state : ** < img width= '' 1343 '' alt= '' screen shot 2017-04-10 at 10 37 54 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24874548/e5e3dfee-1dd9-11e7-8fbd-f5ea96e4646d.png '' > **Suggested Modification : ** The following screenshot is just a mock-up of the suggested modifications.We can have a discussion about the best way to approach the same . < img width= '' 1354 '' alt= '' screen shot 2017-04-07 at 10 25 59 am '' src= '' https : //cloud.githubusercontent.com/assets/15383817/24873848/4918f228-1dd7-11e7-89d5-531589e98759.png '' >
Change `` Try addressing the error before you run again . '' to feedback rather than error __EoT__ Repro steps : Start question . Change `` return `` '' to `` return s '' Click `` I think I 'm done '' Click `` I think I 'm done '' Given that you 're getting KNOWN_BUG feedback , it seems more apt to suggest `` Try addressing the feedback before you run again '' rather than `` Try addressing the error '' . It 's an error , sure , but it 's not phrased as such in the conversational UI .
Give graduated feedback for syntax errors . __EoT__ In an interview setting , students are not going to get feedback that a syntax error has happened `` on line X , column Y '' . To be fair , such syntax errors probably do n't matter too much anyway . However , in TIE , the code has to be syntactically correct so that it can run and so that feedback can be given . A possible middle ground might therefore be to do the following : when a syntax error is found , highlight the line that contains it in red , and say something along the lines of `` there 's a syntax error here ; please find it and fix it '' , while offering an optional popup hint that shows the exact error and line number . That way , if a student wants to practice finding syntax errors on inspection , they have the opportunity to do so ; if they do n't want to practice this , they can just open the popup .
Harden the question verification tests . __EoT__ In [ QuestionSchemaValidationService ] ( https : //github.com/google/tie/blob/0580cd73546a9ea77113839b0b84bff5fd1e473f/assets/tests/QuestionSchemaValidationService.js # L21 ) , there are a number of TODOs that involve additional verification ( though I think the first of this has been involved in # 142 -- @ eyurko , is this right ? ) The last of these TODOs ( verify that the starter code matches a language pattern for the language specified ) is probably not worth doing , but I think we should : - [ x ] Verify that AuxiliaryCode contains the AuxiliaryCode class definition . - [ x ] Verify that starter code contains the necessary Python function definitions . since these should be reasonably straightforward tests to write .
Add functionality for e2e tests to ensure that the application does not break . __EoT__ We should probably add an end-to-end test that loads the page and clicks some of the buttons / submits some code , in order to prevent breakages like the one that was fixed in # 80 . ( Note that even an end-to-end test which just loads the app.html page in a browser and checks that there are no console errors would have helped here . ) Protractor is likely to be a good library for this , and is worth investigating .
Add functionality for e2e tests to ensure that the application does not break . __EoT__ We should probably add an end-to-end test that loads the page and clicks some of the buttons / submits some code , in order to prevent breakages like the one that was fixed in # 80 . ( Note that even an end-to-end test which just loads the app.html page in a browser and checks that there are no console errors would have helped here . ) Protractor is likely to be a good library for this , and is worth investigating .
Add functionality for e2e tests to ensure that the application does not break . __EoT__ We should probably add an end-to-end test that loads the page and clicks some of the buttons / submits some code , in order to prevent breakages like the one that was fixed in # 80 . ( Note that even an end-to-end test which just loads the app.html page in a browser and checks that there are no console errors would have helped here . ) Protractor is likely to be a good library for this , and is worth investigating .
Add functionality for e2e tests to ensure that the application does not break . __EoT__ We should probably add an end-to-end test that loads the page and clicks some of the buttons / submits some code , in order to prevent breakages like the one that was fixed in # 80 . ( Note that even an end-to-end test which just loads the app.html page in a browser and checks that there are no console errors would have helped here . ) Protractor is likely to be a good library for this , and is worth investigating .
Add functionality for e2e tests to ensure that the application does not break . __EoT__ We should probably add an end-to-end test that loads the page and clicks some of the buttons / submits some code , in order to prevent breakages like the one that was fixed in # 80 . ( Note that even an end-to-end test which just loads the app.html page in a browser and checks that there are no console errors would have helped here . ) Protractor is likely to be a good library for this , and is worth investigating .
Add functionality for e2e tests to ensure that the application does not break . __EoT__ We should probably add an end-to-end test that loads the page and clicks some of the buttons / submits some code , in order to prevent breakages like the one that was fixed in # 80 . ( Note that even an end-to-end test which just loads the app.html page in a browser and checks that there are no console errors would have helped here . ) Protractor is likely to be a good library for this , and is worth investigating .
Add functionality for e2e tests to ensure that the application does not break . __EoT__ We should probably add an end-to-end test that loads the page and clicks some of the buttons / submits some code , in order to prevent breakages like the one that was fixed in # 80 . ( Note that even an end-to-end test which just loads the app.html page in a browser and checks that there are no console errors would have helped here . ) Protractor is likely to be a good library for this , and is worth investigating .
Add functionality for e2e tests to ensure that the application does not break . __EoT__ We should probably add an end-to-end test that loads the page and clicks some of the buttons / submits some code , in order to prevent breakages like the one that was fixed in # 80 . ( Note that even an end-to-end test which just loads the app.html page in a browser and checks that there are no console errors would have helped here . ) Protractor is likely to be a good library for this , and is worth investigating .
Add functionality for e2e tests to ensure that the application does not break . __EoT__ We should probably add an end-to-end test that loads the page and clicks some of the buttons / submits some code , in order to prevent breakages like the one that was fixed in # 80 . ( Note that even an end-to-end test which just loads the app.html page in a browser and checks that there are no console errors would have helped here . ) Protractor is likely to be a good library for this , and is worth investigating .
Add functionality for e2e tests to ensure that the application does not break . __EoT__ We should probably add an end-to-end test that loads the page and clicks some of the buttons / submits some code , in order to prevent breakages like the one that was fixed in # 80 . ( Note that even an end-to-end test which just loads the app.html page in a browser and checks that there are no console errors would have helped here . ) Protractor is likely to be a good library for this , and is worth investigating .
Add functionality for e2e tests to ensure that the application does not break . __EoT__ We should probably add an end-to-end test that loads the page and clicks some of the buttons / submits some code , in order to prevent breakages like the one that was fixed in # 80 . ( Note that even an end-to-end test which just loads the app.html page in a browser and checks that there are no console errors would have helped here . ) Protractor is likely to be a good library for this , and is worth investigating .
Add functionality for e2e tests to ensure that the application does not break . __EoT__ We should probably add an end-to-end test that loads the page and clicks some of the buttons / submits some code , in order to prevent breakages like the one that was fixed in # 80 . ( Note that even an end-to-end test which just loads the app.html page in a browser and checks that there are no console errors would have helped here . ) Protractor is likely to be a good library for this , and is worth investigating .
Add functionality for e2e tests to ensure that the application does not break . __EoT__ We should probably add an end-to-end test that loads the page and clicks some of the buttons / submits some code , in order to prevent breakages like the one that was fixed in # 80 . ( Note that even an end-to-end test which just loads the app.html page in a browser and checks that there are no console errors would have helped here . ) Protractor is likely to be a good library for this , and is worth investigating .
Add functionality for e2e tests to ensure that the application does not break . __EoT__ We should probably add an end-to-end test that loads the page and clicks some of the buttons / submits some code , in order to prevent breakages like the one that was fixed in # 80 . ( Note that even an end-to-end test which just loads the app.html page in a browser and checks that there are no console errors would have helped here . ) Protractor is likely to be a good library for this , and is worth investigating .
Add functionality for e2e tests to ensure that the application does not break . __EoT__ We should probably add an end-to-end test that loads the page and clicks some of the buttons / submits some code , in order to prevent breakages like the one that was fixed in # 80 . ( Note that even an end-to-end test which just loads the app.html page in a browser and checks that there are no console errors would have helped here . ) Protractor is likely to be a good library for this , and is worth investigating .
Add functionality for e2e tests to ensure that the application does not break . __EoT__ We should probably add an end-to-end test that loads the page and clicks some of the buttons / submits some code , in order to prevent breakages like the one that was fixed in # 80 . ( Note that even an end-to-end test which just loads the app.html page in a browser and checks that there are no console errors would have helped here . ) Protractor is likely to be a good library for this , and is worth investigating .
No reset button to reset the current code __EoT__ @ sniffsky and I are currently working on # 98 , however , we realize that if we are able to save codes , then it 's probably good to have a reset button once # 98 is resolved .
No reset button to reset the current code __EoT__ @ sniffsky and I are currently working on # 98 , however , we realize that if we are able to save codes , then it 's probably good to have a reset button once # 98 is resolved .
No reset button to reset the current code __EoT__ @ sniffsky and I are currently working on # 98 , however , we realize that if we are able to save codes , then it 's probably good to have a reset button once # 98 is resolved .
No reset button to reset the current code __EoT__ @ sniffsky and I are currently working on # 98 , however , we realize that if we are able to save codes , then it 's probably good to have a reset button once # 98 is resolved .
No reset button to reset the current code __EoT__ @ sniffsky and I are currently working on # 98 , however , we realize that if we are able to save codes , then it 's probably good to have a reset button once # 98 is resolved .
No reset button to reset the current code __EoT__ @ sniffsky and I are currently working on # 98 , however , we realize that if we are able to save codes , then it 's probably good to have a reset button once # 98 is resolved .
No reset button to reset the current code __EoT__ @ sniffsky and I are currently working on # 98 , however , we realize that if we are able to save codes , then it 's probably good to have a reset button once # 98 is resolved .
No reset button to reset the current code __EoT__ @ sniffsky and I are currently working on # 98 , however , we realize that if we are able to save codes , then it 's probably good to have a reset button once # 98 is resolved .
No reset button to reset the current code __EoT__ @ sniffsky and I are currently working on # 98 , however , we realize that if we are able to save codes , then it 's probably good to have a reset button once # 98 is resolved .
No reset button to reset the current code __EoT__ @ sniffsky and I are currently working on # 98 , however , we realize that if we are able to save codes , then it 's probably good to have a reset button once # 98 is resolved .
No reset button to reset the current code __EoT__ @ sniffsky and I are currently working on # 98 , however , we realize that if we are able to save codes , then it 's probably good to have a reset button once # 98 is resolved .
No reset button to reset the current code __EoT__ @ sniffsky and I are currently working on # 98 , however , we realize that if we are able to save codes , then it 's probably good to have a reset button once # 98 is resolved .
No reset button to reset the current code __EoT__ @ sniffsky and I are currently working on # 98 , however , we realize that if we are able to save codes , then it 's probably good to have a reset button once # 98 is resolved .
Add a pre-push git hook to catch silly mistakes prior to submit . __EoT__ It would be nice to have a script that runs the lint checks and karma tests just prior to a push -- see e.g . the `` pre-push '' hook in [ this link ] ( https : //git-scm.com/book/gr/v2/Customizing-Git-Git-Hooks ) . This script would then be run automatically after `` git push '' is done on the command line . We probably do not need to include e2e tests in this script , since they generally take a while to run and the developer is expected to do some manual testing anyway .
Add a pre-push git hook to catch silly mistakes prior to submit . __EoT__ It would be nice to have a script that runs the lint checks and karma tests just prior to a push -- see e.g . the `` pre-push '' hook in [ this link ] ( https : //git-scm.com/book/gr/v2/Customizing-Git-Git-Hooks ) . This script would then be run automatically after `` git push '' is done on the command line . We probably do not need to include e2e tests in this script , since they generally take a while to run and the developer is expected to do some manual testing anyway .
Add a pre-push git hook to catch silly mistakes prior to submit . __EoT__ It would be nice to have a script that runs the lint checks and karma tests just prior to a push -- see e.g . the `` pre-push '' hook in [ this link ] ( https : //git-scm.com/book/gr/v2/Customizing-Git-Git-Hooks ) . This script would then be run automatically after `` git push '' is done on the command line . We probably do not need to include e2e tests in this script , since they generally take a while to run and the developer is expected to do some manual testing anyway .
Add a pre-push git hook to catch silly mistakes prior to submit . __EoT__ It would be nice to have a script that runs the lint checks and karma tests just prior to a push -- see e.g . the `` pre-push '' hook in [ this link ] ( https : //git-scm.com/book/gr/v2/Customizing-Git-Git-Hooks ) . This script would then be run automatically after `` git push '' is done on the command line . We probably do not need to include e2e tests in this script , since they generally take a while to run and the developer is expected to do some manual testing anyway .
Add a pre-push git hook to catch silly mistakes prior to submit . __EoT__ It would be nice to have a script that runs the lint checks and karma tests just prior to a push -- see e.g . the `` pre-push '' hook in [ this link ] ( https : //git-scm.com/book/gr/v2/Customizing-Git-Git-Hooks ) . This script would then be run automatically after `` git push '' is done on the command line . We probably do not need to include e2e tests in this script , since they generally take a while to run and the developer is expected to do some manual testing anyway .
Add a pre-push git hook to catch silly mistakes prior to submit . __EoT__ It would be nice to have a script that runs the lint checks and karma tests just prior to a push -- see e.g . the `` pre-push '' hook in [ this link ] ( https : //git-scm.com/book/gr/v2/Customizing-Git-Git-Hooks ) . This script would then be run automatically after `` git push '' is done on the command line . We probably do not need to include e2e tests in this script , since they generally take a while to run and the developer is expected to do some manual testing anyway .
Add a pre-push git hook to catch silly mistakes prior to submit . __EoT__ It would be nice to have a script that runs the lint checks and karma tests just prior to a push -- see e.g . the `` pre-push '' hook in [ this link ] ( https : //git-scm.com/book/gr/v2/Customizing-Git-Git-Hooks ) . This script would then be run automatically after `` git push '' is done on the command line . We probably do not need to include e2e tests in this script , since they generally take a while to run and the developer is expected to do some manual testing anyway .
Add a pre-push git hook to catch silly mistakes prior to submit . __EoT__ It would be nice to have a script that runs the lint checks and karma tests just prior to a push -- see e.g . the `` pre-push '' hook in [ this link ] ( https : //git-scm.com/book/gr/v2/Customizing-Git-Git-Hooks ) . This script would then be run automatically after `` git push '' is done on the command line . We probably do not need to include e2e tests in this script , since they generally take a while to run and the developer is expected to do some manual testing anyway .
Add a pre-push git hook to catch silly mistakes prior to submit . __EoT__ It would be nice to have a script that runs the lint checks and karma tests just prior to a push -- see e.g . the `` pre-push '' hook in [ this link ] ( https : //git-scm.com/book/gr/v2/Customizing-Git-Git-Hooks ) . This script would then be run automatically after `` git push '' is done on the command line . We probably do not need to include e2e tests in this script , since they generally take a while to run and the developer is expected to do some manual testing anyway .
Add a pre-push git hook to catch silly mistakes prior to submit . __EoT__ It would be nice to have a script that runs the lint checks and karma tests just prior to a push -- see e.g . the `` pre-push '' hook in [ this link ] ( https : //git-scm.com/book/gr/v2/Customizing-Git-Git-Hooks ) . This script would then be run automatically after `` git push '' is done on the command line . We probably do not need to include e2e tests in this script , since they generally take a while to run and the developer is expected to do some manual testing anyway .
Add a pre-push git hook to catch silly mistakes prior to submit . __EoT__ It would be nice to have a script that runs the lint checks and karma tests just prior to a push -- see e.g . the `` pre-push '' hook in [ this link ] ( https : //git-scm.com/book/gr/v2/Customizing-Git-Git-Hooks ) . This script would then be run automatically after `` git push '' is done on the command line . We probably do not need to include e2e tests in this script , since they generally take a while to run and the developer is expected to do some manual testing anyway .
Add a pre-push git hook to catch silly mistakes prior to submit . __EoT__ It would be nice to have a script that runs the lint checks and karma tests just prior to a push -- see e.g . the `` pre-push '' hook in [ this link ] ( https : //git-scm.com/book/gr/v2/Customizing-Git-Git-Hooks ) . This script would then be run automatically after `` git push '' is done on the command line . We probably do not need to include e2e tests in this script , since they generally take a while to run and the developer is expected to do some manual testing anyway .
Add a pre-push git hook to catch silly mistakes prior to submit . __EoT__ It would be nice to have a script that runs the lint checks and karma tests just prior to a push -- see e.g . the `` pre-push '' hook in [ this link ] ( https : //git-scm.com/book/gr/v2/Customizing-Git-Git-Hooks ) . This script would then be run automatically after `` git push '' is done on the command line . We probably do not need to include e2e tests in this script , since they generally take a while to run and the developer is expected to do some manual testing anyway .
Add a pre-push git hook to catch silly mistakes prior to submit . __EoT__ It would be nice to have a script that runs the lint checks and karma tests just prior to a push -- see e.g . the `` pre-push '' hook in [ this link ] ( https : //git-scm.com/book/gr/v2/Customizing-Git-Git-Hooks ) . This script would then be run automatically after `` git push '' is done on the command line . We probably do not need to include e2e tests in this script , since they generally take a while to run and the developer is expected to do some manual testing anyway .
Add a pre-push git hook to catch silly mistakes prior to submit . __EoT__ It would be nice to have a script that runs the lint checks and karma tests just prior to a push -- see e.g . the `` pre-push '' hook in [ this link ] ( https : //git-scm.com/book/gr/v2/Customizing-Git-Git-Hooks ) . This script would then be run automatically after `` git push '' is done on the command line . We probably do not need to include e2e tests in this script , since they generally take a while to run and the developer is expected to do some manual testing anyway .
Add a pre-push git hook to catch silly mistakes prior to submit . __EoT__ It would be nice to have a script that runs the lint checks and karma tests just prior to a push -- see e.g . the `` pre-push '' hook in [ this link ] ( https : //git-scm.com/book/gr/v2/Customizing-Git-Git-Hooks ) . This script would then be run automatically after `` git push '' is done on the command line . We probably do not need to include e2e tests in this script , since they generally take a while to run and the developer is expected to do some manual testing anyway .
Add a pre-push git hook to catch silly mistakes prior to submit . __EoT__ It would be nice to have a script that runs the lint checks and karma tests just prior to a push -- see e.g . the `` pre-push '' hook in [ this link ] ( https : //git-scm.com/book/gr/v2/Customizing-Git-Git-Hooks ) . This script would then be run automatically after `` git push '' is done on the command line . We probably do not need to include e2e tests in this script , since they generally take a while to run and the developer is expected to do some manual testing anyway .
The prompt says `` click the 'Next ' button to move on '' but the button says `` Run '' . __EoT__ Repro steps from scrach : Start any question Complete a task within that question Prompt will have inconsistent message with button -- button should change , I think .
Indentation is often inconsistent and incorrect . __EoT__ For some reason the editor is conflating tabs and spaces and it 's causing all sorts of errors . I usually run into this on the i18n question when adding the length check to abbreviate . Open app.html Copy/paste solution . Add lines to solution ( hitting `` Enter '' after a complete line ) , and then tab inwards . You see the correct indentation , but it 's now mixed tabs with spaces .
Prevent UI components rearranging when window is resized . __EoT__ When the page is zoomed or when the browser window is resized , the instructions pane drops to the bottom of the page . This is confusing and also probably not that useful -- instead , it should just horizontally overflow to the right of the page . The aim of this issue is therefore to remove this behavior . In other words , always show two columns on the page , and overflow off the right if needed . /cf # 81 /cc @ rabidbit @ shaman-rajan
Prevent UI components rearranging when window is resized . __EoT__ When the page is zoomed or when the browser window is resized , the instructions pane drops to the bottom of the page . This is confusing and also probably not that useful -- instead , it should just horizontally overflow to the right of the page . The aim of this issue is therefore to remove this behavior . In other words , always show two columns on the page , and overflow off the right if needed . /cf # 81 /cc @ rabidbit @ shaman-rajan
Prevent UI components rearranging when window is resized . __EoT__ When the page is zoomed or when the browser window is resized , the instructions pane drops to the bottom of the page . This is confusing and also probably not that useful -- instead , it should just horizontally overflow to the right of the page . The aim of this issue is therefore to remove this behavior . In other words , always show two columns on the page , and overflow off the right if needed . /cf # 81 /cc @ rabidbit @ shaman-rajan
Prevent UI components rearranging when window is resized . __EoT__ When the page is zoomed or when the browser window is resized , the instructions pane drops to the bottom of the page . This is confusing and also probably not that useful -- instead , it should just horizontally overflow to the right of the page . The aim of this issue is therefore to remove this behavior . In other words , always show two columns on the page , and overflow off the right if needed . /cf # 81 /cc @ rabidbit @ shaman-rajan
Prevent UI components rearranging when window is resized . __EoT__ When the page is zoomed or when the browser window is resized , the instructions pane drops to the bottom of the page . This is confusing and also probably not that useful -- instead , it should just horizontally overflow to the right of the page . The aim of this issue is therefore to remove this behavior . In other words , always show two columns on the page , and overflow off the right if needed . /cf # 81 /cc @ rabidbit @ shaman-rajan
Prevent UI components rearranging when window is resized . __EoT__ When the page is zoomed or when the browser window is resized , the instructions pane drops to the bottom of the page . This is confusing and also probably not that useful -- instead , it should just horizontally overflow to the right of the page . The aim of this issue is therefore to remove this behavior . In other words , always show two columns on the page , and overflow off the right if needed . /cf # 81 /cc @ rabidbit @ shaman-rajan
Prevent UI components rearranging when window is resized . __EoT__ When the page is zoomed or when the browser window is resized , the instructions pane drops to the bottom of the page . This is confusing and also probably not that useful -- instead , it should just horizontally overflow to the right of the page . The aim of this issue is therefore to remove this behavior . In other words , always show two columns on the page , and overflow off the right if needed . /cf # 81 /cc @ rabidbit @ shaman-rajan
Prevent UI components rearranging when window is resized . __EoT__ When the page is zoomed or when the browser window is resized , the instructions pane drops to the bottom of the page . This is confusing and also probably not that useful -- instead , it should just horizontally overflow to the right of the page . The aim of this issue is therefore to remove this behavior . In other words , always show two columns on the page , and overflow off the right if needed . /cf # 81 /cc @ rabidbit @ shaman-rajan
Prevent UI components rearranging when window is resized . __EoT__ When the page is zoomed or when the browser window is resized , the instructions pane drops to the bottom of the page . This is confusing and also probably not that useful -- instead , it should just horizontally overflow to the right of the page . The aim of this issue is therefore to remove this behavior . In other words , always show two columns on the page , and overflow off the right if needed . /cf # 81 /cc @ rabidbit @ shaman-rajan
Prevent UI components rearranging when window is resized . __EoT__ When the page is zoomed or when the browser window is resized , the instructions pane drops to the bottom of the page . This is confusing and also probably not that useful -- instead , it should just horizontally overflow to the right of the page . The aim of this issue is therefore to remove this behavior . In other words , always show two columns on the page , and overflow off the right if needed . /cf # 81 /cc @ rabidbit @ shaman-rajan
Prevent UI components rearranging when window is resized . __EoT__ When the page is zoomed or when the browser window is resized , the instructions pane drops to the bottom of the page . This is confusing and also probably not that useful -- instead , it should just horizontally overflow to the right of the page . The aim of this issue is therefore to remove this behavior . In other words , always show two columns on the page , and overflow off the right if needed . /cf # 81 /cc @ rabidbit @ shaman-rajan
Prevent UI components rearranging when window is resized . __EoT__ When the page is zoomed or when the browser window is resized , the instructions pane drops to the bottom of the page . This is confusing and also probably not that useful -- instead , it should just horizontally overflow to the right of the page . The aim of this issue is therefore to remove this behavior . In other words , always show two columns on the page , and overflow off the right if needed . /cf # 81 /cc @ rabidbit @ shaman-rajan
Prevent UI components rearranging when window is resized . __EoT__ When the page is zoomed or when the browser window is resized , the instructions pane drops to the bottom of the page . This is confusing and also probably not that useful -- instead , it should just horizontally overflow to the right of the page . The aim of this issue is therefore to remove this behavior . In other words , always show two columns on the page , and overflow off the right if needed . /cf # 81 /cc @ rabidbit @ shaman-rajan
Create `` Find closest value in BST '' question __EoT__
Confusing text in instructions for findAlphabet question __EoT__ https : //github.com/google/tie/blob/master/assets/questions/findAlphabet.js # L232 The word 'frequency ' on this line and 'return them in alphabetical order ' are both confusing . The first one is not related to the question , and the second might confuse the user as to which alphabetical ordering we are asking for . It might make more sense to change the sentence to something like `` In case there is not enough information to order multiple characters , return them in the conventional alphabetical order '' .
Confusing text in instructions for findAlphabet question __EoT__ https : //github.com/google/tie/blob/master/assets/questions/findAlphabet.js # L232 The word 'frequency ' on this line and 'return them in alphabetical order ' are both confusing . The first one is not related to the question , and the second might confuse the user as to which alphabetical ordering we are asking for . It might make more sense to change the sentence to something like `` In case there is not enough information to order multiple characters , return them in the conventional alphabetical order '' .
Create question to find if a given string is a palindrome __EoT__
Create `` Find best meeting point in grid-like city '' problem __EoT__
[ UI Issues ] Output text can not wrap within the text area __EoT__ The output screen at the left top corner has a black box to indicate the Run results , which only scales based on the screen size instead of the text length . Thus , if the text within this black box is longer than screen size , the rest text will simply extend out of the black box , which is not very elegant . Repro instructions : 1 . Run app.html 2 . Navigate to Question-1 3 . Change code to return a very long string , such as < img width= '' 674 '' alt= '' screen shot 2017-04-03 at 1 40 57 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630894/374ca652-1873-11e7-8a7d-220de99b2957.png '' > 4 . Click on Run button 5 . Actual result will come out in this black box < img width= '' 664 '' alt= '' screen shot 2017-04-03 at 1 36 29 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630929/5b647b46-1873-11e7-8c06-f0c7081b3591.png '' > We can find out the output text exceeds the actual size of the black box .
[ UI Issues ] Output text can not wrap within the text area __EoT__ The output screen at the left top corner has a black box to indicate the Run results , which only scales based on the screen size instead of the text length . Thus , if the text within this black box is longer than screen size , the rest text will simply extend out of the black box , which is not very elegant . Repro instructions : 1 . Run app.html 2 . Navigate to Question-1 3 . Change code to return a very long string , such as < img width= '' 674 '' alt= '' screen shot 2017-04-03 at 1 40 57 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630894/374ca652-1873-11e7-8a7d-220de99b2957.png '' > 4 . Click on Run button 5 . Actual result will come out in this black box < img width= '' 664 '' alt= '' screen shot 2017-04-03 at 1 36 29 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630929/5b647b46-1873-11e7-8c06-f0c7081b3591.png '' > We can find out the output text exceeds the actual size of the black box .
[ UI Issues ] Output text can not wrap within the text area __EoT__ The output screen at the left top corner has a black box to indicate the Run results , which only scales based on the screen size instead of the text length . Thus , if the text within this black box is longer than screen size , the rest text will simply extend out of the black box , which is not very elegant . Repro instructions : 1 . Run app.html 2 . Navigate to Question-1 3 . Change code to return a very long string , such as < img width= '' 674 '' alt= '' screen shot 2017-04-03 at 1 40 57 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630894/374ca652-1873-11e7-8a7d-220de99b2957.png '' > 4 . Click on Run button 5 . Actual result will come out in this black box < img width= '' 664 '' alt= '' screen shot 2017-04-03 at 1 36 29 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630929/5b647b46-1873-11e7-8c06-f0c7081b3591.png '' > We can find out the output text exceeds the actual size of the black box .
[ UI Issues ] Output text can not wrap within the text area __EoT__ The output screen at the left top corner has a black box to indicate the Run results , which only scales based on the screen size instead of the text length . Thus , if the text within this black box is longer than screen size , the rest text will simply extend out of the black box , which is not very elegant . Repro instructions : 1 . Run app.html 2 . Navigate to Question-1 3 . Change code to return a very long string , such as < img width= '' 674 '' alt= '' screen shot 2017-04-03 at 1 40 57 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630894/374ca652-1873-11e7-8a7d-220de99b2957.png '' > 4 . Click on Run button 5 . Actual result will come out in this black box < img width= '' 664 '' alt= '' screen shot 2017-04-03 at 1 36 29 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630929/5b647b46-1873-11e7-8c06-f0c7081b3591.png '' > We can find out the output text exceeds the actual size of the black box .
[ UI Issues ] Output text can not wrap within the text area __EoT__ The output screen at the left top corner has a black box to indicate the Run results , which only scales based on the screen size instead of the text length . Thus , if the text within this black box is longer than screen size , the rest text will simply extend out of the black box , which is not very elegant . Repro instructions : 1 . Run app.html 2 . Navigate to Question-1 3 . Change code to return a very long string , such as < img width= '' 674 '' alt= '' screen shot 2017-04-03 at 1 40 57 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630894/374ca652-1873-11e7-8a7d-220de99b2957.png '' > 4 . Click on Run button 5 . Actual result will come out in this black box < img width= '' 664 '' alt= '' screen shot 2017-04-03 at 1 36 29 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630929/5b647b46-1873-11e7-8c06-f0c7081b3591.png '' > We can find out the output text exceeds the actual size of the black box .
[ UI Issues ] Output text can not wrap within the text area __EoT__ The output screen at the left top corner has a black box to indicate the Run results , which only scales based on the screen size instead of the text length . Thus , if the text within this black box is longer than screen size , the rest text will simply extend out of the black box , which is not very elegant . Repro instructions : 1 . Run app.html 2 . Navigate to Question-1 3 . Change code to return a very long string , such as < img width= '' 674 '' alt= '' screen shot 2017-04-03 at 1 40 57 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630894/374ca652-1873-11e7-8a7d-220de99b2957.png '' > 4 . Click on Run button 5 . Actual result will come out in this black box < img width= '' 664 '' alt= '' screen shot 2017-04-03 at 1 36 29 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630929/5b647b46-1873-11e7-8c06-f0c7081b3591.png '' > We can find out the output text exceeds the actual size of the black box .
[ UI Issues ] Output text can not wrap within the text area __EoT__ The output screen at the left top corner has a black box to indicate the Run results , which only scales based on the screen size instead of the text length . Thus , if the text within this black box is longer than screen size , the rest text will simply extend out of the black box , which is not very elegant . Repro instructions : 1 . Run app.html 2 . Navigate to Question-1 3 . Change code to return a very long string , such as < img width= '' 674 '' alt= '' screen shot 2017-04-03 at 1 40 57 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630894/374ca652-1873-11e7-8a7d-220de99b2957.png '' > 4 . Click on Run button 5 . Actual result will come out in this black box < img width= '' 664 '' alt= '' screen shot 2017-04-03 at 1 36 29 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630929/5b647b46-1873-11e7-8c06-f0c7081b3591.png '' > We can find out the output text exceeds the actual size of the black box .
[ UI Issues ] Output text can not wrap within the text area __EoT__ The output screen at the left top corner has a black box to indicate the Run results , which only scales based on the screen size instead of the text length . Thus , if the text within this black box is longer than screen size , the rest text will simply extend out of the black box , which is not very elegant . Repro instructions : 1 . Run app.html 2 . Navigate to Question-1 3 . Change code to return a very long string , such as < img width= '' 674 '' alt= '' screen shot 2017-04-03 at 1 40 57 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630894/374ca652-1873-11e7-8a7d-220de99b2957.png '' > 4 . Click on Run button 5 . Actual result will come out in this black box < img width= '' 664 '' alt= '' screen shot 2017-04-03 at 1 36 29 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630929/5b647b46-1873-11e7-8c06-f0c7081b3591.png '' > We can find out the output text exceeds the actual size of the black box .
[ UI Issues ] Output text can not wrap within the text area __EoT__ The output screen at the left top corner has a black box to indicate the Run results , which only scales based on the screen size instead of the text length . Thus , if the text within this black box is longer than screen size , the rest text will simply extend out of the black box , which is not very elegant . Repro instructions : 1 . Run app.html 2 . Navigate to Question-1 3 . Change code to return a very long string , such as < img width= '' 674 '' alt= '' screen shot 2017-04-03 at 1 40 57 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630894/374ca652-1873-11e7-8a7d-220de99b2957.png '' > 4 . Click on Run button 5 . Actual result will come out in this black box < img width= '' 664 '' alt= '' screen shot 2017-04-03 at 1 36 29 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630929/5b647b46-1873-11e7-8c06-f0c7081b3591.png '' > We can find out the output text exceeds the actual size of the black box .
[ UI Issues ] Output text can not wrap within the text area __EoT__ The output screen at the left top corner has a black box to indicate the Run results , which only scales based on the screen size instead of the text length . Thus , if the text within this black box is longer than screen size , the rest text will simply extend out of the black box , which is not very elegant . Repro instructions : 1 . Run app.html 2 . Navigate to Question-1 3 . Change code to return a very long string , such as < img width= '' 674 '' alt= '' screen shot 2017-04-03 at 1 40 57 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630894/374ca652-1873-11e7-8a7d-220de99b2957.png '' > 4 . Click on Run button 5 . Actual result will come out in this black box < img width= '' 664 '' alt= '' screen shot 2017-04-03 at 1 36 29 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630929/5b647b46-1873-11e7-8c06-f0c7081b3591.png '' > We can find out the output text exceeds the actual size of the black box .
[ UI Issues ] Output text can not wrap within the text area __EoT__ The output screen at the left top corner has a black box to indicate the Run results , which only scales based on the screen size instead of the text length . Thus , if the text within this black box is longer than screen size , the rest text will simply extend out of the black box , which is not very elegant . Repro instructions : 1 . Run app.html 2 . Navigate to Question-1 3 . Change code to return a very long string , such as < img width= '' 674 '' alt= '' screen shot 2017-04-03 at 1 40 57 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630894/374ca652-1873-11e7-8a7d-220de99b2957.png '' > 4 . Click on Run button 5 . Actual result will come out in this black box < img width= '' 664 '' alt= '' screen shot 2017-04-03 at 1 36 29 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630929/5b647b46-1873-11e7-8c06-f0c7081b3591.png '' > We can find out the output text exceeds the actual size of the black box .
[ UI Issues ] Output text can not wrap within the text area __EoT__ The output screen at the left top corner has a black box to indicate the Run results , which only scales based on the screen size instead of the text length . Thus , if the text within this black box is longer than screen size , the rest text will simply extend out of the black box , which is not very elegant . Repro instructions : 1 . Run app.html 2 . Navigate to Question-1 3 . Change code to return a very long string , such as < img width= '' 674 '' alt= '' screen shot 2017-04-03 at 1 40 57 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630894/374ca652-1873-11e7-8a7d-220de99b2957.png '' > 4 . Click on Run button 5 . Actual result will come out in this black box < img width= '' 664 '' alt= '' screen shot 2017-04-03 at 1 36 29 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630929/5b647b46-1873-11e7-8c06-f0c7081b3591.png '' > We can find out the output text exceeds the actual size of the black box .
[ UI Issues ] Output text can not wrap within the text area __EoT__ The output screen at the left top corner has a black box to indicate the Run results , which only scales based on the screen size instead of the text length . Thus , if the text within this black box is longer than screen size , the rest text will simply extend out of the black box , which is not very elegant . Repro instructions : 1 . Run app.html 2 . Navigate to Question-1 3 . Change code to return a very long string , such as < img width= '' 674 '' alt= '' screen shot 2017-04-03 at 1 40 57 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630894/374ca652-1873-11e7-8a7d-220de99b2957.png '' > 4 . Click on Run button 5 . Actual result will come out in this black box < img width= '' 664 '' alt= '' screen shot 2017-04-03 at 1 36 29 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630929/5b647b46-1873-11e7-8c06-f0c7081b3591.png '' > We can find out the output text exceeds the actual size of the black box .
[ UI Issues ] Output text can not wrap within the text area __EoT__ The output screen at the left top corner has a black box to indicate the Run results , which only scales based on the screen size instead of the text length . Thus , if the text within this black box is longer than screen size , the rest text will simply extend out of the black box , which is not very elegant . Repro instructions : 1 . Run app.html 2 . Navigate to Question-1 3 . Change code to return a very long string , such as < img width= '' 674 '' alt= '' screen shot 2017-04-03 at 1 40 57 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630894/374ca652-1873-11e7-8a7d-220de99b2957.png '' > 4 . Click on Run button 5 . Actual result will come out in this black box < img width= '' 664 '' alt= '' screen shot 2017-04-03 at 1 36 29 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630929/5b647b46-1873-11e7-8c06-f0c7081b3591.png '' > We can find out the output text exceeds the actual size of the black box .
[ UI Issues ] Output text can not wrap within the text area __EoT__ The output screen at the left top corner has a black box to indicate the Run results , which only scales based on the screen size instead of the text length . Thus , if the text within this black box is longer than screen size , the rest text will simply extend out of the black box , which is not very elegant . Repro instructions : 1 . Run app.html 2 . Navigate to Question-1 3 . Change code to return a very long string , such as < img width= '' 674 '' alt= '' screen shot 2017-04-03 at 1 40 57 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630894/374ca652-1873-11e7-8a7d-220de99b2957.png '' > 4 . Click on Run button 5 . Actual result will come out in this black box < img width= '' 664 '' alt= '' screen shot 2017-04-03 at 1 36 29 pm '' src= '' https : //cloud.githubusercontent.com/assets/14989337/24630929/5b647b46-1873-11e7-8c06-f0c7081b3591.png '' > We can find out the output text exceeds the actual size of the black box .
Show reinforcement inputs in code font . __EoT__ In the following screenshot , the last line reads ` Fails on `` lots of spaces '' ` . However , the input given here was actually meant to be `` lots & nbsp ; & nbsp ; & nbsp ; of & nbsp ; & nbsp ; & nbsp ; & nbsp ; & nbsp ; spaces '' . ! [ screenshot from 2017-07-13 20 03 41 ] ( https : //user-images.githubusercontent.com/10575562/28196588-88732078-6806-11e7-890a-343a1af1ac94.png ) The input should be shown in code font so that the space characters are visible .
Syntax errors only display the last error __EoT__ Expected behavior : when running into multiple syntax errors , all past errors can be viewed . This is important for the alternative UI ( # 268 ) , which maintains a history of feedback . Observed behavior : In alternative UI , past syntax error feedback is displayed , but only the most recent error message can be viewed . Steps to reproduce : In alternative UI ( # 268 ) , generate multiple syntax errors and try to view all the errors . Notice only details of the last error is displayed .
Syntax errors only display the last error __EoT__ Expected behavior : when running into multiple syntax errors , all past errors can be viewed . This is important for the alternative UI ( # 268 ) , which maintains a history of feedback . Observed behavior : In alternative UI , past syntax error feedback is displayed , but only the most recent error message can be viewed . Steps to reproduce : In alternative UI ( # 268 ) , generate multiple syntax errors and try to view all the errors . Notice only details of the last error is displayed .
Syntax errors only display the last error __EoT__ Expected behavior : when running into multiple syntax errors , all past errors can be viewed . This is important for the alternative UI ( # 268 ) , which maintains a history of feedback . Observed behavior : In alternative UI , past syntax error feedback is displayed , but only the most recent error message can be viewed . Steps to reproduce : In alternative UI ( # 268 ) , generate multiple syntax errors and try to view all the errors . Notice only details of the last error is displayed .
Syntax errors only display the last error __EoT__ Expected behavior : when running into multiple syntax errors , all past errors can be viewed . This is important for the alternative UI ( # 268 ) , which maintains a history of feedback . Observed behavior : In alternative UI , past syntax error feedback is displayed , but only the most recent error message can be viewed . Steps to reproduce : In alternative UI ( # 268 ) , generate multiple syntax errors and try to view all the errors . Notice only details of the last error is displayed .
Show better error message if user deletes or modifies starter code . __EoT__ Repro steps : 1 . Open app.html 2 . Navigate to the i18n problem . 3 . Change `` abbreviate '' to `` isBalanced '' 4 . Run code . 5 . You should see this error : [ `` Your code threw an error : AttributeError : 'StudentCode ' object has no attribute 'abbreviate ' on line 42 '' ] It might be worth catching that error and saying something closer to `` You seem to be missing an 'abbreviate ' method . Check the starter code and make sure that it has n't been deleted or modified accidentally . ''
Show better error message if user deletes or modifies starter code . __EoT__ Repro steps : 1 . Open app.html 2 . Navigate to the i18n problem . 3 . Change `` abbreviate '' to `` isBalanced '' 4 . Run code . 5 . You should see this error : [ `` Your code threw an error : AttributeError : 'StudentCode ' object has no attribute 'abbreviate ' on line 42 '' ] It might be worth catching that error and saying something closer to `` You seem to be missing an 'abbreviate ' method . Check the starter code and make sure that it has n't been deleted or modified accidentally . ''
Show better error message if user deletes or modifies starter code . __EoT__ Repro steps : 1 . Open app.html 2 . Navigate to the i18n problem . 3 . Change `` abbreviate '' to `` isBalanced '' 4 . Run code . 5 . You should see this error : [ `` Your code threw an error : AttributeError : 'StudentCode ' object has no attribute 'abbreviate ' on line 42 '' ] It might be worth catching that error and saying something closer to `` You seem to be missing an 'abbreviate ' method . Check the starter code and make sure that it has n't been deleted or modified accidentally . ''
Show better error message if user deletes or modifies starter code . __EoT__ Repro steps : 1 . Open app.html 2 . Navigate to the i18n problem . 3 . Change `` abbreviate '' to `` isBalanced '' 4 . Run code . 5 . You should see this error : [ `` Your code threw an error : AttributeError : 'StudentCode ' object has no attribute 'abbreviate ' on line 42 '' ] It might be worth catching that error and saying something closer to `` You seem to be missing an 'abbreviate ' method . Check the starter code and make sure that it has n't been deleted or modified accidentally . ''
Create `` Increment a decimal-coded number '' problem __EoT__
Resetting feedback window does not remove syntax error __EoT__ Expected behavior : When resetting the feedback window , displayed syntax errors are also cleared . Observed behavior : Resetting the feedback window does not clear displayed syntax errors . Steps to reproduce : Generate a syntax error and display details . Reset feedback window and note that syntax error is not cleared .
User study - Alternative to output / expected output feedback __EoT__ Users are relying on the default output / expected output feedback , and it 's causing them to use this feedback as a form of printing and compare it more closely to existing tools . @ seanlip suggested only showing the input and asking the user to consider that input . This would provide us with a default feedback that should always be useful yet prevents the user ( and us ) from depending on output , which is a closer experience to a real interview .
User study - Alternative to output / expected output feedback __EoT__ Users are relying on the default output / expected output feedback , and it 's causing them to use this feedback as a form of printing and compare it more closely to existing tools . @ seanlip suggested only showing the input and asking the user to consider that input . This would provide us with a default feedback that should always be useful yet prevents the user ( and us ) from depending on output , which is a closer experience to a real interview .
User study - Alternative to output / expected output feedback __EoT__ Users are relying on the default output / expected output feedback , and it 's causing them to use this feedback as a form of printing and compare it more closely to existing tools . @ seanlip suggested only showing the input and asking the user to consider that input . This would provide us with a default feedback that should always be useful yet prevents the user ( and us ) from depending on output , which is a closer experience to a real interview .
User study - Alternative to output / expected output feedback __EoT__ Users are relying on the default output / expected output feedback , and it 's causing them to use this feedback as a form of printing and compare it more closely to existing tools . @ seanlip suggested only showing the input and asking the user to consider that input . This would provide us with a default feedback that should always be useful yet prevents the user ( and us ) from depending on output , which is a closer experience to a real interview .
User study - Alternative to output / expected output feedback __EoT__ Users are relying on the default output / expected output feedback , and it 's causing them to use this feedback as a form of printing and compare it more closely to existing tools . @ seanlip suggested only showing the input and asking the user to consider that input . This would provide us with a default feedback that should always be useful yet prevents the user ( and us ) from depending on output , which is a closer experience to a real interview .
User study - Alternative to output / expected output feedback __EoT__ Users are relying on the default output / expected output feedback , and it 's causing them to use this feedback as a form of printing and compare it more closely to existing tools . @ seanlip suggested only showing the input and asking the user to consider that input . This would provide us with a default feedback that should always be useful yet prevents the user ( and us ) from depending on output , which is a closer experience to a real interview .
